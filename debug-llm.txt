
2025-04-01T17:59:39.056Z:
Loaded node-llama-cpp: 

2025-04-01T17:59:46.482Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T17:59:46.482Z:
llamaCpp available methods: 

2025-04-01T17:59:46.482Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T17:59:46.483Z:
Attempting to load real model with llamaCpp

2025-04-01T17:59:46.483Z:
llamaCpp content: {}

2025-04-01T17:59:46.483Z:
No valid constructor found, using mock implementation

2025-04-01T17:59:46.484Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf loaded successfully

2025-04-01T17:59:51.166Z:
Unloading model...

2025-04-01T17:59:51.166Z:
Model unloaded successfully

2025-04-01T17:59:51.166Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T17:59:51.166Z:
llamaCpp available methods: 

2025-04-01T17:59:51.167Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T17:59:51.167Z:
Attempting to load real model with llamaCpp

2025-04-01T17:59:51.167Z:
llamaCpp content: {}

2025-04-01T17:59:51.168Z:
No valid constructor found, using mock implementation

2025-04-01T17:59:51.168Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf loaded successfully

2025-04-01T17:59:59.617Z:
Generate response called with message: Testing who is this...

2025-04-01T17:59:59.617Z:
Using mock implementation for response generation

2025-04-01T17:59:59.726Z:
Sent chunk: I'm 

2025-04-01T17:59:59.835Z:
Sent chunk: currently 

2025-04-01T17:59:59.943Z:
Sent chunk: running 

2025-04-01T18:00:00.051Z:
Sent chunk: in 

2025-04-01T18:00:00.161Z:
Sent chunk: a 

2025-04-01T18:00:00.269Z:
Sent chunk: mock 

2025-04-01T18:00:00.380Z:
Sent chunk: mode 

2025-04-01T18:00:00.489Z:
Sent chunk: because 

2025-04-01T18:00:00.596Z:
Sent chunk: there 

2025-04-01T18:00:00.704Z:
Sent chunk: were 

2025-04-01T18:00:00.813Z:
Sent chunk: issues 

2025-04-01T18:00:00.921Z:
Sent chunk: loading 

2025-04-01T18:00:01.033Z:
Sent chunk: the 

2025-04-01T18:00:01.141Z:
Sent chunk: LLM 

2025-04-01T18:00:01.250Z:
Sent chunk: library. 

2025-04-01T18:00:01.359Z:
Sent chunk: This 

2025-04-01T18:00:01.466Z:
Sent chunk: is 

2025-04-01T18:00:01.576Z:
Sent chunk: a 

2025-04-01T18:00:01.686Z:
Sent chunk: placeholder 

2025-04-01T18:00:01.795Z:
Sent chunk: response 

2025-04-01T18:00:01.904Z:
Sent chunk: to 

2025-04-01T18:00:02.013Z:
Sent chunk: help 

2025-04-01T18:00:02.122Z:
Sent chunk: debug 

2025-04-01T18:00:02.231Z:
Sent chunk: the 

2025-04-01T18:00:02.341Z:
Sent chunk: application 

2025-04-01T18:00:02.451Z:
Sent chunk: flow. 

2025-04-01T18:00:02.560Z:
Sent chunk: Your 

2025-04-01T18:00:02.670Z:
Sent chunk: message 

2025-04-01T18:00:02.778Z:
Sent chunk: was: 

2025-04-01T18:00:02.886Z:
Sent chunk: Testing who is this 

2025-04-01T18:15:43.082Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:15:43.083Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:15:43.083Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:15:43.084Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:15:45.705Z:
Unloading model...

2025-04-01T18:15:45.706Z:
Model unloaded successfully

2025-04-01T18:15:45.706Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:15:45.706Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:15:45.707Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:15:45.707Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:15:54.494Z:
Generate response called with message: Hi testing who am I talking to...

2025-04-01T18:15:54.495Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: Hi testing who am I talking to
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:16:04.425Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:16:04.449Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T18:16:04.471Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:16:04.491Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:16:04.505Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T18:16:04.567Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T18:16:04.568Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:16:04.585Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00


2025-04-01T18:16:04.585Z:
Process stderr: print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T18:16:06.108Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T18:16:06.374Z:
Process stderr: .

2025-04-01T18:16:06.398Z:
Process stderr: .

2025-04-01T18:16:06.422Z:
Process stderr: .

2025-04-01T18:16:06.502Z:
Process stderr: .

2025-04-01T18:16:07.150Z:
Process stderr: .

2025-04-01T18:16:07.674Z:
Process stderr: .

2025-04-01T18:16:08.057Z:
Process stderr: .

2025-04-01T18:16:08.524Z:
Process stderr: .

2025-04-01T18:16:08.597Z:
Process stderr: .

2025-04-01T18:16:08.998Z:
Process stderr: .

2025-04-01T18:16:09.100Z:
Process stderr: .

2025-04-01T18:16:09.284Z:
Process stderr: .

2025-04-01T18:16:09.594Z:
Process stderr: .

2025-04-01T18:16:10.026Z:
Process stderr: .

2025-04-01T18:16:10.257Z:
Process stderr: .

2025-04-01T18:16:10.307Z:
Process stderr: .

2025-04-01T18:16:10.380Z:
Process stderr: .

2025-04-01T18:16:10.571Z:
Process stderr: .

2025-04-01T18:16:10.636Z:
Process stderr: .

2025-04-01T18:16:10.715Z:
Process stderr: .

2025-04-01T18:16:10.770Z:
Process stderr: .

2025-04-01T18:16:10.851Z:
Process stderr: .

2025-04-01T18:16:11.044Z:
Process stderr: .

2025-04-01T18:16:11.226Z:
Process stderr: .

2025-04-01T18:16:11.296Z:
Process stderr: .

2025-04-01T18:16:11.552Z:
Process stderr: .

2025-04-01T18:16:11.628Z:
Process stderr: .

2025-04-01T18:16:11.677Z:
Process stderr: .

2025-04-01T18:16:11.752Z:
Process stderr: .

2025-04-01T18:16:11.944Z:
Process stderr: .

2025-04-01T18:16:11.995Z:
Process stderr: .

2025-04-01T18:16:12.068Z:
Process stderr: .

2025-04-01T18:16:12.259Z:
Process stderr: .

2025-04-01T18:16:12.308Z:
Process stderr: .

2025-04-01T18:16:12.382Z:
Process stderr: .

2025-04-01T18:16:12.433Z:
Process stderr: .

2025-04-01T18:16:12.507Z:
Process stderr: .

2025-04-01T18:16:12.699Z:
Process stderr: .

2025-04-01T18:16:12.769Z:
Process stderr: .

2025-04-01T18:16:12.836Z:
Process stderr: .

2025-04-01T18:16:13.123Z:
Process stderr: .

2025-04-01T18:16:13.146Z:
Process stderr: .

2025-04-01T18:16:13.419Z:
Process stderr: .

2025-04-01T18:16:13.450Z:
Process stderr: .

2025-04-01T18:16:13.723Z:
Process stderr: .

2025-04-01T18:16:13.787Z:
Process stderr: .

2025-04-01T18:16:13.872Z:
Process stderr: .

2025-04-01T18:16:14.263Z:
Process stderr: .

2025-04-01T18:16:14.315Z:
Process stderr: .

2025-04-01T18:16:14.386Z:
Process stderr: .

2025-04-01T18:16:14.435Z:
Process stderr: .

2025-04-01T18:16:14.514Z:
Process stderr: .

2025-04-01T18:16:14.730Z:
Process stderr: .

2025-04-01T18:16:14.783Z:
Process stderr: .

2025-04-01T18:16:14.872Z:
Process stderr: .

2025-04-01T18:16:15.082Z:
Process stderr: .

2025-04-01T18:16:15.248Z:
Process stderr: ..

2025-04-01T18:16:15.312Z:
Process stderr: .

2025-04-01T18:16:15.423Z:
Process stderr: .

2025-04-01T18:16:15.496Z:
Process stderr: .

2025-04-01T18:16:15.620Z:
Process stderr: .

2025-04-01T18:16:15.649Z:
Process stderr: .

2025-04-01T18:16:15.827Z:
Process stderr: .

2025-04-01T18:16:15.887Z:
Process stderr: .

2025-04-01T18:16:16.235Z:
Process stderr: .

2025-04-01T18:16:16.236Z:
Process stderr: ...

2025-04-01T18:16:16.238Z:
Process stderr: ............

2025-04-01T18:16:16.238Z:
Process stderr: .

2025-04-01T18:16:16.238Z:
Process stderr: .


2025-04-01T18:16:16.405Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T18:16:16.411Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB


2025-04-01T18:16:16.412Z:
Process stderr: init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T18:16:16.435Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB


2025-04-01T18:16:16.455Z:
Process stderr: llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T18:16:16.467Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T18:16:18.574Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T18:16:18.575Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>



2025-04-01T18:16:18.575Z:
Process stderr: system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T18:16:18.595Z:
Process stderr: main: interactive mode on.


2025-04-01T18:16:18.597Z:
Process stderr: sampler seed: 3737049038


2025-04-01T18:16:18.597Z:
Process stderr: sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T18:16:18.824Z:
Received chunk: <think>...

2025-04-01T18:16:18.915Z:
Received chunk: 
...

2025-04-01T18:16:19.007Z:
Received chunk: Alright...

2025-04-01T18:16:19.098Z:
Received chunk: ,...

2025-04-01T18:16:19.193Z:
Received chunk:  the...

2025-04-01T18:16:19.291Z:
Received chunk:  user...

2025-04-01T18:16:19.384Z:
Received chunk:  just...

2025-04-01T18:16:19.473Z:
Received chunk:  said...

2025-04-01T18:16:19.560Z:
Received chunk:  "...

2025-04-01T18:16:19.646Z:
Received chunk: Hi...

2025-04-01T18:16:19.735Z:
Received chunk:  testing...

2025-04-01T18:16:19.821Z:
Received chunk:  who...

2025-04-01T18:16:19.908Z:
Received chunk:  am...

2025-04-01T18:16:19.995Z:
Received chunk:  I...

2025-04-01T18:16:20.079Z:
Received chunk:  talking...

2025-04-01T18:16:20.164Z:
Received chunk:  to...

2025-04-01T18:16:20.250Z:
Received chunk: ."...

2025-04-01T18:16:20.336Z:
Received chunk:  Hmm...

2025-04-01T18:16:20.419Z:
Received chunk: ,...

2025-04-01T18:16:20.504Z:
Received chunk:  that...

2025-04-01T18:16:20.590Z:
Received chunk: 's...

2025-04-01T18:16:20.676Z:
Received chunk:  an...

2025-04-01T18:16:20.761Z:
Received chunk:  interesting...

2025-04-01T18:16:20.849Z:
Received chunk:  way...

2025-04-01T18:16:20.934Z:
Received chunk:  to...

2025-04-01T18:16:21.022Z:
Received chunk:  start...

2025-04-01T18:16:21.114Z:
Received chunk: ....

2025-04-01T18:16:21.207Z:
Received chunk:  They...

2025-04-01T18:16:21.300Z:
Received chunk: 're...

2025-04-01T18:16:21.397Z:
Received chunk:  probably...

2025-04-01T18:16:21.488Z:
Received chunk:  trying...

2025-04-01T18:16:21.580Z:
Received chunk:  out...

2025-04-01T18:16:21.669Z:
Received chunk:  how...

2025-04-01T18:16:21.754Z:
Received chunk:  this...

2025-04-01T18:16:21.841Z:
Received chunk:  chat...

2025-04-01T18:16:21.932Z:
Received chunk:  works...

2025-04-01T18:16:22.018Z:
Received chunk:  or...

2025-04-01T18:16:22.101Z:
Received chunk:  maybe...

2025-04-01T18:16:22.184Z:
Received chunk:  checking...

2025-04-01T18:16:22.271Z:
Received chunk:  if...

2025-04-01T18:16:22.358Z:
Received chunk:  AI...

2025-04-01T18:16:22.443Z:
Received chunk:  can...

2025-04-01T18:16:22.528Z:
Received chunk:  understand...

2025-04-01T18:16:22.614Z:
Received chunk:  their...

2025-04-01T18:16:22.700Z:
Received chunk:  intent...

2025-04-01T18:16:22.788Z:
Received chunk: .

...

2025-04-01T18:16:22.873Z:
Received chunk: I...

2025-04-01T18:16:22.958Z:
Received chunk:  should...

2025-04-01T18:16:23.044Z:
Received chunk:  respond...

2025-04-01T18:16:23.128Z:
Received chunk:  in...

2025-04-01T18:16:23.214Z:
Received chunk:  a...

2025-04-01T18:16:23.301Z:
Received chunk:  friendly...

2025-04-01T18:16:23.386Z:
Received chunk:  manner...

2025-04-01T18:16:23.473Z:
Received chunk: ....

2025-04-01T18:16:23.564Z:
Received chunk:  Maybe...

2025-04-01T18:16:23.652Z:
Received chunk:  acknowledge...

2025-04-01T18:16:23.743Z:
Received chunk:  that...

2025-04-01T18:16:23.831Z:
Received chunk:  they...

2025-04-01T18:16:23.921Z:
Received chunk: 've...

2025-04-01T18:16:24.012Z:
Received chunk:  tested...

2025-04-01T18:16:24.098Z:
Received chunk:  something...

2025-04-01T18:16:24.187Z:
Received chunk:  and...

2025-04-01T18:16:24.277Z:
Received chunk:  ask...

2025-04-01T18:16:24.364Z:
Received chunk:  them...

2025-04-01T18:16:24.452Z:
Received chunk:  what...

2025-04-01T18:16:24.538Z:
Received chunk:  they...

2025-04-01T18:16:24.624Z:
Received chunk:  need...

2025-04-01T18:16:24.710Z:
Received chunk:  help...

2025-04-01T18:16:24.796Z:
Received chunk:  with...

2025-04-01T18:16:24.884Z:
Received chunk: ....

2025-04-01T18:16:24.970Z:
Received chunk:  That...

2025-04-01T18:16:25.057Z:
Received chunk:  way...

2025-04-01T18:16:25.142Z:
Received chunk: ,...

2025-04-01T18:16:25.230Z:
Received chunk:  I...

2025-04-01T18:16:25.323Z:
Received chunk:  keep...

2025-04-01T18:16:25.414Z:
Received chunk:  the...

2025-04-01T18:16:25.502Z:
Received chunk:  conversation...

2025-04-01T18:16:25.588Z:
Received chunk:  going...

2025-04-01T18:16:25.675Z:
Received chunk:  smoothly...

2025-04-01T18:16:25.766Z:
Received chunk:  without...

2025-04-01T18:16:25.855Z:
Received chunk:  being...

2025-04-01T18:16:25.948Z:
Received chunk:  too...

2025-04-01T18:16:26.037Z:
Received chunk:  robotic...

2025-04-01T18:16:26.127Z:
Received chunk: .

...

2025-04-01T18:16:26.214Z:
Received chunk: Also...

2025-04-01T18:16:26.302Z:
Received chunk: ,...

2025-04-01T18:16:26.394Z:
Received chunk:  making...

2025-04-01T18:16:26.481Z:
Received chunk:  sure...

2025-04-01T18:16:26.568Z:
Received chunk:  my...

2025-04-01T18:16:26.654Z:
Received chunk:  response...

2025-04-01T18:16:26.741Z:
Received chunk:  is...

2025-04-01T18:16:26.829Z:
Received chunk:  clear...

2025-04-01T18:16:26.930Z:
Received chunk:  and...

2025-04-01T18:16:27.017Z:
Received chunk:  approach...

2025-04-01T18:16:27.104Z:
Received chunk: able...

2025-04-01T18:16:27.190Z:
Received chunk:  will...

2025-04-01T18:16:27.276Z:
Received chunk:  help...

2025-04-01T18:16:27.365Z:
Received chunk:  build...

2025-04-01T18:16:27.456Z:
Received chunk:  trust...

2025-04-01T18:16:27.548Z:
Received chunk: ....

2025-04-01T18:16:27.635Z:
Received chunk:  Let...

2025-04-01T18:16:27.726Z:
Received chunk: 's...

2025-04-01T18:16:27.820Z:
Received chunk:  go...

2025-04-01T18:16:27.912Z:
Received chunk:  with...

2025-04-01T18:16:28.002Z:
Received chunk:  something...

2025-04-01T18:16:28.086Z:
Received chunk:  like...

2025-04-01T18:16:28.172Z:
Received chunk: ,...

2025-04-01T18:16:28.262Z:
Received chunk:  "...

2025-04-01T18:16:28.351Z:
Received chunk: Hello...

2025-04-01T18:16:28.433Z:
Received chunk: !...

2025-04-01T18:16:28.520Z:
Received chunk:  It...

2025-04-01T18:16:28.604Z:
Received chunk:  looks...

2025-04-01T18:16:28.692Z:
Received chunk:  like...

2025-04-01T18:16:28.783Z:
Received chunk:  you...

2025-04-01T18:16:28.872Z:
Received chunk: ’ve...

2025-04-01T18:16:28.960Z:
Received chunk:  tested...

2025-04-01T18:16:29.045Z:
Received chunk:  us...

2025-04-01T18:16:29.128Z:
Received chunk:  out...

2025-04-01T18:16:29.213Z:
Received chunk:  a...

2025-04-01T18:16:29.298Z:
Received chunk:  bit...

2025-04-01T18:16:29.382Z:
Received chunk: ....

2025-04-01T18:16:29.465Z:
Received chunk:  How...

2025-04-01T18:16:29.551Z:
Received chunk:  can...

2025-04-01T18:16:29.636Z:
Received chunk:  I...

2025-04-01T18:16:29.722Z:
Received chunk:  assist...

2025-04-01T18:16:29.806Z:
Received chunk:  you...

2025-04-01T18:16:29.891Z:
Received chunk:  today...

2025-04-01T18:16:29.975Z:
Received chunk: ?"...

2025-04-01T18:16:30.061Z:
Received chunk:  Yeah...

2025-04-01T18:16:30.144Z:
Received chunk: ,...

2025-04-01T18:16:30.229Z:
Received chunk:  that...

2025-04-01T18:16:30.315Z:
Received chunk:  should...

2025-04-01T18:16:30.406Z:
Received chunk:  do...

2025-04-01T18:16:30.495Z:
Received chunk:  it...

2025-04-01T18:16:30.583Z:
Received chunk: .
...

2025-04-01T18:16:30.674Z:
Received chunk: </think>...

2025-04-01T18:16:30.759Z:
Received chunk: 

...

2025-04-01T18:16:30.858Z:
Received chunk: Hello...

2025-04-01T18:16:30.957Z:
Received chunk: !...

2025-04-01T18:16:31.056Z:
Received chunk:  It...

2025-04-01T18:16:31.155Z:
Received chunk:  seems...

2025-04-01T18:16:31.253Z:
Received chunk:  like...

2025-04-01T18:16:31.350Z:
Received chunk:  you...

2025-04-01T18:16:31.447Z:
Received chunk: 've...

2025-04-01T18:16:31.539Z:
Received chunk:  tested...

2025-04-01T18:16:31.626Z:
Received chunk:  us...

2025-04-01T18:16:31.716Z:
Received chunk:  out...

2025-04-01T18:16:31.808Z:
Received chunk:  a...

2025-04-01T18:16:31.896Z:
Received chunk:  bit...

2025-04-01T18:16:31.982Z:
Received chunk: ....

2025-04-01T18:16:32.068Z:
Received chunk:  How...

2025-04-01T18:16:32.156Z:
Received chunk:  can...

2025-04-01T18:16:32.242Z:
Received chunk:  I...

2025-04-01T18:16:32.331Z:
Received chunk:  assist...

2025-04-01T18:16:32.421Z:
Received chunk:  you...

2025-04-01T18:16:32.510Z:
Received chunk:  today...

2025-04-01T18:16:32.601Z:
Received chunk: ?...

2025-04-01T18:16:32.694Z:
Received chunk: 

> ...

2025-04-01T18:17:17.215Z:
Unloading model...

2025-04-01T18:17:17.215Z:
Terminating running process

2025-04-01T18:17:17.216Z:
Model unloaded successfully

2025-04-01T18:17:17.216Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:17.217Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:17:17.217Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:17.218Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:17:17.641Z:
Process exited with code null

2025-04-01T18:17:17.641Z:
Process failed, falling back to simulation

2025-04-01T18:17:17.642Z:
Generating simulated response

2025-04-01T18:17:17.733Z:
Sent simulated chunk: Hello!

2025-04-01T18:17:17.825Z:
Sent simulated chunk: I'm

2025-04-01T18:17:17.890Z:
Sent simulated chunk: a

2025-04-01T18:17:17.967Z:
Sent simulated chunk: local

2025-04-01T18:17:18.061Z:
Sent simulated chunk: AI

2025-04-01T18:17:18.123Z:
Sent simulated chunk: assistant

2025-04-01T18:17:18.200Z:
Sent simulated chunk: running

2025-04-01T18:17:18.288Z:
Sent simulated chunk: on

2025-04-01T18:17:18.355Z:
Sent simulated chunk: your

2025-04-01T18:17:18.418Z:
Sent simulated chunk: device

2025-04-01T18:17:18.482Z:
Sent simulated chunk: through

2025-04-01T18:17:18.591Z:
Sent simulated chunk: the

2025-04-01T18:17:18.684Z:
Sent simulated chunk: LM

2025-04-01T18:17:18.730Z:
Sent simulated chunk: Terminal.

2025-04-01T18:17:18.823Z:
Sent simulated chunk: How

2025-04-01T18:17:18.928Z:
Sent simulated chunk: can

2025-04-01T18:17:18.990Z:
Sent simulated chunk: I

2025-04-01T18:17:19.021Z:
Sent simulated chunk: help

2025-04-01T18:17:19.083Z:
Sent simulated chunk: you

2025-04-01T18:17:19.162Z:
Sent simulated chunk: today?

(This

2025-04-01T18:17:19.253Z:
Sent simulated chunk: response

2025-04-01T18:17:19.280Z:
Unloading model...

2025-04-01T18:17:19.280Z:
Model unloaded successfully

2025-04-01T18:17:19.280Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:19.280Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:17:19.281Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:19.281Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:17:19.362Z:
Sent simulated chunk: was

2025-04-01T18:17:19.455Z:
Sent simulated chunk: generated

2025-04-01T18:17:19.532Z:
Sent simulated chunk: using

2025-04-01T18:17:19.609Z:
Sent simulated chunk: a

2025-04-01T18:17:19.719Z:
Sent simulated chunk: simulated

2025-04-01T18:17:19.782Z:
Sent simulated chunk: version

2025-04-01T18:17:19.859Z:
Sent simulated chunk: of

2025-04-01T18:17:19.952Z:
Sent simulated chunk: the

2025-04-01T18:17:20.064Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T18:17:20.110Z:
Sent simulated chunk: model)

2025-04-01T18:17:20.110Z:
Simulated response generation completed

2025-04-01T18:17:26.360Z:
Generate response called with message: hi who are you no thinking tags please...

2025-04-01T18:17:26.360Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: hi who are you no thinking tags please
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:17:26.402Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:17:26.426Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T18:17:26.449Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:17:26.459Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:17:26.482Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T18:17:26.538Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T18:17:26.539Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:17:26.556Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1


2025-04-01T18:17:26.556Z:
Process stderr: print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T18:17:27.292Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T18:17:27.316Z:
Process stderr: .

2025-04-01T18:17:27.337Z:
Process stderr: .

2025-04-01T18:17:27.358Z:
Process stderr: .

2025-04-01T18:17:27.378Z:
Process stderr: .

2025-04-01T18:17:27.399Z:
Process stderr: .

2025-04-01T18:17:27.420Z:
Process stderr: .

2025-04-01T18:17:27.442Z:
Process stderr: .

2025-04-01T18:17:27.477Z:
Process stderr: .

2025-04-01T18:17:27.484Z:
Process stderr: .

2025-04-01T18:17:27.519Z:
Process stderr: .

2025-04-01T18:17:27.536Z:
Process stderr: .

2025-04-01T18:17:27.561Z:
Process stderr: .

2025-04-01T18:17:27.579Z:
Process stderr: .

2025-04-01T18:17:27.605Z:
Process stderr: .

2025-04-01T18:17:27.623Z:
Process stderr: .

2025-04-01T18:17:27.641Z:
Process stderr: .

2025-04-01T18:17:27.667Z:
Process stderr: .

2025-04-01T18:17:27.685Z:
Process stderr: .

2025-04-01T18:17:27.703Z:
Process stderr: .

2025-04-01T18:17:27.729Z:
Process stderr: .

2025-04-01T18:17:27.748Z:
Process stderr: .

2025-04-01T18:17:27.774Z:
Process stderr: .

2025-04-01T18:17:27.792Z:
Process stderr: .

2025-04-01T18:17:27.814Z:
Process stderr: .

2025-04-01T18:17:27.837Z:
Process stderr: .

2025-04-01T18:17:27.875Z:
Process stderr: .

2025-04-01T18:17:27.902Z:
Process stderr: .

2025-04-01T18:17:27.921Z:
Process stderr: .

2025-04-01T18:17:27.950Z:
Process stderr: .

2025-04-01T18:17:27.971Z:
Process stderr: .

2025-04-01T18:17:27.991Z:
Process stderr: .

2025-04-01T18:17:28.020Z:
Process stderr: .

2025-04-01T18:17:28.040Z:
Process stderr: .

2025-04-01T18:17:28.060Z:
Process stderr: .

2025-04-01T18:17:28.089Z:
Process stderr: .

2025-04-01T18:17:28.110Z:
Process stderr: .

2025-04-01T18:17:28.139Z:
Process stderr: .

2025-04-01T18:17:28.159Z:
Process stderr: .

2025-04-01T18:17:28.183Z:
Process stderr: .

2025-04-01T18:17:28.207Z:
Process stderr: .

2025-04-01T18:17:28.247Z:
Process stderr: .

2025-04-01T18:17:28.255Z:
Process stderr: .

2025-04-01T18:17:28.295Z:
Process stderr: .

2025-04-01T18:17:28.304Z:
Process stderr: .

2025-04-01T18:17:28.343Z:
Process stderr: .

2025-04-01T18:17:28.364Z:
Process stderr: .

2025-04-01T18:17:28.393Z:
Process stderr: .

2025-04-01T18:17:28.412Z:
Process stderr: .

2025-04-01T18:17:28.432Z:
Process stderr: .

2025-04-01T18:17:28.463Z:
Process stderr: .

2025-04-01T18:17:28.491Z:
Process stderr: .

2025-04-01T18:17:28.521Z:
Process stderr: .

2025-04-01T18:17:28.539Z:
Process stderr: .

2025-04-01T18:17:28.560Z:
Process stderr: .

2025-04-01T18:17:28.589Z:
Process stderr: .

2025-04-01T18:17:28.607Z:
Process stderr: .

2025-04-01T18:17:28.632Z:
Process stderr: .

2025-04-01T18:17:28.657Z:
Process stderr: .

2025-04-01T18:17:28.682Z:
Process stderr: .

2025-04-01T18:17:28.707Z:
Process stderr: .

2025-04-01T18:17:28.731Z:
Process stderr: .

2025-04-01T18:17:28.776Z:
Process stderr: .

2025-04-01T18:17:28.784Z:
Process stderr: .

2025-04-01T18:17:28.824Z:
Process stderr: .

2025-04-01T18:17:28.834Z:
Process stderr: .

2025-04-01T18:17:28.877Z:
Process stderr: .....

2025-04-01T18:17:28.878Z:
Process stderr: ........

2025-04-01T18:17:28.878Z:
Process stderr: .....


2025-04-01T18:17:28.880Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T18:17:28.881Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T18:17:28.900Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T18:17:28.906Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T18:17:29.155Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T18:17:29.156Z:
Process stderr: main: interactive mode on.


2025-04-01T18:17:29.157Z:
Process stderr: sampler seed: 2668447034
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T18:17:29.400Z:
Received chunk: <think>...

2025-04-01T18:17:29.493Z:
Received chunk: 

...

2025-04-01T18:17:29.582Z:
Received chunk: </think>...

2025-04-01T18:17:29.673Z:
Received chunk: 

...

2025-04-01T18:17:29.763Z:
Received chunk: Hi...

2025-04-01T18:17:29.853Z:
Received chunk: !...

2025-04-01T18:17:29.939Z:
Received chunk:  I...

2025-04-01T18:17:30.026Z:
Received chunk: 'm...

2025-04-01T18:17:30.112Z:
Received chunk:  Deep...

2025-04-01T18:17:30.207Z:
Received chunk: Seek...

2025-04-01T18:17:30.302Z:
Received chunk: -R...

2025-04-01T18:17:30.392Z:
Received chunk: 1...

2025-04-01T18:17:30.482Z:
Received chunk: ,...

2025-04-01T18:17:30.584Z:
Received chunk:  an...

2025-04-01T18:17:30.672Z:
Received chunk:  artificial...

2025-04-01T18:17:30.763Z:
Received chunk:  intelligence...

2025-04-01T18:17:30.856Z:
Received chunk:  assistant...

2025-04-01T18:17:30.946Z:
Received chunk:  created...

2025-04-01T18:17:31.033Z:
Received chunk:  by...

2025-04-01T18:17:31.127Z:
Received chunk:  Deep...

2025-04-01T18:17:31.219Z:
Received chunk: Seek...

2025-04-01T18:17:31.307Z:
Received chunk: ....

2025-04-01T18:17:31.395Z:
Received chunk:  I...

2025-04-01T18:17:31.482Z:
Received chunk: 'm...

2025-04-01T18:17:31.571Z:
Received chunk:  at...

2025-04-01T18:17:31.658Z:
Received chunk:  your...

2025-04-01T18:17:31.743Z:
Received chunk:  service...

2025-04-01T18:17:31.829Z:
Received chunk:  and...

2025-04-01T18:17:31.917Z:
Received chunk:  would...

2025-04-01T18:17:32.005Z:
Received chunk:  be...

2025-04-01T18:17:32.093Z:
Received chunk:  delighted...

2025-04-01T18:17:32.189Z:
Received chunk:  to...

2025-04-01T18:17:32.274Z:
Received chunk:  assist...

2025-04-01T18:17:32.359Z:
Received chunk:  you...

2025-04-01T18:17:32.449Z:
Received chunk:  with...

2025-04-01T18:17:32.538Z:
Received chunk:  any...

2025-04-01T18:17:32.636Z:
Received chunk:  inquiries...

2025-04-01T18:17:32.727Z:
Received chunk:  or...

2025-04-01T18:17:32.814Z:
Received chunk:  tasks...

2025-04-01T18:17:32.897Z:
Received chunk:  you...

2025-04-01T18:17:32.982Z:
Received chunk:  may...

2025-04-01T18:17:33.069Z:
Received chunk:  have...

2025-04-01T18:17:33.156Z:
Received chunk: ....

2025-04-01T18:17:33.244Z:
Received chunk: 

> ...

2025-04-01T18:17:53.747Z:
Unloading model...

2025-04-01T18:17:53.748Z:
Terminating running process

2025-04-01T18:17:53.748Z:
Model unloaded successfully

2025-04-01T18:17:53.750Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:53.750Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:17:53.750Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:53.751Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:17:54.086Z:
Unloading model...

2025-04-01T18:17:54.087Z:
Model unloaded successfully

2025-04-01T18:17:54.087Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:54.087Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:17:54.087Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:17:54.088Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:17:54.194Z:
Process exited with code null

2025-04-01T18:17:54.195Z:
Process failed, falling back to simulation

2025-04-01T18:17:54.195Z:
Generating simulated response

2025-04-01T18:17:54.283Z:
Sent simulated chunk: Hello!

2025-04-01T18:17:54.360Z:
Sent simulated chunk: I'm

2025-04-01T18:17:54.423Z:
Sent simulated chunk: a

2025-04-01T18:17:54.501Z:
Sent simulated chunk: local

2025-04-01T18:17:54.579Z:
Sent simulated chunk: AI

2025-04-01T18:17:54.686Z:
Sent simulated chunk: assistant

2025-04-01T18:17:54.763Z:
Sent simulated chunk: running

2025-04-01T18:17:54.840Z:
Sent simulated chunk: on

2025-04-01T18:17:54.916Z:
Sent simulated chunk: your

2025-04-01T18:17:54.995Z:
Sent simulated chunk: device

2025-04-01T18:17:55.087Z:
Sent simulated chunk: through

2025-04-01T18:17:55.178Z:
Sent simulated chunk: the

2025-04-01T18:17:55.210Z:
Sent simulated chunk: LM

2025-04-01T18:17:55.303Z:
Sent simulated chunk: Terminal.

2025-04-01T18:17:55.397Z:
Sent simulated chunk: How

2025-04-01T18:17:55.476Z:
Sent simulated chunk: can

2025-04-01T18:17:55.552Z:
Sent simulated chunk: I

2025-04-01T18:17:55.661Z:
Sent simulated chunk: help

2025-04-01T18:17:55.723Z:
Sent simulated chunk: you

2025-04-01T18:17:55.817Z:
Sent simulated chunk: today?

(This

2025-04-01T18:17:55.863Z:
Sent simulated chunk: response

2025-04-01T18:17:55.924Z:
Sent simulated chunk: was

2025-04-01T18:17:56.018Z:
Sent simulated chunk: generated

2025-04-01T18:17:56.066Z:
Sent simulated chunk: using

2025-04-01T18:17:56.175Z:
Sent simulated chunk: a

2025-04-01T18:17:56.220Z:
Sent simulated chunk: simulated

2025-04-01T18:17:56.314Z:
Sent simulated chunk: version

2025-04-01T18:17:56.407Z:
Sent simulated chunk: of

2025-04-01T18:17:56.469Z:
Sent simulated chunk: the

2025-04-01T18:17:56.563Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T18:17:56.644Z:
Sent simulated chunk: model)

2025-04-01T18:17:56.645Z:
Simulated response generation completed

2025-04-01T18:17:58.384Z:
Generate response called with message: hi who are you ...

2025-04-01T18:17:58.384Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: hi who are you 
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:18:05.137Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:18:05.166Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T18:18:05.189Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:18:05.198Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:18:05.221Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T18:18:05.276Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T18:18:05.277Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:18:05.294Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'


2025-04-01T18:18:05.294Z:
Process stderr: print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T18:18:05.582Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T18:18:05.606Z:
Process stderr: .

2025-04-01T18:18:05.625Z:
Process stderr: .

2025-04-01T18:18:05.646Z:
Process stderr: .

2025-04-01T18:18:05.666Z:
Process stderr: .

2025-04-01T18:18:05.686Z:
Process stderr: .

2025-04-01T18:18:05.706Z:
Process stderr: .

2025-04-01T18:18:05.727Z:
Process stderr: .

2025-04-01T18:18:05.761Z:
Process stderr: .

2025-04-01T18:18:05.767Z:
Process stderr: .

2025-04-01T18:18:05.800Z:
Process stderr: .

2025-04-01T18:18:05.817Z:
Process stderr: .

2025-04-01T18:18:05.841Z:
Process stderr: .

2025-04-01T18:18:05.857Z:
Process stderr: .

2025-04-01T18:18:05.882Z:
Process stderr: .

2025-04-01T18:18:05.898Z:
Process stderr: .

2025-04-01T18:18:05.915Z:
Process stderr: .

2025-04-01T18:18:05.940Z:
Process stderr: .

2025-04-01T18:18:05.955Z:
Process stderr: .

2025-04-01T18:18:05.972Z:
Process stderr: .

2025-04-01T18:18:05.996Z:
Process stderr: .

2025-04-01T18:18:06.013Z:
Process stderr: .

2025-04-01T18:18:06.037Z:
Process stderr: .

2025-04-01T18:18:06.053Z:
Process stderr: .

2025-04-01T18:18:06.075Z:
Process stderr: .

2025-04-01T18:18:06.095Z:
Process stderr: .

2025-04-01T18:18:06.128Z:
Process stderr: .

2025-04-01T18:18:06.152Z:
Process stderr: .

2025-04-01T18:18:06.169Z:
Process stderr: .

2025-04-01T18:18:06.193Z:
Process stderr: .

2025-04-01T18:18:06.210Z:
Process stderr: .

2025-04-01T18:18:06.227Z:
Process stderr: .

2025-04-01T18:18:06.251Z:
Process stderr: .

2025-04-01T18:18:06.267Z:
Process stderr: .

2025-04-01T18:18:06.284Z:
Process stderr: .

2025-04-01T18:18:06.308Z:
Process stderr: .

2025-04-01T18:18:06.326Z:
Process stderr: .

2025-04-01T18:18:06.351Z:
Process stderr: .

2025-04-01T18:18:06.367Z:
Process stderr: .

2025-04-01T18:18:06.387Z:
Process stderr: .

2025-04-01T18:18:06.408Z:
Process stderr: .

2025-04-01T18:18:06.445Z:
Process stderr: .

2025-04-01T18:18:06.453Z:
Process stderr: .

2025-04-01T18:18:06.490Z:
Process stderr: .

2025-04-01T18:18:06.498Z:
Process stderr: .

2025-04-01T18:18:06.536Z:
Process stderr: .

2025-04-01T18:18:06.558Z:
Process stderr: .

2025-04-01T18:18:06.589Z:
Process stderr: .

2025-04-01T18:18:06.608Z:
Process stderr: .

2025-04-01T18:18:06.630Z:
Process stderr: .

2025-04-01T18:18:06.659Z:
Process stderr: .

2025-04-01T18:18:06.679Z:
Process stderr: .

2025-04-01T18:18:06.709Z:
Process stderr: .

2025-04-01T18:18:06.728Z:
Process stderr: .

2025-04-01T18:18:06.750Z:
Process stderr: .

2025-04-01T18:18:06.779Z:
Process stderr: .

2025-04-01T18:18:06.798Z:
Process stderr: .

2025-04-01T18:18:06.823Z:
Process stderr: .

2025-04-01T18:18:06.849Z:
Process stderr: .

2025-04-01T18:18:06.873Z:
Process stderr: .

2025-04-01T18:18:06.897Z:
Process stderr: .

2025-04-01T18:18:06.924Z:
Process stderr: .

2025-04-01T18:18:06.970Z:
Process stderr: .

2025-04-01T18:18:06.978Z:
Process stderr: .

2025-04-01T18:18:07.022Z:
Process stderr: .

2025-04-01T18:18:07.030Z:
Process stderr: .

2025-04-01T18:18:07.071Z:
Process stderr: .....

2025-04-01T18:18:07.074Z:
Process stderr: .............


2025-04-01T18:18:07.075Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T18:18:07.098Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T18:18:07.103Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T18:18:07.380Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T18:18:07.381Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T18:18:07.381Z:
Process stderr: sampler seed: 877648913
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.


2025-04-01T18:18:07.382Z:
Process stderr:  - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T18:18:07.644Z:
Received chunk: <think>...

2025-04-01T18:18:07.739Z:
Received chunk: 
...

2025-04-01T18:18:07.833Z:
Received chunk: Okay...

2025-04-01T18:18:07.929Z:
Received chunk: ,...

2025-04-01T18:18:08.026Z:
Received chunk:  the...

2025-04-01T18:18:08.124Z:
Received chunk:  user...

2025-04-01T18:18:08.219Z:
Received chunk:  greeted...

2025-04-01T18:18:08.312Z:
Received chunk:  me...

2025-04-01T18:18:08.406Z:
Received chunk:  with...

2025-04-01T18:18:08.500Z:
Received chunk:  "...

2025-04-01T18:18:08.597Z:
Received chunk: hi...

2025-04-01T18:18:08.692Z:
Received chunk:  who...

2025-04-01T18:18:08.785Z:
Received chunk:  are...

2025-04-01T18:18:08.877Z:
Received chunk:  you...

2025-04-01T18:18:08.962Z:
Received chunk: ."...

2025-04-01T18:18:09.046Z:
Received chunk:  I...

2025-04-01T18:18:09.139Z:
Received chunk:  need...

2025-04-01T18:18:09.232Z:
Received chunk:  to...

2025-04-01T18:18:09.321Z:
Received chunk:  respond...

2025-04-01T18:18:09.409Z:
Received chunk:  in...

2025-04-01T18:18:09.499Z:
Received chunk:  a...

2025-04-01T18:18:09.589Z:
Received chunk:  friendly...

2025-04-01T18:18:09.680Z:
Received chunk:  and...

2025-04-01T18:18:09.769Z:
Received chunk:  informative...

2025-04-01T18:18:09.853Z:
Received chunk:  way...

2025-04-01T18:18:09.936Z:
Received chunk: ....

2025-04-01T18:18:10.020Z:
Received chunk:  Let...

2025-04-01T18:18:10.105Z:
Received chunk: 's...

2025-04-01T18:18:10.188Z:
Received chunk:  see...

2025-04-01T18:18:10.271Z:
Received chunk: ......

2025-04-01T18:18:10.354Z:
Received chunk:  I...

2025-04-01T18:18:10.442Z:
Received chunk:  should...

2025-04-01T18:18:10.533Z:
Received chunk:  introduce...

2025-04-01T18:18:10.621Z:
Received chunk:  myself...

2025-04-01T18:18:10.709Z:
Received chunk:  clearly...

2025-04-01T18:18:10.795Z:
Received chunk:  but...

2025-04-01T18:18:10.882Z:
Received chunk:  not...

2025-04-01T18:18:10.967Z:
Received chunk:  get...

2025-04-01T18:18:11.054Z:
Received chunk:  too...

2025-04-01T18:18:11.138Z:
Received chunk:  technical...

2025-04-01T18:18:11.221Z:
Received chunk: .

...

2025-04-01T18:18:11.305Z:
Received chunk: I...

2025-04-01T18:18:11.389Z:
Received chunk: 'll...

2025-04-01T18:18:11.475Z:
Received chunk:  mention...

2025-04-01T18:18:11.563Z:
Received chunk:  that...

2025-04-01T18:18:11.648Z:
Received chunk:  I...

2025-04-01T18:18:11.735Z:
Received chunk: 'm...

2025-04-01T18:18:11.820Z:
Received chunk:  an...

2025-04-01T18:18:11.907Z:
Received chunk:  AI...

2025-04-01T18:18:11.992Z:
Received chunk:  language...

2025-04-01T18:18:12.075Z:
Received chunk:  model...

2025-04-01T18:18:12.160Z:
Received chunk:  created...

2025-04-01T18:18:12.245Z:
Received chunk:  by...

2025-04-01T18:18:12.330Z:
Received chunk:  Deep...

2025-04-01T18:18:12.412Z:
Received chunk: Seek...

2025-04-01T18:18:12.495Z:
Received chunk: ....

2025-04-01T18:18:12.580Z:
Received chunk:  It...

2025-04-01T18:18:12.664Z:
Received chunk: 's...

2025-04-01T18:18:12.747Z:
Received chunk:  important...

2025-04-01T18:18:12.829Z:
Received chunk:  to...

2025-04-01T18:18:12.912Z:
Received chunk:  highlight...

2025-04-01T18:18:12.996Z:
Received chunk:  what...

2025-04-01T18:18:13.080Z:
Received chunk:  I...

2025-04-01T18:18:13.167Z:
Received chunk:  can...

2025-04-01T18:18:13.254Z:
Received chunk:  do...

2025-04-01T18:18:13.340Z:
Received chunk: ,...

2025-04-01T18:18:13.425Z:
Received chunk:  like...

2025-04-01T18:18:13.508Z:
Received chunk:  answering...

2025-04-01T18:18:13.592Z:
Received chunk:  questions...

2025-04-01T18:18:13.676Z:
Received chunk:  and...

2025-04-01T18:18:13.762Z:
Received chunk:  providing...

2025-04-01T18:18:13.847Z:
Received chunk:  information...

2025-04-01T18:18:13.931Z:
Received chunk: .

...

2025-04-01T18:18:14.015Z:
Received chunk: Also...

2025-04-01T18:18:14.099Z:
Received chunk: ,...

2025-04-01T18:18:14.183Z:
Received chunk:  I...

2025-04-01T18:18:14.271Z:
Received chunk:  should...

2025-04-01T18:18:14.356Z:
Received chunk:  emphasize...

2025-04-01T18:18:14.440Z:
Received chunk:  that...

2025-04-01T18:18:14.524Z:
Received chunk:  I...

2025-04-01T18:18:14.611Z:
Received chunk:  don...

2025-04-01T18:18:14.696Z:
Received chunk: 't...

2025-04-01T18:18:14.782Z:
Received chunk:  have...

2025-04-01T18:18:14.867Z:
Received chunk:  personal...

2025-04-01T18:18:14.953Z:
Received chunk:  experiences...

2025-04-01T18:18:15.038Z:
Received chunk:  or...

2025-04-01T18:18:15.120Z:
Received chunk:  emotions...

2025-04-01T18:18:15.206Z:
Received chunk:  since...

2025-04-01T18:18:15.292Z:
Received chunk:  I...

2025-04-01T18:18:15.380Z:
Received chunk: 'm...

2025-04-01T18:18:15.467Z:
Received chunk:  just...

2025-04-01T18:18:15.553Z:
Received chunk:  a...

2025-04-01T18:18:15.636Z:
Received chunk:  program...

2025-04-01T18:18:15.720Z:
Received chunk: ....

2025-04-01T18:18:15.806Z:
Received chunk:  That...

2025-04-01T18:18:15.891Z:
Received chunk:  sets...

2025-04-01T18:18:15.975Z:
Received chunk:  clear...

2025-04-01T18:18:16.058Z:
Received chunk:  boundaries...

2025-04-01T18:18:16.143Z:
Received chunk:  while...

2025-04-01T18:18:16.226Z:
Received chunk:  still...

2025-04-01T18:18:16.312Z:
Received chunk:  being...

2025-04-01T18:18:16.398Z:
Received chunk:  approach...

2025-04-01T18:18:16.484Z:
Received chunk: able...

2025-04-01T18:18:16.567Z:
Received chunk: .

...

2025-04-01T18:18:16.652Z:
Received chunk: Maybe...

2025-04-01T18:18:16.738Z:
Received chunk:  add...

2025-04-01T18:18:16.822Z:
Received chunk:  something...

2025-04-01T18:18:16.906Z:
Received chunk:  about...

2025-04-01T18:18:16.991Z:
Received chunk:  looking...

2025-04-01T18:18:17.076Z:
Received chunk:  forward...

2025-04-01T18:18:17.160Z:
Received chunk:  to...

2025-04-01T18:18:17.248Z:
Received chunk:  helping...

2025-04-01T18:18:17.333Z:
Received chunk:  them...

2025-04-01T18:18:17.420Z:
Received chunk:  today...

2025-04-01T18:18:17.506Z:
Received chunk: ....

2025-04-01T18:18:17.594Z:
Received chunk:  Keeping...

2025-04-01T18:18:17.681Z:
Received chunk:  it...

2025-04-01T18:18:17.770Z:
Received chunk:  positive...

2025-04-01T18:18:17.856Z:
Received chunk:  and...

2025-04-01T18:18:17.943Z:
Received chunk:  welcoming...

2025-04-01T18:18:18.028Z:
Received chunk: .
...

2025-04-01T18:18:18.111Z:
Received chunk: </think>...

2025-04-01T18:18:18.195Z:
Received chunk: 

...

2025-04-01T18:18:18.278Z:
Received chunk: Hi...

2025-04-01T18:18:18.365Z:
Received chunk: !...

2025-04-01T18:18:18.455Z:
Received chunk:  I...

2025-04-01T18:18:18.539Z:
Received chunk: 'm...

2025-04-01T18:18:18.627Z:
Received chunk:  Deep...

2025-04-01T18:18:18.713Z:
Received chunk: Seek...

2025-04-01T18:18:18.798Z:
Received chunk: -R...

2025-04-01T18:18:18.884Z:
Received chunk: 1...

2025-04-01T18:18:18.972Z:
Received chunk: -L...

2025-04-01T18:18:19.062Z:
Received chunk: ite...

2025-04-01T18:18:19.148Z:
Received chunk: -...

2025-04-01T18:18:19.232Z:
Received chunk: Preview...

2025-04-01T18:18:19.316Z:
Received chunk: ,...

2025-04-01T18:18:19.401Z:
Received chunk:  an...

2025-04-01T18:18:19.487Z:
Received chunk:  AI...

2025-04-01T18:18:19.571Z:
Received chunk:  assistant...

2025-04-01T18:18:19.655Z:
Received chunk:  created...

2025-04-01T18:18:19.741Z:
Received chunk:  by...

2025-04-01T18:18:19.827Z:
Received chunk:  the...

2025-04-01T18:18:19.914Z:
Received chunk:  Chinese...

2025-04-01T18:18:20.001Z:
Received chunk:  company...

2025-04-01T18:18:20.088Z:
Received chunk:  Deep...

2025-04-01T18:18:20.172Z:
Received chunk: Seek...

2025-04-01T18:18:20.257Z:
Received chunk: ....

2025-04-01T18:18:20.342Z:
Received chunk:  I...

2025-04-01T18:18:20.426Z:
Received chunk:  specialize...

2025-04-01T18:18:20.515Z:
Received chunk:  in...

2025-04-01T18:18:20.604Z:
Received chunk:  helping...

2025-04-01T18:18:20.689Z:
Received chunk:  you...

2025-04-01T18:18:20.779Z:
Received chunk:  with...

2025-04-01T18:18:20.869Z:
Received chunk:  questions...

2025-04-01T18:18:20.959Z:
Received chunk: ,...

2025-04-01T18:18:21.054Z:
Received chunk:  providing...

2025-04-01T18:18:21.143Z:
Received chunk:  information...

2025-04-01T18:18:21.226Z:
Received chunk: ,...

2025-04-01T18:18:21.310Z:
Received chunk:  and...

2025-04-01T18:18:21.400Z:
Received chunk:  assisting...

2025-04-01T18:18:21.486Z:
Received chunk:  with...

2025-04-01T18:18:21.570Z:
Received chunk:  various...

2025-04-01T18:18:21.656Z:
Received chunk:  tasks...

2025-04-01T18:18:21.743Z:
Received chunk: ....

2025-04-01T18:18:21.827Z:
Received chunk:  I...

2025-04-01T18:18:21.913Z:
Received chunk:  don...

2025-04-01T18:18:21.999Z:
Received chunk: 't...

2025-04-01T18:18:22.087Z:
Received chunk:  have...

2025-04-01T18:18:22.177Z:
Received chunk:  personal...

2025-04-01T18:18:22.271Z:
Received chunk:  experiences...

2025-04-01T18:18:22.356Z:
Received chunk:  or...

2025-04-01T18:18:22.441Z:
Received chunk:  emotions...

2025-04-01T18:18:22.526Z:
Received chunk: ,...

2025-04-01T18:18:22.609Z:
Received chunk:  but...

2025-04-01T18:18:22.694Z:
Received chunk:  I...

2025-04-01T18:18:22.780Z:
Received chunk: 'm...

2025-04-01T18:18:22.864Z:
Received chunk:  here...

2025-04-01T18:18:22.950Z:
Received chunk:  to...

2025-04-01T18:18:23.034Z:
Received chunk:  help...

2025-04-01T18:18:23.118Z:
Received chunk:  you...

2025-04-01T18:18:23.205Z:
Received chunk:  with...

2025-04-01T18:18:23.290Z:
Received chunk:  whatever...

2025-04-01T18:18:23.374Z:
Received chunk:  you...

2025-04-01T18:18:23.459Z:
Received chunk:  need...

2025-04-01T18:18:23.543Z:
Received chunk: ....

2025-04-01T18:18:23.627Z:
Received chunk:  How...

2025-04-01T18:18:23.712Z:
Received chunk:  can...

2025-04-01T18:18:23.798Z:
Received chunk:  I...

2025-04-01T18:18:23.883Z:
Received chunk:  assist...

2025-04-01T18:18:23.967Z:
Received chunk:  you...

2025-04-01T18:18:24.051Z:
Received chunk:  today...

2025-04-01T18:18:24.134Z:
Received chunk: ?...

2025-04-01T18:18:24.220Z:
Received chunk: 

> ...

2025-04-01T18:20:36.052Z:
Generate response called with message: who are you...

2025-04-01T18:20:36.053Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: who are you
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:20:36.096Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:20:36.120Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T18:20:36.142Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:20:36.152Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:20:36.175Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T18:20:36.227Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T18:20:36.228Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:20:36.244Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944


2025-04-01T18:20:36.245Z:
Process stderr: print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T18:20:36.529Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T18:20:36.553Z:
Process stderr: .

2025-04-01T18:20:36.573Z:
Process stderr: .

2025-04-01T18:20:36.595Z:
Process stderr: .

2025-04-01T18:20:36.614Z:
Process stderr: .

2025-04-01T18:20:36.634Z:
Process stderr: .

2025-04-01T18:20:36.655Z:
Process stderr: .

2025-04-01T18:20:36.675Z:
Process stderr: .

2025-04-01T18:20:36.708Z:
Process stderr: .

2025-04-01T18:20:36.715Z:
Process stderr: .

2025-04-01T18:20:36.758Z:
Process stderr: .

2025-04-01T18:20:36.779Z:
Process stderr: .

2025-04-01T18:20:36.832Z:
Process stderr: .

2025-04-01T18:20:36.867Z:
Process stderr: .

2025-04-01T18:20:36.898Z:
Process stderr: .

2025-04-01T18:20:36.921Z:
Process stderr: .

2025-04-01T18:20:36.945Z:
Process stderr: .

2025-04-01T18:20:36.976Z:
Process stderr: .

2025-04-01T18:20:36.996Z:
Process stderr: .

2025-04-01T18:20:37.017Z:
Process stderr: .

2025-04-01T18:20:37.046Z:
Process stderr: .

2025-04-01T18:20:37.067Z:
Process stderr: .

2025-04-01T18:20:37.168Z:
Process stderr: .

2025-04-01T18:20:37.190Z:
Process stderr: .

2025-04-01T18:20:37.219Z:
Process stderr: .

2025-04-01T18:20:37.245Z:
Process stderr: .

2025-04-01T18:20:37.286Z:
Process stderr: .

2025-04-01T18:20:37.320Z:
Process stderr: .

2025-04-01T18:20:37.345Z:
Process stderr: .

2025-04-01T18:20:37.379Z:
Process stderr: .

2025-04-01T18:20:37.418Z:
Process stderr: .

2025-04-01T18:20:37.440Z:
Process stderr: .

2025-04-01T18:20:37.471Z:
Process stderr: .

2025-04-01T18:20:37.494Z:
Process stderr: .

2025-04-01T18:20:37.517Z:
Process stderr: .

2025-04-01T18:20:37.551Z:
Process stderr: .

2025-04-01T18:20:37.591Z:
Process stderr: .

2025-04-01T18:20:37.622Z:
Process stderr: .

2025-04-01T18:20:37.657Z:
Process stderr: .

2025-04-01T18:20:37.684Z:
Process stderr: .

2025-04-01T18:20:37.718Z:
Process stderr: .

2025-04-01T18:20:37.766Z:
Process stderr: .

2025-04-01T18:20:37.776Z:
Process stderr: .

2025-04-01T18:20:37.840Z:
Process stderr: .

2025-04-01T18:20:37.849Z:
Process stderr: .

2025-04-01T18:20:37.888Z:
Process stderr: .

2025-04-01T18:20:37.906Z:
Process stderr: .

2025-04-01T18:20:37.936Z:
Process stderr: .

2025-04-01T18:20:37.956Z:
Process stderr: .

2025-04-01T18:20:37.980Z:
Process stderr: .

2025-04-01T18:20:38.033Z:
Process stderr: .

2025-04-01T18:20:38.052Z:
Process stderr: .

2025-04-01T18:20:38.081Z:
Process stderr: .

2025-04-01T18:20:38.101Z:
Process stderr: .

2025-04-01T18:20:38.125Z:
Process stderr: .

2025-04-01T18:20:38.156Z:
Process stderr: .

2025-04-01T18:20:38.174Z:
Process stderr: .

2025-04-01T18:20:38.197Z:
Process stderr: .

2025-04-01T18:20:38.219Z:
Process stderr: .

2025-04-01T18:20:38.240Z:
Process stderr: .

2025-04-01T18:20:38.262Z:
Process stderr: .

2025-04-01T18:20:38.284Z:
Process stderr: .

2025-04-01T18:20:38.322Z:
Process stderr: .

2025-04-01T18:20:38.329Z:
Process stderr: .

2025-04-01T18:20:38.363Z:
Process stderr: .

2025-04-01T18:20:38.370Z:
Process stderr: .

2025-04-01T18:20:38.408Z:
Process stderr: .....

2025-04-01T18:20:38.408Z:
Process stderr: ........

2025-04-01T18:20:38.409Z:
Process stderr: .....


2025-04-01T18:20:38.411Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T18:20:38.412Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T18:20:38.460Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T18:20:38.486Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T18:20:38.755Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T18:20:38.756Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T18:20:38.756Z:
Process stderr: main: interactive mode on.


2025-04-01T18:20:38.758Z:
Process stderr: sampler seed: 896187265
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T18:20:38.969Z:
Received chunk: <think>...

2025-04-01T18:20:39.056Z:
Received chunk: 

...

2025-04-01T18:20:39.141Z:
Received chunk: </think>...

2025-04-01T18:20:39.226Z:
Received chunk: 

...

2025-04-01T18:20:39.311Z:
Received chunk: Greetings...

2025-04-01T18:20:39.396Z:
Received chunk: !...

2025-04-01T18:20:39.483Z:
Received chunk:  I...

2025-04-01T18:20:39.569Z:
Received chunk: 'm...

2025-04-01T18:20:39.655Z:
Received chunk:  Deep...

2025-04-01T18:20:39.738Z:
Received chunk: Seek...

2025-04-01T18:20:39.823Z:
Received chunk: -R...

2025-04-01T18:20:39.906Z:
Received chunk: 1...

2025-04-01T18:20:39.989Z:
Received chunk: ,...

2025-04-01T18:20:40.074Z:
Received chunk:  an...

2025-04-01T18:20:40.158Z:
Received chunk:  artificial...

2025-04-01T18:20:40.244Z:
Received chunk:  intelligence...

2025-04-01T18:20:40.329Z:
Received chunk:  assistant...

2025-04-01T18:20:40.419Z:
Received chunk:  created...

2025-04-01T18:20:40.503Z:
Received chunk:  by...

2025-04-01T18:20:40.590Z:
Received chunk:  Deep...

2025-04-01T18:20:40.674Z:
Received chunk: Seek...

2025-04-01T18:20:40.760Z:
Received chunk: ....

2025-04-01T18:20:40.844Z:
Received chunk:  I...

2025-04-01T18:20:40.929Z:
Received chunk: 'm...

2025-04-01T18:20:41.013Z:
Received chunk:  at...

2025-04-01T18:20:41.098Z:
Received chunk:  your...

2025-04-01T18:20:41.182Z:
Received chunk:  service...

2025-04-01T18:20:41.266Z:
Received chunk:  and...

2025-04-01T18:20:41.351Z:
Received chunk:  would...

2025-04-01T18:20:41.436Z:
Received chunk:  be...

2025-04-01T18:20:41.521Z:
Received chunk:  delighted...

2025-04-01T18:20:41.605Z:
Received chunk:  to...

2025-04-01T18:20:41.691Z:
Received chunk:  assist...

2025-04-01T18:20:41.780Z:
Received chunk:  you...

2025-04-01T18:20:41.870Z:
Received chunk:  with...

2025-04-01T18:20:41.957Z:
Received chunk:  any...

2025-04-01T18:20:42.046Z:
Received chunk:  inquiries...

2025-04-01T18:20:42.134Z:
Received chunk:  or...

2025-04-01T18:20:42.219Z:
Received chunk:  tasks...

2025-04-01T18:20:42.304Z:
Received chunk:  you...

2025-04-01T18:20:42.391Z:
Received chunk:  may...

2025-04-01T18:20:42.478Z:
Received chunk:  have...

2025-04-01T18:20:42.561Z:
Received chunk: ....

2025-04-01T18:20:42.648Z:
Received chunk: 

> ...

2025-04-01T18:24:11.331Z:
Unloading model...

2025-04-01T18:24:11.332Z:
Terminating running process

2025-04-01T18:24:11.333Z:
Model unloaded successfully

2025-04-01T18:24:11.335Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T18:24:11.335Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:24:11.337Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T18:24:11.353Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T18:24:11.614Z:
Process exited with code null

2025-04-01T18:24:11.614Z:
Process failed, falling back to simulation

2025-04-01T18:24:11.614Z:
Generating simulated response

2025-04-01T18:24:11.704Z:
Sent simulated chunk: I'm

2025-04-01T18:24:11.766Z:
Sent simulated chunk: a

2025-04-01T18:24:11.827Z:
Sent simulated chunk: locally

2025-04-01T18:24:11.889Z:
Sent simulated chunk: hosted

2025-04-01T18:24:11.996Z:
Sent simulated chunk: AI

2025-04-01T18:24:12.074Z:
Sent simulated chunk: assistant

2025-04-01T18:24:12.166Z:
Sent simulated chunk: running

2025-04-01T18:24:12.260Z:
Sent simulated chunk: directly

2025-04-01T18:24:12.367Z:
Sent simulated chunk: on

2025-04-01T18:24:12.457Z:
Sent simulated chunk: your

2025-04-01T18:24:12.536Z:
Sent simulated chunk: device.

2025-04-01T18:24:12.597Z:
Sent simulated chunk: I

2025-04-01T18:24:12.644Z:
Sent simulated chunk: process

2025-04-01T18:24:12.739Z:
Sent simulated chunk: information

2025-04-01T18:24:12.800Z:
Sent simulated chunk: using

2025-04-01T18:24:12.847Z:
Sent simulated chunk: the

2025-04-01T18:24:12.954Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T18:24:13.064Z:
Sent simulated chunk: model

2025-04-01T18:24:13.126Z:
Sent simulated chunk: loaded

2025-04-01T18:24:13.174Z:
Sent simulated chunk: in

2025-04-01T18:24:13.267Z:
Sent simulated chunk: the

2025-04-01T18:24:13.313Z:
Sent simulated chunk: LM

2025-04-01T18:24:13.421Z:
Sent simulated chunk: Terminal

2025-04-01T18:24:13.500Z:
Sent simulated chunk: application.

2025-04-01T18:24:13.561Z:
Sent simulated chunk: I've

2025-04-01T18:24:13.622Z:
Sent simulated chunk: received

2025-04-01T18:24:13.714Z:
Sent simulated chunk: your

2025-04-01T18:24:13.807Z:
Sent simulated chunk: message:

2025-04-01T18:24:13.853Z:
Sent simulated chunk: "who

2025-04-01T18:24:13.945Z:
Sent simulated chunk: are

2025-04-01T18:24:14.037Z:
Sent simulated chunk: you"

2025-04-01T18:24:14.099Z:
Sent simulated chunk: and

2025-04-01T18:24:14.146Z:
Sent simulated chunk: am

2025-04-01T18:24:14.193Z:
Sent simulated chunk: responding

2025-04-01T18:24:14.283Z:
Sent simulated chunk: without

2025-04-01T18:24:14.392Z:
Sent simulated chunk: requiring

2025-04-01T18:24:14.469Z:
Sent simulated chunk: internet

2025-04-01T18:24:14.516Z:
Sent simulated chunk: access.

(This

2025-04-01T18:24:14.609Z:
Sent simulated chunk: response

2025-04-01T18:24:14.717Z:
Sent simulated chunk: was

2025-04-01T18:24:14.809Z:
Sent simulated chunk: generated

2025-04-01T18:24:14.948Z:
Sent simulated chunk: using

2025-04-01T18:24:14.993Z:
Sent simulated chunk: a

2025-04-01T18:24:15.150Z:
Sent simulated chunk: simulated

2025-04-01T18:24:15.276Z:
Sent simulated chunk: version

2025-04-01T18:24:15.335Z:
Sent simulated chunk: of

2025-04-01T18:24:15.429Z:
Sent simulated chunk: the

2025-04-01T18:24:15.460Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T18:24:15.571Z:
Sent simulated chunk: model)

2025-04-01T18:24:15.571Z:
Simulated response generation completed

2025-04-01T18:24:28.259Z:
Generate response called with message: Testing...

2025-04-01T18:24:28.260Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: Testing
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:24:28.370Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:24:28.405Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T18:24:28.428Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:24:28.437Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:24:28.461Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T18:24:28.514Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:24:28.531Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0


2025-04-01T18:24:28.532Z:
Process stderr: print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T18:24:31.459Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  7717.68 MiB
..

2025-04-01T18:24:31.461Z:
Process stderr: ......................................................................................


2025-04-01T18:24:31.463Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized


2025-04-01T18:24:31.463Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T18:24:31.479Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T18:24:31.484Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T18:24:38.375Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|im_start|>system
You are a helpful assistant<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
Hi there<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T18:24:38.390Z:
Process stderr: main: interactive mode on.


2025-04-01T18:24:38.391Z:
Process stderr: sampler seed: 322562633
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 0

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T18:24:48.591Z:
Received chunk: Hello...

2025-04-01T18:25:23.565Z:
Unloading model...

2025-04-01T18:25:23.796Z:
Terminating running process

2025-04-01T18:25:23.859Z:
Model unloaded successfully

2025-04-01T18:25:23.914Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:25:23.915Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:25:23.970Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:25:24.083Z:
Model mmproj-model-f16 file verified

2025-04-01T18:25:24.538Z:
Process exited with code null

2025-04-01T18:25:24.538Z:
Process failed, falling back to simulation

2025-04-01T18:25:24.590Z:
Generating simulated response

2025-04-01T18:25:24.659Z:
Sent simulated chunk: I'm

2025-04-01T18:25:24.739Z:
Sent simulated chunk: a

2025-04-01T18:25:24.782Z:
Sent simulated chunk: locally

2025-04-01T18:25:24.828Z:
Sent simulated chunk: hosted

2025-04-01T18:25:24.891Z:
Sent simulated chunk: AI

2025-04-01T18:25:24.953Z:
Sent simulated chunk: assistant

2025-04-01T18:25:25.032Z:
Sent simulated chunk: running

2025-04-01T18:25:25.093Z:
Sent simulated chunk: directly

2025-04-01T18:25:25.139Z:
Sent simulated chunk: on

2025-04-01T18:25:25.246Z:
Sent simulated chunk: your

2025-04-01T18:25:25.292Z:
Sent simulated chunk: device.

2025-04-01T18:25:25.370Z:
Sent simulated chunk: I

2025-04-01T18:25:25.447Z:
Sent simulated chunk: process

2025-04-01T18:25:25.509Z:
Sent simulated chunk: information

2025-04-01T18:25:25.602Z:
Sent simulated chunk: using

2025-04-01T18:25:25.682Z:
Sent simulated chunk: the

2025-04-01T18:25:25.759Z:
Sent simulated chunk: Qwen2.5-Coder-7B-Instruct-Q8_0

2025-04-01T18:25:25.820Z:
Sent simulated chunk: model

2025-04-01T18:25:25.912Z:
Sent simulated chunk: loaded

2025-04-01T18:25:25.936Z:
Unloading model...

2025-04-01T18:25:25.936Z:
Model unloaded successfully

2025-04-01T18:25:25.936Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:25:25.936Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:25:25.936Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:25:25.937Z:
Model mmproj-model-f16 file verified

2025-04-01T18:25:26.020Z:
Sent simulated chunk: in

2025-04-01T18:25:26.081Z:
Sent simulated chunk: the

2025-04-01T18:25:26.160Z:
Sent simulated chunk: LM

2025-04-01T18:25:26.235Z:
Sent simulated chunk: Terminal

2025-04-01T18:25:26.301Z:
Sent simulated chunk: application.

2025-04-01T18:25:26.375Z:
Sent simulated chunk: I've

2025-04-01T18:25:26.422Z:
Sent simulated chunk: received

2025-04-01T18:25:26.515Z:
Sent simulated chunk: your

2025-04-01T18:25:26.610Z:
Sent simulated chunk: message:

2025-04-01T18:25:26.672Z:
Sent simulated chunk: "Testing"

2025-04-01T18:25:26.748Z:
Sent simulated chunk: and

2025-04-01T18:25:26.825Z:
Sent simulated chunk: am

2025-04-01T18:25:26.904Z:
Sent simulated chunk: responding

2025-04-01T18:25:26.995Z:
Sent simulated chunk: without

2025-04-01T18:25:27.042Z:
Sent simulated chunk: requiring

2025-04-01T18:25:27.119Z:
Sent simulated chunk: internet

2025-04-01T18:25:27.196Z:
Sent simulated chunk: access.

(This

2025-04-01T18:25:27.257Z:
Sent simulated chunk: response

2025-04-01T18:25:27.305Z:
Sent simulated chunk: was

2025-04-01T18:25:27.383Z:
Sent simulated chunk: generated

2025-04-01T18:25:27.414Z:
Sent simulated chunk: using

2025-04-01T18:25:27.523Z:
Sent simulated chunk: a

2025-04-01T18:25:27.569Z:
Sent simulated chunk: simulated

2025-04-01T18:25:27.677Z:
Sent simulated chunk: version

2025-04-01T18:25:27.739Z:
Sent simulated chunk: of

2025-04-01T18:25:27.846Z:
Sent simulated chunk: the

2025-04-01T18:25:27.922Z:
Sent simulated chunk: Qwen2.5-Coder-7B-Instruct-Q8_0

2025-04-01T18:25:27.984Z:
Sent simulated chunk: model)

2025-04-01T18:25:27.985Z:
Simulated response generation completed

2025-04-01T18:25:32.408Z:
Generate response called with message: Test...

2025-04-01T18:25:32.408Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:25:32.518Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:25:32.528Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T18:25:32.528Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T18:25:32.530Z:
Process exited with code 1

2025-04-01T18:25:32.531Z:
Process failed, falling back to simulation

2025-04-01T18:25:32.531Z:
Generating simulated response

2025-04-01T18:25:32.573Z:
Sent simulated chunk: I'm

2025-04-01T18:25:32.621Z:
Sent simulated chunk: a

2025-04-01T18:25:32.682Z:
Sent simulated chunk: locally

2025-04-01T18:25:32.744Z:
Sent simulated chunk: hosted

2025-04-01T18:25:32.807Z:
Sent simulated chunk: AI

2025-04-01T18:25:32.899Z:
Sent simulated chunk: assistant

2025-04-01T18:25:32.977Z:
Sent simulated chunk: running

2025-04-01T18:25:33.054Z:
Sent simulated chunk: directly

2025-04-01T18:25:33.131Z:
Sent simulated chunk: on

2025-04-01T18:25:33.176Z:
Sent simulated chunk: your

2025-04-01T18:25:33.284Z:
Sent simulated chunk: device.

2025-04-01T18:25:33.363Z:
Sent simulated chunk: I

2025-04-01T18:25:33.473Z:
Sent simulated chunk: process

2025-04-01T18:25:33.551Z:
Sent simulated chunk: information

2025-04-01T18:25:33.658Z:
Sent simulated chunk: using

2025-04-01T18:25:33.751Z:
Sent simulated chunk: the

2025-04-01T18:25:33.812Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T18:25:33.858Z:
Sent simulated chunk: model

2025-04-01T18:25:33.951Z:
Sent simulated chunk: loaded

2025-04-01T18:25:34.043Z:
Sent simulated chunk: in

2025-04-01T18:25:34.121Z:
Sent simulated chunk: the

2025-04-01T18:25:34.196Z:
Sent simulated chunk: LM

2025-04-01T18:25:34.304Z:
Sent simulated chunk: Terminal

2025-04-01T18:25:34.369Z:
Sent simulated chunk: application.

2025-04-01T18:25:34.434Z:
Sent simulated chunk: I've

2025-04-01T18:25:34.478Z:
Sent simulated chunk: received

2025-04-01T18:25:34.574Z:
Sent simulated chunk: your

2025-04-01T18:25:34.653Z:
Sent simulated chunk: message:

2025-04-01T18:25:34.715Z:
Sent simulated chunk: "Test"

2025-04-01T18:25:34.762Z:
Sent simulated chunk: and

2025-04-01T18:25:34.810Z:
Sent simulated chunk: am

2025-04-01T18:25:34.887Z:
Sent simulated chunk: responding

2025-04-01T18:25:34.950Z:
Sent simulated chunk: without

2025-04-01T18:25:35.026Z:
Sent simulated chunk: requiring

2025-04-01T18:25:35.072Z:
Sent simulated chunk: internet

2025-04-01T18:25:35.135Z:
Sent simulated chunk: access.

(This

2025-04-01T18:25:35.213Z:
Sent simulated chunk: response

2025-04-01T18:25:35.322Z:
Sent simulated chunk: was

2025-04-01T18:25:35.401Z:
Sent simulated chunk: generated

2025-04-01T18:25:35.496Z:
Sent simulated chunk: using

2025-04-01T18:25:35.542Z:
Sent simulated chunk: a

2025-04-01T18:25:35.604Z:
Sent simulated chunk: simulated

2025-04-01T18:25:35.651Z:
Sent simulated chunk: version

2025-04-01T18:25:35.684Z:
Sent simulated chunk: of

2025-04-01T18:25:35.776Z:
Sent simulated chunk: the

2025-04-01T18:25:35.823Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T18:25:35.871Z:
Sent simulated chunk: model)

2025-04-01T18:25:35.871Z:
Simulated response generation completed

2025-04-01T18:26:00.009Z:
Unloading model...

2025-04-01T18:26:00.009Z:
Model unloaded successfully

2025-04-01T18:26:00.009Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T18:26:00.009Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:26:00.010Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T18:26:00.010Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T18:26:02.020Z:
Generate response called with message: test...

2025-04-01T18:26:02.021Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:26:02.058Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:26:02.084Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T18:26:02.107Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:26:02.116Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:26:02.139Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T18:26:02.189Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:26:02.207Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear


2025-04-01T18:26:02.207Z:
Process stderr: print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
2025-04-01T18:38:30.081Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:38:30.081Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:38:30.082Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:38:30.082Z:
Model mmproj-model-f16 file verified

2025-04-01T18:38:31.894Z:
Unloading model...

2025-04-01T18:38:31.894Z:
Model unloaded successfully

2025-04-01T18:38:31.895Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:38:31.895Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:38:31.895Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T18:38:31.895Z:
Model mmproj-model-f16 file verified

2025-04-01T18:38:36.627Z:
Generate response called with message: Hi how are you...

2025-04-01T18:38:36.628Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Hi how are you
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:38:36.917Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any
llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T18:38:36.918Z:
Process exited with code 1

2025-04-01T18:38:36.918Z:
Process failed, falling back to simulation

2025-04-01T18:38:36.919Z:
Generating simulated response

2025-04-01T18:38:37.008Z:
Sent simulated chunk: Hello!

2025-04-01T18:38:37.100Z:
Sent simulated chunk: I'm

2025-04-01T18:38:37.139Z:
Sent simulated chunk: a

2025-04-01T18:38:37.189Z:
Sent simulated chunk: local

2025-04-01T18:38:37.243Z:
Sent simulated chunk: AI

2025-04-01T18:38:37.289Z:
Sent simulated chunk: assistant

2025-04-01T18:38:37.400Z:
Sent simulated chunk: running

2025-04-01T18:38:37.477Z:
Sent simulated chunk: on

2025-04-01T18:38:37.540Z:
Sent simulated chunk: your

2025-04-01T18:38:37.618Z:
Sent simulated chunk: device

2025-04-01T18:38:37.664Z:
Sent simulated chunk: through

2025-04-01T18:38:37.710Z:
Sent simulated chunk: the

2025-04-01T18:38:37.772Z:
Sent simulated chunk: LM

2025-04-01T18:38:37.917Z:
Sent simulated chunk: Terminal.

2025-04-01T18:38:38.021Z:
Sent simulated chunk: How

2025-04-01T18:38:38.099Z:
Sent simulated chunk: can

2025-04-01T18:38:38.160Z:
Sent simulated chunk: I

2025-04-01T18:38:38.223Z:
Sent simulated chunk: help

2025-04-01T18:38:38.269Z:
Sent simulated chunk: you

2025-04-01T18:38:38.332Z:
Sent simulated chunk: today?

(This

2025-04-01T18:38:38.409Z:
Sent simulated chunk: response

2025-04-01T18:38:38.488Z:
Sent simulated chunk: was

2025-04-01T18:38:38.597Z:
Sent simulated chunk: generated

2025-04-01T18:38:38.708Z:
Sent simulated chunk: using

2025-04-01T18:38:38.754Z:
Sent simulated chunk: a

2025-04-01T18:38:38.925Z:
Sent simulated chunk: simulated

2025-04-01T18:38:39.006Z:
Sent simulated chunk: version

2025-04-01T18:38:39.054Z:
Sent simulated chunk: of

2025-04-01T18:38:39.099Z:
Sent simulated chunk: the

2025-04-01T18:38:39.186Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T18:38:39.274Z:
Sent simulated chunk: model)

2025-04-01T18:38:39.275Z:
Simulated response generation completed

2025-04-01T18:38:48.688Z:
Unloading model...

2025-04-01T18:38:48.688Z:
Model unloaded successfully

2025-04-01T18:38:48.688Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:38:48.688Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T18:38:48.689Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T18:38:48.689Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T18:38:54.260Z:
Generate response called with message: test...

2025-04-01T18:38:54.260Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T18:38:55.306Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T18:38:55.339Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T18:38:55.362Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T18:38:55.371Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T18:38:55.394Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T18:38:55.457Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T18:38:55.457Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T18:38:55.474Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00


2025-04-01T18:38:55.475Z:
Process stderr: print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
2025-04-01T19:34:02.042Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:34:02.042Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:34:02.043Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:34:02.043Z:
Model mmproj-model-f16 file verified

2025-04-01T19:34:04.651Z:
Unloading model...

2025-04-01T19:34:04.651Z:
Model unloaded successfully

2025-04-01T19:34:04.652Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:34:04.652Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:34:04.652Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:34:04.652Z:
Model mmproj-model-f16 file verified

2025-04-01T19:34:19.553Z:
Generate response called with message: test...

2025-04-01T19:34:19.554Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:34:19.670Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:34:19.678Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T19:34:19.679Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T19:34:19.681Z:
Process exited with code 1

2025-04-01T19:34:19.681Z:
Process failed, falling back to simulation

2025-04-01T19:34:19.682Z:
Generating simulated response

2025-04-01T19:34:19.778Z:
Sent simulated chunk: I'm

2025-04-01T19:34:19.887Z:
Sent simulated chunk: a

2025-04-01T19:34:19.919Z:
Sent simulated chunk: locally

2025-04-01T19:34:20.013Z:
Sent simulated chunk: hosted

2025-04-01T19:34:20.077Z:
Sent simulated chunk: AI

2025-04-01T19:34:20.189Z:
Sent simulated chunk: assistant

2025-04-01T19:34:20.268Z:
Sent simulated chunk: running

2025-04-01T19:34:20.362Z:
Sent simulated chunk: directly

2025-04-01T19:34:20.440Z:
Sent simulated chunk: on

2025-04-01T19:34:20.551Z:
Sent simulated chunk: your

2025-04-01T19:34:20.597Z:
Sent simulated chunk: device.

2025-04-01T19:34:20.660Z:
Sent simulated chunk: I

2025-04-01T19:34:20.734Z:
Sent simulated chunk: process

2025-04-01T19:34:20.816Z:
Sent simulated chunk: information

2025-04-01T19:34:20.860Z:
Sent simulated chunk: using

2025-04-01T19:34:20.897Z:
Sent simulated chunk: the

2025-04-01T19:34:20.990Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T19:34:21.101Z:
Sent simulated chunk: model

2025-04-01T19:34:21.149Z:
Sent simulated chunk: loaded

2025-04-01T19:34:21.195Z:
Sent simulated chunk: in

2025-04-01T19:34:21.290Z:
Sent simulated chunk: the

2025-04-01T19:34:21.355Z:
Sent simulated chunk: LM

2025-04-01T19:34:21.464Z:
Sent simulated chunk: Terminal

2025-04-01T19:34:21.559Z:
Sent simulated chunk: application.

2025-04-01T19:34:21.605Z:
Sent simulated chunk: I've

2025-04-01T19:34:21.713Z:
Sent simulated chunk: received

2025-04-01T19:34:21.760Z:
Sent simulated chunk: your

2025-04-01T19:34:21.800Z:
Sent simulated chunk: message:

2025-04-01T19:34:21.903Z:
Sent simulated chunk: "test"

2025-04-01T19:34:21.934Z:
Sent simulated chunk: and

2025-04-01T19:34:22.031Z:
Sent simulated chunk: am

2025-04-01T19:34:22.110Z:
Sent simulated chunk: responding

2025-04-01T19:34:22.189Z:
Sent simulated chunk: without

2025-04-01T19:34:22.267Z:
Sent simulated chunk: requiring

2025-04-01T19:34:22.345Z:
Sent simulated chunk: internet

2025-04-01T19:34:22.424Z:
Sent simulated chunk: access.

(This

2025-04-01T19:34:22.571Z:
Sent simulated chunk: response

2025-04-01T19:34:22.675Z:
Sent simulated chunk: was

2025-04-01T19:34:22.722Z:
Sent simulated chunk: generated

2025-04-01T19:34:22.824Z:
Sent simulated chunk: using

2025-04-01T19:34:22.910Z:
Sent simulated chunk: a

2025-04-01T19:34:22.973Z:
Sent simulated chunk: simulated

2025-04-01T19:34:23.035Z:
Sent simulated chunk: version

2025-04-01T19:34:23.082Z:
Sent simulated chunk: of

2025-04-01T19:34:23.129Z:
Sent simulated chunk: the

2025-04-01T19:34:23.192Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T19:34:23.254Z:
Sent simulated chunk: model)

2025-04-01T19:34:23.255Z:
Simulated response generation completed

2025-04-01T19:36:31.696Z:
Unloading model...

2025-04-01T19:36:31.696Z:
Model unloaded successfully

2025-04-01T19:36:31.696Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:36:31.696Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:36:31.697Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:36:31.697Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T19:36:39.591Z:
Generate response called with message: hi...

2025-04-01T19:36:39.592Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:36:39.734Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:36:39.770Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T19:36:39.792Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T19:36:39.802Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T19:36:39.825Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T19:36:39.883Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T19:36:39.884Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T19:36:39.901Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T19:36:41.632Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB
.....

2025-04-01T19:36:41.654Z:
Process stderr: .

2025-04-01T19:36:41.686Z:
Process stderr: .

2025-04-01T19:36:41.735Z:
Process stderr: .

2025-04-01T19:36:41.745Z:
Process stderr: .

2025-04-01T19:36:41.781Z:
Process stderr: .

2025-04-01T19:36:41.799Z:
Process stderr: .

2025-04-01T19:36:41.824Z:
Process stderr: .

2025-04-01T19:36:41.843Z:
Process stderr: .

2025-04-01T19:36:41.870Z:
Process stderr: .

2025-04-01T19:36:41.887Z:
Process stderr: .

2025-04-01T19:36:41.907Z:
Process stderr: .

2025-04-01T19:36:41.934Z:
Process stderr: .

2025-04-01T19:36:41.953Z:
Process stderr: .

2025-04-01T19:36:41.971Z:
Process stderr: .

2025-04-01T19:36:41.997Z:
Process stderr: .

2025-04-01T19:36:42.015Z:
Process stderr: .

2025-04-01T19:36:42.040Z:
Process stderr: .

2025-04-01T19:36:42.058Z:
Process stderr: .

2025-04-01T19:36:42.080Z:
Process stderr: .

2025-04-01T19:36:42.101Z:
Process stderr: .

2025-04-01T19:36:42.136Z:
Process stderr: .

2025-04-01T19:36:42.161Z:
Process stderr: .

2025-04-01T19:36:42.179Z:
Process stderr: .

2025-04-01T19:36:42.204Z:
Process stderr: .

2025-04-01T19:36:42.221Z:
Process stderr: .

2025-04-01T19:36:42.239Z:
Process stderr: .

2025-04-01T19:36:42.264Z:
Process stderr: .

2025-04-01T19:36:42.280Z:
Process stderr: .

2025-04-01T19:36:42.298Z:
Process stderr: .

2025-04-01T19:36:42.323Z:
Process stderr: .

2025-04-01T19:36:42.341Z:
Process stderr: .

2025-04-01T19:36:42.366Z:
Process stderr: .

2025-04-01T19:36:42.383Z:
Process stderr: .

2025-04-01T19:36:42.404Z:
Process stderr: .

2025-04-01T19:36:42.427Z:
Process stderr: .

2025-04-01T19:36:42.461Z:
Process stderr: .

2025-04-01T19:36:42.468Z:
Process stderr: .

2025-04-01T19:36:42.504Z:
Process stderr: .

2025-04-01T19:36:42.607Z:
Process stderr: ....

2025-04-01T19:36:42.610Z:
Process stderr: .

2025-04-01T19:36:42.629Z:
Process stderr: .

2025-04-01T19:36:42.674Z:
Process stderr: .

2025-04-01T19:36:42.698Z:
Process stderr: .

2025-04-01T19:36:42.727Z:
Process stderr: .

2025-04-01T19:36:42.745Z:
Process stderr: .

2025-04-01T19:36:42.765Z:
Process stderr: .

2025-04-01T19:36:42.794Z:
Process stderr: .

2025-04-01T19:36:42.812Z:
Process stderr: .

2025-04-01T19:36:42.836Z:
Process stderr: .

2025-04-01T19:36:42.860Z:
Process stderr: .

2025-04-01T19:36:42.884Z:
Process stderr: .

2025-04-01T19:36:42.909Z:
Process stderr: .

2025-04-01T19:36:42.933Z:
Process stderr: .

2025-04-01T19:36:42.974Z:
Process stderr: .

2025-04-01T19:36:42.981Z:
Process stderr: .

2025-04-01T19:36:43.021Z:
Process stderr: .

2025-04-01T19:36:43.029Z:
Process stderr: .

2025-04-01T19:36:43.069Z:
Process stderr: .....

2025-04-01T19:36:43.070Z:
Process stderr: .........

2025-04-01T19:36:43.070Z:
Process stderr: ....


2025-04-01T19:36:43.072Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T19:36:43.073Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T19:36:43.097Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T19:36:43.103Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T19:36:43.368Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T19:36:43.369Z:
Process stderr: main: interactive mode on.


2025-04-01T19:36:43.369Z:
Process stderr: sampler seed: 3100470656
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T19:36:43.644Z:
Received chunk: <think>...

2025-04-01T19:36:43.755Z:
Received chunk: 

...

2025-04-01T19:36:43.847Z:
Received chunk: </think>...

2025-04-01T19:36:43.948Z:
Received chunk: 

...

2025-04-01T19:36:44.061Z:
Received chunk: Hello...

2025-04-01T19:36:44.177Z:
Received chunk: !...

2025-04-01T19:36:44.269Z:
Received chunk:  How...

2025-04-01T19:36:44.359Z:
Received chunk:  can...

2025-04-01T19:36:44.447Z:
Received chunk:  I...

2025-04-01T19:36:44.665Z:
Received chunk:  assist you...

2025-04-01T19:36:44.760Z:
Received chunk:  today...

2025-04-01T19:36:44.857Z:
Received chunk: ?...

2025-04-01T19:36:44.956Z:
Received chunk:  �...

2025-04-01T19:36:45.054Z:
Received chunk: �...

2025-04-01T19:36:45.141Z:
Received chunk: 

> ...

2025-04-01T19:37:15.896Z:
Unloading model...

2025-04-01T19:37:15.896Z:
Terminating running process

2025-04-01T19:37:15.897Z:
Model unloaded successfully

2025-04-01T19:37:15.898Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:37:15.898Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:37:15.898Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:37:15.898Z:
Model mmproj-model-f16 file verified

2025-04-01T19:37:16.306Z:
Process exited with code null

2025-04-01T19:37:16.306Z:
Process failed, falling back to simulation

2025-04-01T19:37:16.307Z:
Generating simulated response

2025-04-01T19:37:16.398Z:
Sent simulated chunk: Hello!

2025-04-01T19:37:16.460Z:
Sent simulated chunk: I'm

2025-04-01T19:37:16.507Z:
Sent simulated chunk: a

2025-04-01T19:37:16.619Z:
Sent simulated chunk: local

2025-04-01T19:37:16.697Z:
Sent simulated chunk: AI

2025-04-01T19:37:16.758Z:
Sent simulated chunk: assistant

2025-04-01T19:37:16.897Z:
Sent simulated chunk: running

2025-04-01T19:37:16.944Z:
Sent simulated chunk: on

2025-04-01T19:37:16.976Z:
Sent simulated chunk: your

2025-04-01T19:37:17.038Z:
Sent simulated chunk: device

2025-04-01T19:37:17.116Z:
Sent simulated chunk: through

2025-04-01T19:37:17.208Z:
Sent simulated chunk: the

2025-04-01T19:37:17.317Z:
Sent simulated chunk: LM

2025-04-01T19:37:17.364Z:
Sent simulated chunk: Terminal.

2025-04-01T19:37:17.427Z:
Sent simulated chunk: How

2025-04-01T19:37:17.488Z:
Sent simulated chunk: can

2025-04-01T19:37:17.535Z:
Sent simulated chunk: I

2025-04-01T19:37:17.612Z:
Sent simulated chunk: help

2025-04-01T19:37:17.673Z:
Sent simulated chunk: you

2025-04-01T19:37:17.751Z:
Sent simulated chunk: today?

(This

2025-04-01T19:37:17.798Z:
Sent simulated chunk: response

2025-04-01T19:37:17.903Z:
Sent simulated chunk: was

2025-04-01T19:37:17.940Z:
Sent simulated chunk: generated

2025-04-01T19:37:18.033Z:
Sent simulated chunk: using

2025-04-01T19:37:18.081Z:
Sent simulated chunk: a

2025-04-01T19:37:18.190Z:
Sent simulated chunk: simulated

2025-04-01T19:37:18.236Z:
Sent simulated chunk: version

2025-04-01T19:37:18.282Z:
Sent simulated chunk: of

2025-04-01T19:37:18.345Z:
Sent simulated chunk: the

2025-04-01T19:37:18.392Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T19:37:18.439Z:
Sent simulated chunk: model)

2025-04-01T19:37:18.439Z:
Simulated response generation completed

2025-04-01T19:37:19.987Z:
Generate response called with message: hi...

2025-04-01T19:37:19.987Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:37:20.022Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:37:20.030Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T19:37:20.031Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T19:37:20.031Z:
Process exited with code 1

2025-04-01T19:37:20.032Z:
Process failed, falling back to simulation

2025-04-01T19:37:20.032Z:
Generating simulated response

2025-04-01T19:37:20.073Z:
Sent simulated chunk: Hello!

2025-04-01T19:37:20.144Z:
Sent simulated chunk: I'm

2025-04-01T19:37:20.180Z:
Sent simulated chunk: a

2025-04-01T19:37:20.230Z:
Sent simulated chunk: local

2025-04-01T19:37:20.292Z:
Sent simulated chunk: AI

2025-04-01T19:37:20.401Z:
Sent simulated chunk: assistant

2025-04-01T19:37:20.477Z:
Sent simulated chunk: running

2025-04-01T19:37:20.523Z:
Sent simulated chunk: on

2025-04-01T19:37:20.570Z:
Sent simulated chunk: your

2025-04-01T19:37:20.664Z:
Sent simulated chunk: device

2025-04-01T19:37:20.709Z:
Sent simulated chunk: through

2025-04-01T19:37:20.817Z:
Sent simulated chunk: the

2025-04-01T19:37:20.934Z:
Sent simulated chunk: LM

2025-04-01T19:37:20.989Z:
Sent simulated chunk: Terminal.

2025-04-01T19:37:21.067Z:
Sent simulated chunk: How

2025-04-01T19:37:21.130Z:
Sent simulated chunk: can

2025-04-01T19:37:21.173Z:
Sent simulated chunk: I

2025-04-01T19:37:21.255Z:
Sent simulated chunk: help

2025-04-01T19:37:21.365Z:
Sent simulated chunk: you

2025-04-01T19:37:21.442Z:
Sent simulated chunk: today?

(This

2025-04-01T19:37:21.503Z:
Sent simulated chunk: response

2025-04-01T19:37:21.551Z:
Sent simulated chunk: was

2025-04-01T19:37:21.630Z:
Sent simulated chunk: generated

2025-04-01T19:37:21.739Z:
Sent simulated chunk: using

2025-04-01T19:37:21.802Z:
Sent simulated chunk: a

2025-04-01T19:37:21.936Z:
Sent simulated chunk: simulated

2025-04-01T19:37:22.005Z:
Sent simulated chunk: version

2025-04-01T19:37:22.106Z:
Sent simulated chunk: of

2025-04-01T19:37:22.173Z:
Sent simulated chunk: the

2025-04-01T19:37:22.253Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T19:37:22.315Z:
Sent simulated chunk: model)

2025-04-01T19:37:22.316Z:
Simulated response generation completed

2025-04-01T19:39:41.876Z:
Unloading model...

2025-04-01T19:39:41.876Z:
Model unloaded successfully

2025-04-01T19:39:41.876Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T19:39:41.877Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:39:41.877Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T19:39:41.877Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T19:39:43.145Z:
Unloading model...

2025-04-01T19:39:43.145Z:
Model unloaded successfully

2025-04-01T19:39:43.145Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T19:39:43.146Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:39:43.146Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T19:39:43.146Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T19:39:47.020Z:
Generate response called with message: hi...

2025-04-01T19:39:47.021Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf --prompt USER: hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:39:47.050Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:39:47.084Z:
Process stderr: llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It
llama_model_loader: - kv   3:                           general.finetune str              = it
llama_model_loader: - kv   4:                           general.basename str              = gemma-3
llama_model_loader: - kv   5:                         general.size_label str              = 4B
llama_model_loader: - kv   6:                            general.license str              = gemma
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Google
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...
llama_model_loader: - kv  11:                               general.tags arr[str,1]       = ["image-text-to-text"]
llama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072
llama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560
llama_model_loader: - kv  14:                         gemma3.block_count u32              = 34
llama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240
llama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8
llama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256
llama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256
llama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024
llama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4
llama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear
llama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000
llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default


2025-04-01T19:39:47.122Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...


2025-04-01T19:39:47.204Z:
Process stderr: llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...


2025-04-01T19:39:47.220Z:
Process stderr: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\n{%- if messages[0]['r...
llama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  38:               general.quantization_version u32              = 2
llama_model_loader: - kv  39:                          general.file_type u32              = 7
llama_model_loader: - type  f32:  205 tensors
llama_model_loader: - type q8_0:  239 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 3.84 GiB (8.50 BPW) 


2025-04-01T19:39:47.328Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 6414
load: token to piece cache size = 1.9446 MB
print_info: arch             = gemma3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2560
print_info: n_layer          = 34
print_info: n_head           = 8
print_info: n_head_kv        = 4
print_info: n_rot            = 256
print_info: n_swa            = 1024
print_info: n_swa_pattern    = 6
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 6.2e-02
print_info: n_ff             = 10240
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.125
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 4B
print_info: model params     = 3.88 B
print_info: general.name     = Gemma 3 4b It
print_info: vocab type       = SPM
print_info: n_vocab          = 262144
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 106 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 248 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 106 '<end_of_turn>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T19:39:49.419Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/35 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  3932.65 MiB
....................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.125
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.00 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 34, can_shift = 1


2025-04-01T19:39:49.440Z:
Process stderr: init:        CPU KV buffer size =   272.00 MiB
llama_context: KV self size  =  272.00 MiB, K (f16):  136.00 MiB, V (f16):  136.00 MiB


2025-04-01T19:39:49.451Z:
Process stderr: llama_context:        CPU compute buffer size =   522.00 MiB
llama_context: graph nodes  = 1435
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T19:39:49.918Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<start_of_turn>user
You are a helpful assistant

Hello<end_of_turn>
<start_of_turn>model
Hi there<end_of_turn>
<start_of_turn>user
How are you?<end_of_turn>
<start_of_turn>model


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T19:39:49.918Z:
Process stderr: main: interactive mode on.


2025-04-01T19:39:49.919Z:
Process stderr: sampler seed: 2704147084
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T19:39:50.114Z:
Received chunk: Hello...

2025-04-01T19:39:50.195Z:
Received chunk:  there...

2025-04-01T19:39:50.432Z:
Received chunk: ! How can...

2025-04-01T19:39:50.558Z:
Received chunk:  I...

2025-04-01T19:39:50.636Z:
Received chunk:  help...

2025-04-01T19:39:50.723Z:
Received chunk:  you...

2025-04-01T19:39:50.807Z:
Received chunk:  today...

2025-04-01T19:39:50.892Z:
Received chunk: ?...

2025-04-01T19:39:50.964Z:
Received chunk:  😊...

2025-04-01T19:39:51.035Z:
Received chunk:  ...

2025-04-01T19:39:51.103Z:
Received chunk: 

...

2025-04-01T19:39:51.171Z:
Received chunk: Do...

2025-04-01T19:39:51.239Z:
Received chunk:  you...

2025-04-01T19:39:51.438Z:
Received chunk:  want to...

2025-04-01T19:39:51.453Z:
Received chunk: :...

2025-04-01T19:39:51.586Z:
Received chunk: 

...

2025-04-01T19:39:51.664Z:
Received chunk: *...

2025-04-01T19:39:51.745Z:
Received chunk:    ...

2025-04-01T19:39:51.829Z:
Received chunk: Chat...

2025-04-01T19:39:51.909Z:
Received chunk:  about...

2025-04-01T19:39:51.982Z:
Received chunk:  something...

2025-04-01T19:39:52.053Z:
Received chunk: ?...

2025-04-01T19:39:52.122Z:
Received chunk: 
...

2025-04-01T19:39:52.192Z:
Received chunk: *...

2025-04-01T19:39:52.261Z:
Received chunk:    ...

2025-04-01T19:39:52.453Z:
Received chunk: Ask me...

2025-04-01T19:39:52.480Z:
Received chunk:  a...

2025-04-01T19:39:52.612Z:
Received chunk:  question...

2025-04-01T19:39:52.694Z:
Received chunk: ?...

2025-04-01T19:39:52.784Z:
Received chunk: 
...

2025-04-01T19:39:52.873Z:
Received chunk: *...

2025-04-01T19:39:52.961Z:
Received chunk:    ...

2025-04-01T19:39:53.037Z:
Received chunk: Have...

2025-04-01T19:39:53.114Z:
Received chunk:  me...

2025-04-01T19:39:53.186Z:
Received chunk:  write...

2025-04-01T19:39:53.259Z:
Received chunk:  something...

2025-04-01T19:39:53.473Z:
Received chunk:  for you...

2025-04-01T19:39:53.484Z:
Received chunk:  (...

2025-04-01T19:39:53.619Z:
Received chunk: like...

2025-04-01T19:39:53.700Z:
Received chunk:  a...

2025-04-01T19:39:53.778Z:
Received chunk:  story...

2025-04-01T19:39:53.856Z:
Received chunk: ,...

2025-04-01T19:39:53.938Z:
Received chunk:  poem...

2025-04-01T19:39:54.012Z:
Received chunk: ,...

2025-04-01T19:39:54.081Z:
Received chunk:  or...

2025-04-01T19:39:54.150Z:
Received chunk:  email...

2025-04-01T19:39:54.218Z:
Received chunk: )?...

2025-04-01T19:39:54.402Z:
Received chunk: 

> ...

2025-04-01T19:49:05.268Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:49:05.268Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:49:05.268Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:49:05.269Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T19:49:07.031Z:
Unloading model...

2025-04-01T19:49:07.031Z:
Model unloaded successfully

2025-04-01T19:49:07.032Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:49:07.032Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:49:07.032Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T19:49:07.033Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T19:49:35.078Z:
Generate response called with message: hi friend...

2025-04-01T19:49:35.078Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: hi friend
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:49:35.106Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:49:35.135Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T19:49:35.157Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T19:49:35.166Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T19:49:35.189Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T19:49:35.240Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T19:49:35.241Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T19:49:35.257Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2


2025-04-01T19:49:35.258Z:
Process stderr: print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T19:49:35.516Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T19:49:35.541Z:
Process stderr: .

2025-04-01T19:49:35.567Z:
Process stderr: .

2025-04-01T19:49:35.588Z:
Process stderr: .

2025-04-01T19:49:35.608Z:
Process stderr: .

2025-04-01T19:49:35.628Z:
Process stderr: .

2025-04-01T19:49:35.647Z:
Process stderr: .

2025-04-01T19:49:35.670Z:
Process stderr: .

2025-04-01T19:49:35.705Z:
Process stderr: .

2025-04-01T19:49:35.713Z:
Process stderr: .

2025-04-01T19:49:35.749Z:
Process stderr: .

2025-04-01T19:49:35.768Z:
Process stderr: .

2025-04-01T19:49:35.793Z:
Process stderr: .

2025-04-01T19:49:35.812Z:
Process stderr: .

2025-04-01T19:49:35.840Z:
Process stderr: .

2025-04-01T19:49:35.859Z:
Process stderr: .

2025-04-01T19:49:35.879Z:
Process stderr: .

2025-04-01T19:49:35.908Z:
Process stderr: .

2025-04-01T19:49:35.928Z:
Process stderr: .

2025-04-01T19:49:35.955Z:
Process stderr: .

2025-04-01T19:49:35.983Z:
Process stderr: .

2025-04-01T19:49:36.003Z:
Process stderr: .

2025-04-01T19:49:36.031Z:
Process stderr: .

2025-04-01T19:49:36.048Z:
Process stderr: .

2025-04-01T19:49:36.069Z:
Process stderr: .

2025-04-01T19:49:36.090Z:
Process stderr: .

2025-04-01T19:49:36.125Z:
Process stderr: .

2025-04-01T19:49:36.149Z:
Process stderr: .

2025-04-01T19:49:36.167Z:
Process stderr: .

2025-04-01T19:49:36.192Z:
Process stderr: .

2025-04-01T19:49:36.208Z:
Process stderr: .

2025-04-01T19:49:36.226Z:
Process stderr: .

2025-04-01T19:49:36.252Z:
Process stderr: .

2025-04-01T19:49:36.268Z:
Process stderr: .

2025-04-01T19:49:36.285Z:
Process stderr: .

2025-04-01T19:49:36.310Z:
Process stderr: .

2025-04-01T19:49:36.328Z:
Process stderr: .

2025-04-01T19:49:36.353Z:
Process stderr: .

2025-04-01T19:49:36.370Z:
Process stderr: .

2025-04-01T19:49:36.394Z:
Process stderr: .

2025-04-01T19:49:36.416Z:
Process stderr: .

2025-04-01T19:49:36.518Z:
Process stderr: ....

2025-04-01T19:49:36.541Z:
Process stderr: .

2025-04-01T19:49:36.564Z:
Process stderr: .

2025-04-01T19:49:36.594Z:
Process stderr: .

2025-04-01T19:49:36.612Z:
Process stderr: .

2025-04-01T19:49:36.631Z:
Process stderr: .

2025-04-01T19:49:36.657Z:
Process stderr: .

2025-04-01T19:49:36.676Z:
Process stderr: .

2025-04-01T19:49:36.702Z:
Process stderr: .

2025-04-01T19:49:36.720Z:
Process stderr: .

2025-04-01T19:49:36.738Z:
Process stderr: .

2025-04-01T19:49:36.766Z:
Process stderr: .

2025-04-01T19:49:36.784Z:
Process stderr: .

2025-04-01T19:49:36.808Z:
Process stderr: .

2025-04-01T19:49:36.833Z:
Process stderr: .

2025-04-01T19:49:36.859Z:
Process stderr: .

2025-04-01T19:49:36.886Z:
Process stderr: .

2025-04-01T19:49:36.911Z:
Process stderr: .

2025-04-01T19:49:36.955Z:
Process stderr: .

2025-04-01T19:49:36.963Z:
Process stderr: .

2025-04-01T19:49:37.002Z:
Process stderr: .

2025-04-01T19:49:37.010Z:
Process stderr: .

2025-04-01T19:49:37.049Z:
Process stderr: ...

2025-04-01T19:49:37.049Z:
Process stderr: ...........

2025-04-01T19:49:37.049Z:
Process stderr: ....


2025-04-01T19:49:37.051Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T19:49:37.052Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T19:49:37.068Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T19:49:37.073Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T19:49:37.318Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T19:49:37.319Z:
Process stderr: main: interactive mode on.


2025-04-01T19:49:37.319Z:
Process stderr: sampler seed: 602532850
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T19:49:37.573Z:
Received chunk: <think>...

2025-04-01T19:49:37.648Z:
Received chunk: 

...

2025-04-01T19:49:37.748Z:
Received chunk: </think>...

2025-04-01T19:49:37.845Z:
Received chunk: 

...

2025-04-01T19:49:37.940Z:
Received chunk: Hi...

2025-04-01T19:49:38.034Z:
Received chunk:  friend...

2025-04-01T19:49:38.115Z:
Received chunk: !...

2025-04-01T19:49:38.198Z:
Received chunk:  How...

2025-04-01T19:49:38.280Z:
Received chunk:  can...

2025-04-01T19:49:38.361Z:
Received chunk:  I...

2025-04-01T19:49:38.582Z:
Received chunk:  assist you...

2025-04-01T19:49:38.674Z:
Received chunk:  today...

2025-04-01T19:49:38.775Z:
Received chunk: ?...

2025-04-01T19:49:38.879Z:
Received chunk:  �...

2025-04-01T19:49:38.984Z:
Received chunk: �...

2025-04-01T19:49:39.070Z:
Received chunk: 

> ...

2025-04-01T19:49:46.965Z:
Unloading model...

2025-04-01T19:49:46.965Z:
Terminating running process

2025-04-01T19:49:46.965Z:
Model unloaded successfully

2025-04-01T19:49:46.966Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:49:46.966Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T19:49:46.967Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T19:49:46.968Z:
Model mmproj-model-f16 file verified

2025-04-01T19:49:47.429Z:
Process exited with code null

2025-04-01T19:49:47.429Z:
Process failed, falling back to simulation

2025-04-01T19:49:47.429Z:
Generating simulated response

2025-04-01T19:49:47.472Z:
Sent simulated chunk: Hello!

2025-04-01T19:49:47.520Z:
Sent simulated chunk: I'm

2025-04-01T19:49:47.623Z:
Sent simulated chunk: a

2025-04-01T19:49:47.721Z:
Sent simulated chunk: local

2025-04-01T19:49:47.800Z:
Sent simulated chunk: AI

2025-04-01T19:49:47.871Z:
Sent simulated chunk: assistant

2025-04-01T19:49:47.942Z:
Sent simulated chunk: running

2025-04-01T19:49:48.054Z:
Sent simulated chunk: on

2025-04-01T19:49:48.114Z:
Sent simulated chunk: your

2025-04-01T19:49:48.208Z:
Sent simulated chunk: device

2025-04-01T19:49:48.270Z:
Sent simulated chunk: through

2025-04-01T19:49:48.347Z:
Sent simulated chunk: the

2025-04-01T19:49:48.439Z:
Sent simulated chunk: LM

2025-04-01T19:49:48.517Z:
Sent simulated chunk: Terminal.

2025-04-01T19:49:48.621Z:
Sent simulated chunk: How

2025-04-01T19:49:48.674Z:
Sent simulated chunk: can

2025-04-01T19:49:48.737Z:
Sent simulated chunk: I

2025-04-01T19:49:48.799Z:
Sent simulated chunk: help

2025-04-01T19:49:48.854Z:
Sent simulated chunk: you

2025-04-01T19:49:48.954Z:
Sent simulated chunk: today?

(This

2025-04-01T19:49:49.033Z:
Sent simulated chunk: response

2025-04-01T19:49:49.096Z:
Sent simulated chunk: was

2025-04-01T19:49:49.189Z:
Sent simulated chunk: generated

2025-04-01T19:49:49.252Z:
Sent simulated chunk: using

2025-04-01T19:49:49.314Z:
Sent simulated chunk: a

2025-04-01T19:49:49.392Z:
Sent simulated chunk: simulated

2025-04-01T19:49:49.469Z:
Sent simulated chunk: version

2025-04-01T19:49:49.633Z:
Sent simulated chunk: of

2025-04-01T19:49:49.672Z:
Sent simulated chunk: the

2025-04-01T19:49:49.750Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T19:49:49.828Z:
Sent simulated chunk: model)

2025-04-01T19:49:49.829Z:
Simulated response generation completed

2025-04-01T19:49:50.162Z:
Generate response called with message: hello...

2025-04-01T19:49:50.162Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: hello
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T19:49:50.185Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T19:49:50.191Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T19:49:50.191Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T19:49:50.192Z:
Process exited with code 1

2025-04-01T19:49:50.192Z:
Process failed, falling back to simulation

2025-04-01T19:49:50.193Z:
Generating simulated response

2025-04-01T19:49:50.281Z:
Sent simulated chunk: Hello!

2025-04-01T19:49:50.374Z:
Sent simulated chunk: I'm

2025-04-01T19:49:50.453Z:
Sent simulated chunk: a

2025-04-01T19:49:50.530Z:
Sent simulated chunk: local

2025-04-01T19:49:50.653Z:
Sent simulated chunk: AI

2025-04-01T19:49:50.717Z:
Sent simulated chunk: assistant

2025-04-01T19:49:50.800Z:
Sent simulated chunk: running

2025-04-01T19:49:50.852Z:
Sent simulated chunk: on

2025-04-01T19:49:50.952Z:
Sent simulated chunk: your

2025-04-01T19:49:51.014Z:
Sent simulated chunk: device

2025-04-01T19:49:51.060Z:
Sent simulated chunk: through

2025-04-01T19:49:51.107Z:
Sent simulated chunk: the

2025-04-01T19:49:51.154Z:
Sent simulated chunk: LM

2025-04-01T19:49:51.216Z:
Sent simulated chunk: Terminal.

2025-04-01T19:49:51.293Z:
Sent simulated chunk: How

2025-04-01T19:49:51.324Z:
Sent simulated chunk: can

2025-04-01T19:49:51.434Z:
Sent simulated chunk: I

2025-04-01T19:49:51.542Z:
Sent simulated chunk: help

2025-04-01T19:49:51.661Z:
Sent simulated chunk: you

2025-04-01T19:49:51.759Z:
Sent simulated chunk: today?

(This

2025-04-01T19:49:51.812Z:
Sent simulated chunk: response

2025-04-01T19:49:51.906Z:
Sent simulated chunk: was

2025-04-01T19:49:52.008Z:
Sent simulated chunk: generated

2025-04-01T19:49:52.103Z:
Sent simulated chunk: using

2025-04-01T19:49:52.180Z:
Sent simulated chunk: a

2025-04-01T19:49:52.258Z:
Sent simulated chunk: simulated

2025-04-01T19:49:52.351Z:
Sent simulated chunk: version

2025-04-01T19:49:52.398Z:
Sent simulated chunk: of

2025-04-01T19:49:52.491Z:
Sent simulated chunk: the

2025-04-01T19:49:52.675Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T19:49:52.773Z:
Sent simulated chunk: model)

2025-04-01T19:49:52.773Z:
Simulated response generation completed

2025-04-01T20:07:24.260Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:07:24.260Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:07:24.260Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:07:24.261Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:07:24.948Z:
Unloading model...

2025-04-01T20:07:24.948Z:
Model unloaded successfully

2025-04-01T20:07:24.948Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:07:24.948Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:07:24.949Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:07:24.949Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:21:58.727Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:21:58.728Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:21:58.728Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:21:58.728Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T20:21:59.880Z:
Unloading model...

2025-04-01T20:21:59.880Z:
Model unloaded successfully

2025-04-01T20:21:59.881Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:21:59.881Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:21:59.881Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:21:59.881Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T20:22:08.372Z:
Generate response called with message: test...

2025-04-01T20:22:08.372Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:22:08.397Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:22:08.429Z:
Process stderr: llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It
llama_model_loader: - kv   3:                           general.finetune str              = it
llama_model_loader: - kv   4:                           general.basename str              = gemma-3
llama_model_loader: - kv   5:                         general.size_label str              = 4B
llama_model_loader: - kv   6:                            general.license str              = gemma
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Google
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...
llama_model_loader: - kv  11:                               general.tags arr[str,1]       = ["image-text-to-text"]
llama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072
llama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560
llama_model_loader: - kv  14:                         gemma3.block_count u32              = 34
llama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240
llama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8
llama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256
llama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256
llama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024
llama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4
llama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear
llama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000
llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama


2025-04-01T20:22:08.429Z:
Process stderr: llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default


2025-04-01T20:22:08.466Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...


2025-04-01T20:22:08.551Z:
Process stderr: llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...


2025-04-01T20:22:08.567Z:
Process stderr: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\n{%- if messages[0]['r...
llama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  38:               general.quantization_version u32              = 2
llama_model_loader: - kv  39:                          general.file_type u32              = 7
llama_model_loader: - type  f32:  205 tensors
llama_model_loader: - type q8_0:  239 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 3.84 GiB (8.50 BPW) 


2025-04-01T20:22:08.612Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T20:22:08.612Z:
Process stderr: load: special tokens cache size = 6414


2025-04-01T20:22:08.625Z:
Process stderr: load: token to piece cache size = 1.9446 MB
print_info: arch             = gemma3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2560
print_info: n_layer          = 34


2025-04-01T20:22:08.626Z:
Process stderr: print_info: n_head           = 8
print_info: n_head_kv        = 4
print_info: n_rot            = 256
print_info: n_swa            = 1024
print_info: n_swa_pattern    = 6
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 6.2e-02
print_info: n_ff             = 10240
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.125
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 4B
print_info: model params     = 3.88 B
print_info: general.name     = Gemma 3 4b It
print_info: vocab type       = SPM
print_info: n_vocab          = 262144
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 106 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 248 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 106 '<end_of_turn>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:22:08.814Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/35 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  3932.65 MiB


2025-04-01T20:22:08.814Z:
Process stderr: ............................

2025-04-01T20:22:08.814Z:
Process stderr: ..............

2025-04-01T20:22:08.815Z:
Process stderr: ................

2025-04-01T20:22:08.816Z:
Process stderr: ..........................


2025-04-01T20:22:08.817Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.125
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T20:22:08.817Z:
Process stderr: llama_context:        CPU  output buffer size =     1.00 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 34, can_shift = 1


2025-04-01T20:22:08.870Z:
Process stderr: init:        CPU KV buffer size =   272.00 MiB
llama_context: KV self size  =  272.00 MiB, K (f16):  136.00 MiB, V (f16):  136.00 MiB


2025-04-01T20:22:08.875Z:
Process stderr: llama_context:        CPU compute buffer size =   522.00 MiB
llama_context: graph nodes  = 1435
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:22:09.284Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<start_of_turn>user
You are a helpful assistant

Hello<end_of_turn>
<start_of_turn>model
Hi there<end_of_turn>
<start_of_turn>user
How are you?<end_of_turn>
<start_of_turn>model


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T20:22:09.285Z:
Process stderr: main: interactive mode on.


2025-04-01T20:22:09.285Z:
Process stderr: sampler seed: 153559820
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:22:09.470Z:
Received chunk: Okay...

2025-04-01T20:22:09.540Z:
Received chunk: ,...

2025-04-01T20:22:09.609Z:
Received chunk:  I...

2025-04-01T20:22:09.679Z:
Received chunk:  received...

2025-04-01T20:22:09.748Z:
Received chunk:  your...

2025-04-01T20:22:09.817Z:
Received chunk:  "...

2025-04-01T20:22:09.886Z:
Received chunk: test...

2025-04-01T20:22:09.957Z:
Received chunk: "!...

2025-04-01T20:22:10.027Z:
Received chunk:  ...

2025-04-01T20:22:10.099Z:
Received chunk: 

...

2025-04-01T20:22:10.171Z:
Received chunk: Is...

2025-04-01T20:22:10.247Z:
Received chunk:  there...

2025-04-01T20:22:10.315Z:
Received chunk:  anything...

2025-04-01T20:22:10.386Z:
Received chunk:  specific...

2025-04-01T20:22:10.456Z:
Received chunk:  you...

2025-04-01T20:22:10.527Z:
Received chunk: ’...

2025-04-01T20:22:10.597Z:
Received chunk: d...

2025-04-01T20:22:10.667Z:
Received chunk:  like...

2025-04-01T20:22:10.737Z:
Received chunk:  me...

2025-04-01T20:22:10.805Z:
Received chunk:  to...

2025-04-01T20:22:10.875Z:
Received chunk:  do...

2025-04-01T20:22:10.945Z:
Received chunk:  with...

2025-04-01T20:22:11.017Z:
Received chunk:  that...

2025-04-01T20:22:11.092Z:
Received chunk: ?...

2025-04-01T20:22:11.165Z:
Received chunk:  Do...

2025-04-01T20:22:11.243Z:
Received chunk:  you...

2025-04-01T20:22:11.314Z:
Received chunk:  want...

2025-04-01T20:22:11.388Z:
Received chunk:  me...

2025-04-01T20:22:11.459Z:
Received chunk:  to...

2025-04-01T20:22:11.529Z:
Received chunk: :...

2025-04-01T20:22:11.600Z:
Received chunk: 

...

2025-04-01T20:22:11.670Z:
Received chunk: *...

2025-04-01T20:22:11.740Z:
Received chunk:    ...

2025-04-01T20:22:11.817Z:
Received chunk: Respond...

2025-04-01T20:22:11.892Z:
Received chunk:  in...

2025-04-01T20:22:11.968Z:
Received chunk:  a...

2025-04-01T20:22:12.046Z:
Received chunk:  certain...

2025-04-01T20:22:12.122Z:
Received chunk:  way...

2025-04-01T20:22:12.197Z:
Received chunk: ?...

2025-04-01T20:22:12.274Z:
Received chunk:  (...

2025-04-01T20:22:12.350Z:
Received chunk: e...

2025-04-01T20:22:12.428Z:
Received chunk: ....

2025-04-01T20:22:12.508Z:
Received chunk: g...

2025-04-01T20:22:12.583Z:
Received chunk: .,...

2025-04-01T20:22:12.657Z:
Received chunk:  “...

2025-04-01T20:22:12.729Z:
Received chunk: That...

2025-04-01T20:22:12.799Z:
Received chunk: '...

2025-04-01T20:22:12.871Z:
Received chunk: s...

2025-04-01T20:22:12.942Z:
Received chunk:  a...

2025-04-01T20:22:13.016Z:
Received chunk:  good...

2025-04-01T20:22:13.089Z:
Received chunk:  test...

2025-04-01T20:22:13.164Z:
Received chunk: !”...

2025-04-01T20:22:13.238Z:
Received chunk: )...

2025-04-01T20:22:13.312Z:
Received chunk: 
...

2025-04-01T20:22:13.386Z:
Received chunk: *...

2025-04-01T20:22:13.456Z:
Received chunk:    ...

2025-04-01T20:22:13.527Z:
Received chunk: Continue...

2025-04-01T20:22:13.598Z:
Received chunk:  the...

2025-04-01T20:22:13.668Z:
Received chunk:  conversation...

2025-04-01T20:22:13.738Z:
Received chunk: ?...

2025-04-01T20:22:13.807Z:
Received chunk: 
...

2025-04-01T20:22:13.876Z:
Received chunk: *...

2025-04-01T20:22:13.946Z:
Received chunk:    ...

2025-04-01T20:22:14.016Z:
Received chunk: Perform...

2025-04-01T20:22:14.087Z:
Received chunk:  a...

2025-04-01T20:22:14.160Z:
Received chunk:  task...

2025-04-01T20:22:14.232Z:
Received chunk:  based...

2025-04-01T20:22:14.307Z:
Received chunk:  on...

2025-04-01T20:22:14.378Z:
Received chunk:  it...

2025-04-01T20:22:14.448Z:
Received chunk: ?...

2025-04-01T20:22:14.517Z:
Received chunk: 

> ...

2025-04-01T20:23:01.029Z:
Unloading model...

2025-04-01T20:23:01.029Z:
Terminating running process

2025-04-01T20:23:01.030Z:
Model unloaded successfully

2025-04-01T20:23:01.031Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:23:01.031Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:23:01.032Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:23:01.033Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T20:23:01.279Z:
Process exited with code null

2025-04-01T20:23:01.280Z:
Process failed, falling back to simulation

2025-04-01T20:23:01.280Z:
Generating simulated response

2025-04-01T20:23:01.346Z:
Sent simulated chunk: I'm

2025-04-01T20:23:01.439Z:
Sent simulated chunk: a

2025-04-01T20:23:01.486Z:
Sent simulated chunk: locally

2025-04-01T20:23:01.580Z:
Sent simulated chunk: hosted

2025-04-01T20:23:01.627Z:
Sent simulated chunk: AI

2025-04-01T20:23:01.721Z:
Sent simulated chunk: assistant

2025-04-01T20:23:01.815Z:
Sent simulated chunk: running

2025-04-01T20:23:01.907Z:
Sent simulated chunk: directly

2025-04-01T20:23:02.000Z:
Sent simulated chunk: on

2025-04-01T20:23:02.062Z:
Sent simulated chunk: your

2025-04-01T20:23:02.157Z:
Sent simulated chunk: device.

2025-04-01T20:23:02.218Z:
Sent simulated chunk: I

2025-04-01T20:23:02.313Z:
Sent simulated chunk: process

2025-04-01T20:23:02.360Z:
Sent simulated chunk: information

2025-04-01T20:23:02.452Z:
Sent simulated chunk: using

2025-04-01T20:23:02.498Z:
Sent simulated chunk: the

2025-04-01T20:23:02.545Z:
Sent simulated chunk: gemma-3-4b-it-Q8_0

2025-04-01T20:23:02.648Z:
Sent simulated chunk: model

2025-04-01T20:23:02.702Z:
Sent simulated chunk: loaded

2025-04-01T20:23:02.778Z:
Sent simulated chunk: in

2025-04-01T20:23:02.825Z:
Sent simulated chunk: the

2025-04-01T20:23:02.872Z:
Sent simulated chunk: LM

2025-04-01T20:23:02.935Z:
Sent simulated chunk: Terminal

2025-04-01T20:23:03.028Z:
Sent simulated chunk: application.

2025-04-01T20:23:03.090Z:
Sent simulated chunk: I've

2025-04-01T20:23:03.182Z:
Sent simulated chunk: received

2025-04-01T20:23:03.261Z:
Sent simulated chunk: your

2025-04-01T20:23:03.307Z:
Sent simulated chunk: message:

2025-04-01T20:23:03.386Z:
Sent simulated chunk: "test"

2025-04-01T20:23:03.478Z:
Sent simulated chunk: and

2025-04-01T20:23:03.556Z:
Sent simulated chunk: am

2025-04-01T20:23:03.603Z:
Sent simulated chunk: responding

2025-04-01T20:23:03.670Z:
Sent simulated chunk: without

2025-04-01T20:23:03.728Z:
Sent simulated chunk: requiring

2025-04-01T20:23:03.775Z:
Sent simulated chunk: internet

2025-04-01T20:23:03.869Z:
Sent simulated chunk: access.

(This

2025-04-01T20:23:03.916Z:
Sent simulated chunk: response

2025-04-01T20:23:04.010Z:
Sent simulated chunk: was

2025-04-01T20:23:04.072Z:
Sent simulated chunk: generated

2025-04-01T20:23:04.149Z:
Sent simulated chunk: using

2025-04-01T20:23:04.195Z:
Sent simulated chunk: a

2025-04-01T20:23:04.258Z:
Sent simulated chunk: simulated

2025-04-01T20:23:04.335Z:
Sent simulated chunk: version

2025-04-01T20:23:04.412Z:
Sent simulated chunk: of

2025-04-01T20:23:04.484Z:
Generate response called with message: hey friend...

2025-04-01T20:23:04.485Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: hey friend
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:23:04.508Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:23:04.521Z:
Sent simulated chunk: the

2025-04-01T20:23:04.537Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T20:23:04.559Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T20:23:04.569Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T20:23:04.591Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T20:23:04.615Z:
Sent simulated chunk: gemma-3-4b-it-Q8_0

2025-04-01T20:23:04.643Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T20:23:04.660Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'


2025-04-01T20:23:04.660Z:
Sent simulated chunk: model)

2025-04-01T20:23:04.661Z:
Simulated response generation completed

2025-04-01T20:23:04.661Z:
Process stderr: print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:23:10.526Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  7717.68 MiB


2025-04-01T20:23:10.528Z:
Process stderr: ........................................................................................


2025-04-01T20:23:10.530Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized


2025-04-01T20:23:10.530Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T20:23:10.546Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T20:23:10.551Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:23:11.195Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|im_start|>system
You are a helpful assistant<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
Hi there<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant



2025-04-01T20:23:11.196Z:
Process stderr: 
system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T20:23:11.196Z:
Process stderr: sampler seed: 2111257589
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 0

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:23:11.916Z:
Received chunk: Hey...

2025-04-01T20:23:12.036Z:
Received chunk:  there...

2025-04-01T20:23:12.158Z:
Received chunk: !...

2025-04-01T20:23:12.279Z:
Received chunk:  How...

2025-04-01T20:23:12.400Z:
Received chunk:  can...

2025-04-01T20:23:12.521Z:
Received chunk:  I...

2025-04-01T20:23:12.638Z:
Received chunk:  assist...

2025-04-01T20:23:12.755Z:
Received chunk:  you...

2025-04-01T20:23:12.873Z:
Received chunk:  today...

2025-04-01T20:23:12.995Z:
Received chunk: ?...

2025-04-01T20:23:13.119Z:
Received chunk: 

> ...

2025-04-01T20:23:45.563Z:
Unloading model...

2025-04-01T20:23:45.563Z:
Terminating running process

2025-04-01T20:23:45.563Z:
Model unloaded successfully

2025-04-01T20:23:45.566Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:23:45.566Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:23:45.567Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:23:45.567Z:
Model mmproj-model-f16 file verified

2025-04-01T20:23:46.070Z:
Process exited with code null

2025-04-01T20:23:46.070Z:
Process failed, falling back to simulation

2025-04-01T20:23:46.070Z:
Generating simulated response

2025-04-01T20:23:46.166Z:
Sent simulated chunk: I'm

2025-04-01T20:23:46.212Z:
Sent simulated chunk: a

2025-04-01T20:23:46.275Z:
Sent simulated chunk: locally

2025-04-01T20:23:46.308Z:
Sent simulated chunk: hosted

2025-04-01T20:23:46.365Z:
Sent simulated chunk: AI

2025-04-01T20:23:46.431Z:
Sent simulated chunk: assistant

2025-04-01T20:23:46.494Z:
Sent simulated chunk: running

2025-04-01T20:23:46.556Z:
Sent simulated chunk: directly

2025-04-01T20:23:46.602Z:
Sent simulated chunk: on

2025-04-01T20:23:46.648Z:
Sent simulated chunk: your

2025-04-01T20:23:46.695Z:
Sent simulated chunk: device.

2025-04-01T20:23:46.757Z:
Sent simulated chunk: I

2025-04-01T20:23:46.865Z:
Sent simulated chunk: process

2025-04-01T20:23:46.973Z:
Sent simulated chunk: information

2025-04-01T20:23:47.035Z:
Sent simulated chunk: using

2025-04-01T20:23:47.130Z:
Sent simulated chunk: the

2025-04-01T20:23:47.178Z:
Sent simulated chunk: Qwen2.5-Coder-7B-Instruct-Q8_0

2025-04-01T20:23:47.224Z:
Sent simulated chunk: model

2025-04-01T20:23:47.271Z:
Sent simulated chunk: loaded

2025-04-01T20:23:47.303Z:
Sent simulated chunk: in

2025-04-01T20:23:47.349Z:
Sent simulated chunk: the

2025-04-01T20:23:47.426Z:
Sent simulated chunk: LM

2025-04-01T20:23:47.503Z:
Sent simulated chunk: Terminal

2025-04-01T20:23:47.566Z:
Sent simulated chunk: application.

2025-04-01T20:23:47.612Z:
Sent simulated chunk: I've

2025-04-01T20:23:47.667Z:
Unloading model...

2025-04-01T20:23:47.668Z:
Model unloaded successfully

2025-04-01T20:23:47.668Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:23:47.668Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:23:47.668Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:23:47.669Z:
Model mmproj-model-f16 file verified

2025-04-01T20:23:47.690Z:
Sent simulated chunk: received

2025-04-01T20:23:47.753Z:
Sent simulated chunk: your

2025-04-01T20:23:47.799Z:
Sent simulated chunk: message:

2025-04-01T20:23:47.861Z:
Sent simulated chunk: "hey

2025-04-01T20:23:47.907Z:
Sent simulated chunk: friend"

2025-04-01T20:23:48.001Z:
Sent simulated chunk: and

2025-04-01T20:23:48.094Z:
Sent simulated chunk: am

2025-04-01T20:23:48.188Z:
Sent simulated chunk: responding

2025-04-01T20:23:48.235Z:
Sent simulated chunk: without

2025-04-01T20:23:48.282Z:
Sent simulated chunk: requiring

2025-04-01T20:23:48.332Z:
Sent simulated chunk: internet

2025-04-01T20:23:48.439Z:
Sent simulated chunk: access.

(This

2025-04-01T20:23:48.532Z:
Sent simulated chunk: response

2025-04-01T20:23:48.609Z:
Sent simulated chunk: was

2025-04-01T20:23:48.671Z:
Sent simulated chunk: generated

2025-04-01T20:23:48.717Z:
Sent simulated chunk: using

2025-04-01T20:23:48.780Z:
Sent simulated chunk: a

2025-04-01T20:23:48.841Z:
Sent simulated chunk: simulated

2025-04-01T20:23:48.950Z:
Sent simulated chunk: version

2025-04-01T20:23:48.982Z:
Sent simulated chunk: of

2025-04-01T20:23:49.028Z:
Sent simulated chunk: the

2025-04-01T20:23:49.076Z:
Sent simulated chunk: Qwen2.5-Coder-7B-Instruct-Q8_0

2025-04-01T20:23:49.137Z:
Sent simulated chunk: model)

2025-04-01T20:23:49.137Z:
Simulated response generation completed

2025-04-01T20:23:52.390Z:
Generate response called with message: test...

2025-04-01T20:23:52.390Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:23:52.414Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:23:52.415Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T20:23:52.415Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T20:23:52.416Z:
Process exited with code 1

2025-04-01T20:23:52.417Z:
Process failed, falling back to simulation

2025-04-01T20:23:52.417Z:
Generating simulated response

2025-04-01T20:23:52.473Z:
Sent simulated chunk: I'm

2025-04-01T20:23:52.552Z:
Sent simulated chunk: a

2025-04-01T20:23:52.616Z:
Sent simulated chunk: locally

2025-04-01T20:23:52.694Z:
Sent simulated chunk: hosted

2025-04-01T20:23:52.771Z:
Sent simulated chunk: AI

2025-04-01T20:23:52.881Z:
Sent simulated chunk: assistant

2025-04-01T20:23:52.959Z:
Sent simulated chunk: running

2025-04-01T20:23:53.005Z:
Sent simulated chunk: directly

2025-04-01T20:23:53.114Z:
Sent simulated chunk: on

2025-04-01T20:23:53.161Z:
Sent simulated chunk: your

2025-04-01T20:23:53.207Z:
Sent simulated chunk: device.

2025-04-01T20:23:53.286Z:
Sent simulated chunk: I

2025-04-01T20:23:53.349Z:
Sent simulated chunk: process

2025-04-01T20:23:53.428Z:
Sent simulated chunk: information

2025-04-01T20:23:53.521Z:
Sent simulated chunk: using

2025-04-01T20:23:53.613Z:
Sent simulated chunk: the

2025-04-01T20:23:53.660Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T20:23:53.722Z:
Sent simulated chunk: model

2025-04-01T20:23:53.816Z:
Sent simulated chunk: loaded

2025-04-01T20:23:53.878Z:
Sent simulated chunk: in

2025-04-01T20:23:53.971Z:
Sent simulated chunk: the

2025-04-01T20:23:54.033Z:
Sent simulated chunk: LM

2025-04-01T20:23:54.096Z:
Sent simulated chunk: Terminal

2025-04-01T20:23:54.158Z:
Sent simulated chunk: application.

2025-04-01T20:23:54.251Z:
Sent simulated chunk: I've

2025-04-01T20:23:54.299Z:
Sent simulated chunk: received

2025-04-01T20:23:54.346Z:
Sent simulated chunk: your

2025-04-01T20:23:54.424Z:
Sent simulated chunk: message:

2025-04-01T20:23:54.501Z:
Sent simulated chunk: "test"

2025-04-01T20:23:54.611Z:
Sent simulated chunk: and

2025-04-01T20:23:54.643Z:
Sent simulated chunk: am

2025-04-01T20:23:54.705Z:
Sent simulated chunk: responding

2025-04-01T20:23:54.753Z:
Sent simulated chunk: without

2025-04-01T20:23:54.847Z:
Sent simulated chunk: requiring

2025-04-01T20:23:54.893Z:
Sent simulated chunk: internet

2025-04-01T20:23:54.986Z:
Sent simulated chunk: access.

(This

2025-04-01T20:23:55.033Z:
Sent simulated chunk: response

2025-04-01T20:23:55.127Z:
Sent simulated chunk: was

2025-04-01T20:23:55.190Z:
Sent simulated chunk: generated

2025-04-01T20:23:55.252Z:
Sent simulated chunk: using

2025-04-01T20:23:55.330Z:
Sent simulated chunk: a

2025-04-01T20:23:55.422Z:
Sent simulated chunk: simulated

2025-04-01T20:23:55.469Z:
Sent simulated chunk: version

2025-04-01T20:23:55.562Z:
Sent simulated chunk: of

2025-04-01T20:23:55.624Z:
Sent simulated chunk: the

2025-04-01T20:23:55.702Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T20:23:55.764Z:
Sent simulated chunk: model)

2025-04-01T20:23:55.764Z:
Simulated response generation completed

2025-04-01T20:24:39.876Z:
Unloading model...

2025-04-01T20:24:39.876Z:
Model unloaded successfully

2025-04-01T20:24:39.876Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:24:39.877Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:24:39.877Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:24:39.877Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T20:24:47.641Z:
Generate response called with message: hi who are you...

2025-04-01T20:24:47.642Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: hi who are you
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:24:47.665Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:24:47.688Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T20:24:47.710Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T20:24:47.719Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T20:24:47.740Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T20:24:47.789Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T20:24:47.805Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:24:48.249Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  7717.68 MiB


2025-04-01T20:24:48.249Z:
Process stderr: ...........................

2025-04-01T20:24:48.250Z:
Process stderr: ...........................

2025-04-01T20:24:48.250Z:
Process stderr: .........................

2025-04-01T20:24:48.251Z:
Process stderr: .........


2025-04-01T20:24:48.252Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized


2025-04-01T20:24:48.253Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T20:24:48.270Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T20:24:48.276Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:24:48.830Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|im_start|>system
You are a helpful assistant<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
Hi there<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T20:24:48.831Z:
Process stderr: main: interactive mode on.


2025-04-01T20:24:48.831Z:
Process stderr: sampler seed: 3276292274
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 0

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:24:49.583Z:
Received chunk: Hello...

2025-04-01T20:24:49.703Z:
Received chunk: !...

2025-04-01T20:24:49.821Z:
Received chunk:  I...

2025-04-01T20:24:49.941Z:
Received chunk: 'm...

2025-04-01T20:24:50.065Z:
Received chunk:  Q...

2025-04-01T20:24:50.190Z:
Received chunk: wen...

2025-04-01T20:24:50.314Z:
Received chunk: ,...

2025-04-01T20:24:50.435Z:
Received chunk:  an...

2025-04-01T20:24:50.555Z:
Received chunk:  AI...

2025-04-01T20:24:50.674Z:
Received chunk:  language...

2025-04-01T20:24:50.794Z:
Received chunk:  model...

2025-04-01T20:24:50.913Z:
Received chunk:  developed...

2025-04-01T20:24:51.036Z:
Received chunk:  by...

2025-04-01T20:24:51.161Z:
Received chunk:  Alibaba...

2025-04-01T20:24:51.285Z:
Received chunk:  Cloud...

2025-04-01T20:24:51.405Z:
Received chunk: ....

2025-04-01T20:24:51.527Z:
Received chunk:  My...

2025-04-01T20:24:51.646Z:
Received chunk:  purpose...

2025-04-01T20:24:51.764Z:
Received chunk:  is...

2025-04-01T20:24:51.882Z:
Received chunk:  to...

2025-04-01T20:24:52.004Z:
Received chunk:  assist...

2025-04-01T20:24:52.126Z:
Received chunk:  with...

2025-04-01T20:24:52.256Z:
Received chunk:  answering...

2025-04-01T20:24:52.385Z:
Received chunk:  questions...

2025-04-01T20:24:52.509Z:
Received chunk: ,...

2025-04-01T20:24:52.627Z:
Received chunk:  providing...

2025-04-01T20:24:52.746Z:
Received chunk:  information...

2025-04-01T20:24:52.864Z:
Received chunk: ,...

2025-04-01T20:24:52.984Z:
Received chunk:  and...

2025-04-01T20:24:53.116Z:
Received chunk:  helping...

2025-04-01T20:24:53.232Z:
Received chunk:  with...

2025-04-01T20:24:53.354Z:
Received chunk:  various...

2025-04-01T20:24:53.479Z:
Received chunk:  tasks...

2025-04-01T20:24:53.603Z:
Received chunk: ....

2025-04-01T20:24:53.725Z:
Received chunk:  How...

2025-04-01T20:24:53.847Z:
Received chunk:  can...

2025-04-01T20:24:53.966Z:
Received chunk:  I...

2025-04-01T20:24:54.088Z:
Received chunk:  assist...

2025-04-01T20:24:54.215Z:
Received chunk:  you...

2025-04-01T20:24:54.338Z:
Received chunk:  today...

2025-04-01T20:24:54.459Z:
Received chunk: ?...

2025-04-01T20:24:54.580Z:
Received chunk: 

> ...

2025-04-01T20:25:05.497Z:
Unloading model...

2025-04-01T20:25:05.497Z:
Terminating running process

2025-04-01T20:25:05.497Z:
Model unloaded successfully

2025-04-01T20:25:05.499Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:25:05.500Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:25:05.500Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:25:05.501Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T20:25:05.995Z:
Process exited with code null

2025-04-01T20:25:05.995Z:
Process failed, falling back to simulation

2025-04-01T20:25:05.995Z:
Generating simulated response

2025-04-01T20:25:06.095Z:
Sent simulated chunk: Hello!

2025-04-01T20:25:06.174Z:
Sent simulated chunk: I'm

2025-04-01T20:25:06.237Z:
Sent simulated chunk: a

2025-04-01T20:25:06.330Z:
Sent simulated chunk: local

2025-04-01T20:25:06.392Z:
Sent simulated chunk: AI

2025-04-01T20:25:06.440Z:
Sent simulated chunk: assistant

2025-04-01T20:25:06.518Z:
Sent simulated chunk: running

2025-04-01T20:25:06.596Z:
Sent simulated chunk: on

2025-04-01T20:25:06.659Z:
Sent simulated chunk: your

2025-04-01T20:25:06.690Z:
Sent simulated chunk: device

2025-04-01T20:25:06.736Z:
Sent simulated chunk: through

2025-04-01T20:25:06.783Z:
Sent simulated chunk: the

2025-04-01T20:25:06.862Z:
Sent simulated chunk: LM

2025-04-01T20:25:06.909Z:
Sent simulated chunk: Terminal.

2025-04-01T20:25:06.956Z:
Sent simulated chunk: How

2025-04-01T20:25:07.034Z:
Sent simulated chunk: can

2025-04-01T20:25:07.101Z:
Sent simulated chunk: I

2025-04-01T20:25:07.175Z:
Sent simulated chunk: help

2025-04-01T20:25:07.237Z:
Sent simulated chunk: you

2025-04-01T20:25:07.301Z:
Sent simulated chunk: today?

(This

2025-04-01T20:25:07.395Z:
Sent simulated chunk: response

2025-04-01T20:25:07.457Z:
Sent simulated chunk: was

2025-04-01T20:25:07.534Z:
Sent simulated chunk: generated

2025-04-01T20:25:07.582Z:
Sent simulated chunk: using

2025-04-01T20:25:07.659Z:
Sent simulated chunk: a

2025-04-01T20:25:07.767Z:
Sent simulated chunk: simulated

2025-04-01T20:25:07.814Z:
Sent simulated chunk: version

2025-04-01T20:25:07.861Z:
Sent simulated chunk: of

2025-04-01T20:25:07.940Z:
Sent simulated chunk: the

2025-04-01T20:25:08.020Z:
Sent simulated chunk: Qwen2.5-Coder-7B-Instruct-Q8_0

2025-04-01T20:25:08.098Z:
Sent simulated chunk: model)

2025-04-01T20:25:08.098Z:
Simulated response generation completed

2025-04-01T20:38:32.163Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:38:32.163Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:38:32.163Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T20:38:32.164Z:
Model mmproj-model-f16 file verified

2025-04-01T20:38:32.727Z:
Unloading model...

2025-04-01T20:38:32.727Z:
Model unloaded successfully

2025-04-01T20:38:32.728Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:38:32.728Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:38:32.728Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:38:32.729Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T20:47:40.095Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:47:40.096Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:47:40.096Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:47:40.097Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:47:41.056Z:
Unloading model...

2025-04-01T20:47:41.057Z:
Model unloaded successfully

2025-04-01T20:47:41.057Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:47:41.057Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:47:41.057Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:47:41.058Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:47:49.006Z:
Generate response called with message: test...

2025-04-01T20:47:49.007Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:47:49.033Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:47:49.063Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T20:47:49.085Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T20:47:49.094Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T20:47:49.117Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T20:47:49.168Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T20:47:49.168Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T20:47:49.185Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:47:50.840Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T20:47:50.860Z:
Process stderr: .

2025-04-01T20:47:50.881Z:
Process stderr: .

2025-04-01T20:47:50.902Z:
Process stderr: .

2025-04-01T20:47:50.924Z:
Process stderr: .

2025-04-01T20:47:50.945Z:
Process stderr: .

2025-04-01T20:47:50.966Z:
Process stderr: .

2025-04-01T20:47:50.988Z:
Process stderr: .

2025-04-01T20:47:51.022Z:
Process stderr: .

2025-04-01T20:47:51.029Z:
Process stderr: .

2025-04-01T20:47:51.062Z:
Process stderr: .

2025-04-01T20:47:51.079Z:
Process stderr: .

2025-04-01T20:47:51.104Z:
Process stderr: .

2025-04-01T20:47:51.122Z:
Process stderr: .

2025-04-01T20:47:51.147Z:
Process stderr: .

2025-04-01T20:47:51.164Z:
Process stderr: .

2025-04-01T20:47:51.181Z:
Process stderr: .

2025-04-01T20:47:51.206Z:
Process stderr: .

2025-04-01T20:47:51.222Z:
Process stderr: .

2025-04-01T20:47:51.239Z:
Process stderr: .

2025-04-01T20:47:51.266Z:
Process stderr: .

2025-04-01T20:47:51.284Z:
Process stderr: .

2025-04-01T20:47:51.309Z:
Process stderr: .

2025-04-01T20:47:51.325Z:
Process stderr: .

2025-04-01T20:47:51.346Z:
Process stderr: .

2025-04-01T20:47:51.367Z:
Process stderr: .

2025-04-01T20:47:51.402Z:
Process stderr: .

2025-04-01T20:47:51.426Z:
Process stderr: .

2025-04-01T20:47:51.443Z:
Process stderr: .

2025-04-01T20:47:51.468Z:
Process stderr: .

2025-04-01T20:47:51.484Z:
Process stderr: .

2025-04-01T20:47:51.501Z:
Process stderr: .

2025-04-01T20:47:51.526Z:
Process stderr: .

2025-04-01T20:47:51.542Z:
Process stderr: .

2025-04-01T20:47:51.559Z:
Process stderr: .

2025-04-01T20:47:51.583Z:
Process stderr: .

2025-04-01T20:47:51.599Z:
Process stderr: .

2025-04-01T20:47:51.629Z:
Process stderr: .

2025-04-01T20:47:51.641Z:
Process stderr: .

2025-04-01T20:47:51.662Z:
Process stderr: .

2025-04-01T20:47:51.684Z:
Process stderr: .

2025-04-01T20:47:51.717Z:
Process stderr: .

2025-04-01T20:47:51.725Z:
Process stderr: .

2025-04-01T20:47:51.759Z:
Process stderr: .

2025-04-01T20:47:51.766Z:
Process stderr: .

2025-04-01T20:47:51.801Z:
Process stderr: .

2025-04-01T20:47:51.818Z:
Process stderr: .

2025-04-01T20:47:51.845Z:
Process stderr: .

2025-04-01T20:47:51.862Z:
Process stderr: .

2025-04-01T20:47:51.880Z:
Process stderr: .

2025-04-01T20:47:51.905Z:
Process stderr: .

2025-04-01T20:47:51.923Z:
Process stderr: .

2025-04-01T20:47:51.950Z:
Process stderr: .

2025-04-01T20:47:51.966Z:
Process stderr: .

2025-04-01T20:47:51.984Z:
Process stderr: .

2025-04-01T20:47:52.009Z:
Process stderr: .

2025-04-01T20:47:52.025Z:
Process stderr: .

2025-04-01T20:47:52.046Z:
Process stderr: .

2025-04-01T20:47:52.068Z:
Process stderr: .

2025-04-01T20:47:52.090Z:
Process stderr: .

2025-04-01T20:47:52.112Z:
Process stderr: .

2025-04-01T20:47:52.134Z:
Process stderr: .

2025-04-01T20:47:52.173Z:
Process stderr: .

2025-04-01T20:47:52.180Z:
Process stderr: .

2025-04-01T20:47:52.216Z:
Process stderr: .

2025-04-01T20:47:52.223Z:
Process stderr: .

2025-04-01T20:47:52.258Z:
Process stderr: ....

2025-04-01T20:47:52.258Z:
Process stderr: .........

2025-04-01T20:47:52.258Z:
Process stderr: .....


2025-04-01T20:47:52.260Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T20:47:52.260Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T20:47:52.277Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T20:47:52.282Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:47:52.531Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T20:47:52.531Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T20:47:52.532Z:
Process stderr: sampler seed: 3864754337
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:47:52.728Z:
Received chunk: <think>...

2025-04-01T20:47:52.818Z:
Received chunk: 

...

2025-04-01T20:47:52.907Z:
Received chunk: </think>...

2025-04-01T20:47:52.996Z:
Received chunk: 

...

2025-04-01T20:47:53.084Z:
Received chunk: Hello...

2025-04-01T20:47:53.171Z:
Received chunk: !...

2025-04-01T20:47:53.258Z:
Received chunk:  How...

2025-04-01T20:47:53.345Z:
Received chunk:  can...

2025-04-01T20:47:53.428Z:
Received chunk:  I...

2025-04-01T20:47:53.510Z:
Received chunk:  assist...

2025-04-01T20:47:53.592Z:
Received chunk:  you...

2025-04-01T20:47:53.675Z:
Received chunk:  today...

2025-04-01T20:47:53.761Z:
Received chunk: ?...

2025-04-01T20:47:53.847Z:
Received chunk:  �...

2025-04-01T20:47:53.933Z:
Received chunk: �...

2025-04-01T20:47:54.014Z:
Received chunk: 

> ...

2025-04-01T20:53:14.405Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:53:14.405Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:53:14.406Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:53:14.406Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:53:15.308Z:
Unloading model...

2025-04-01T20:53:15.309Z:
Model unloaded successfully

2025-04-01T20:53:15.309Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:53:15.310Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:53:15.310Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:53:15.310Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:53:18.574Z:
Generate response called with message: hey friend...

2025-04-01T20:53:18.574Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: hey friend
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:53:18.599Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:53:18.623Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T20:53:18.645Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T20:53:18.654Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T20:53:18.676Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T20:53:18.728Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T20:53:18.728Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T20:53:18.745Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28


2025-04-01T20:53:18.745Z:
Process stderr: print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:53:18.974Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T20:53:18.997Z:
Process stderr: .

2025-04-01T20:53:19.016Z:
Process stderr: .

2025-04-01T20:53:19.035Z:
Process stderr: .

2025-04-01T20:53:19.054Z:
Process stderr: .

2025-04-01T20:53:19.074Z:
Process stderr: .

2025-04-01T20:53:19.093Z:
Process stderr: .

2025-04-01T20:53:19.113Z:
Process stderr: .

2025-04-01T20:53:19.144Z:
Process stderr: .

2025-04-01T20:53:19.151Z:
Process stderr: .

2025-04-01T20:53:19.182Z:
Process stderr: .

2025-04-01T20:53:19.198Z:
Process stderr: .

2025-04-01T20:53:19.220Z:
Process stderr: .

2025-04-01T20:53:19.237Z:
Process stderr: .

2025-04-01T20:53:19.260Z:
Process stderr: .

2025-04-01T20:53:19.276Z:
Process stderr: .

2025-04-01T20:53:19.292Z:
Process stderr: .

2025-04-01T20:53:19.315Z:
Process stderr: .

2025-04-01T20:53:19.332Z:
Process stderr: .

2025-04-01T20:53:19.349Z:
Process stderr: .

2025-04-01T20:53:19.373Z:
Process stderr: .

2025-04-01T20:53:19.390Z:
Process stderr: .

2025-04-01T20:53:19.414Z:
Process stderr: .

2025-04-01T20:53:19.431Z:
Process stderr: .

2025-04-01T20:53:19.452Z:
Process stderr: .

2025-04-01T20:53:19.473Z:
Process stderr: .

2025-04-01T20:53:19.506Z:
Process stderr: .

2025-04-01T20:53:19.531Z:
Process stderr: .

2025-04-01T20:53:19.549Z:
Process stderr: .

2025-04-01T20:53:19.575Z:
Process stderr: .

2025-04-01T20:53:19.591Z:
Process stderr: .

2025-04-01T20:53:19.610Z:
Process stderr: .

2025-04-01T20:53:19.636Z:
Process stderr: .

2025-04-01T20:53:19.654Z:
Process stderr: .

2025-04-01T20:53:19.673Z:
Process stderr: .

2025-04-01T20:53:19.699Z:
Process stderr: .

2025-04-01T20:53:19.720Z:
Process stderr: .

2025-04-01T20:53:19.746Z:
Process stderr: .

2025-04-01T20:53:19.764Z:
Process stderr: .

2025-04-01T20:53:19.787Z:
Process stderr: .

2025-04-01T20:53:19.811Z:
Process stderr: .

2025-04-01T20:53:19.847Z:
Process stderr: .

2025-04-01T20:53:19.854Z:
Process stderr: .

2025-04-01T20:53:19.891Z:
Process stderr: .

2025-04-01T20:53:19.899Z:
Process stderr: .

2025-04-01T20:53:19.935Z:
Process stderr: .

2025-04-01T20:53:19.954Z:
Process stderr: .

2025-04-01T20:53:19.981Z:
Process stderr: .

2025-04-01T20:53:19.999Z:
Process stderr: .

2025-04-01T20:53:20.018Z:
Process stderr: .

2025-04-01T20:53:20.045Z:
Process stderr: .

2025-04-01T20:53:20.064Z:
Process stderr: .

2025-04-01T20:53:20.093Z:
Process stderr: .

2025-04-01T20:53:20.111Z:
Process stderr: .

2025-04-01T20:53:20.133Z:
Process stderr: .

2025-04-01T20:53:20.163Z:
Process stderr: .

2025-04-01T20:53:20.184Z:
Process stderr: .

2025-04-01T20:53:20.210Z:
Process stderr: .

2025-04-01T20:53:20.236Z:
Process stderr: .

2025-04-01T20:53:20.263Z:
Process stderr: .

2025-04-01T20:53:20.291Z:
Process stderr: .

2025-04-01T20:53:20.317Z:
Process stderr: .

2025-04-01T20:53:20.366Z:
Process stderr: .

2025-04-01T20:53:20.375Z:
Process stderr: .

2025-04-01T20:53:20.419Z:
Process stderr: .

2025-04-01T20:53:20.428Z:
Process stderr: .

2025-04-01T20:53:20.472Z:
Process stderr: ...

2025-04-01T20:53:20.472Z:
Process stderr: ........

2025-04-01T20:53:20.472Z:
Process stderr: ....

2025-04-01T20:53:20.473Z:
Process stderr: ...


2025-04-01T20:53:20.475Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T20:53:20.475Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T20:53:20.502Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T20:53:20.509Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:53:20.788Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T20:53:20.788Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T20:53:20.789Z:
Process stderr: sampler seed: 18168166
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1


2025-04-01T20:53:20.789Z:
Process stderr: 
== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:55:20.577Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:55:20.577Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:55:20.578Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:55:20.578Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T20:55:21.653Z:
Unloading model...

2025-04-01T20:55:21.654Z:
Model unloaded successfully

2025-04-01T20:55:21.654Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:55:21.654Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:55:21.654Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T20:55:21.655Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T20:55:24.765Z:
Generate response called with message: test...

2025-04-01T20:55:24.765Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:55:24.793Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:55:24.827Z:
Process stderr: llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It
llama_model_loader: - kv   3:                           general.finetune str              = it
llama_model_loader: - kv   4:                           general.basename str              = gemma-3
llama_model_loader: - kv   5:                         general.size_label str              = 4B
llama_model_loader: - kv   6:                            general.license str              = gemma
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Google
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...
llama_model_loader: - kv  11:                               general.tags arr[str,1]       = ["image-text-to-text"]
llama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072
llama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560
llama_model_loader: - kv  14:                         gemma3.block_count u32              = 34
llama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240
llama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8
llama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256
llama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256
llama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024
llama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4
llama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear
llama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000
llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default


2025-04-01T20:55:24.864Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...


2025-04-01T20:55:24.949Z:
Process stderr: llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...


2025-04-01T20:55:24.965Z:
Process stderr: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\n{%- if messages[0]['r...
llama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  38:               general.quantization_version u32              = 2
llama_model_loader: - kv  39:                          general.file_type u32              = 7
llama_model_loader: - type  f32:  205 tensors
llama_model_loader: - type q8_0:  239 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 3.84 GiB (8.50 BPW) 


2025-04-01T20:55:25.014Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T20:55:25.015Z:
Process stderr: load: special tokens cache size = 6414


2025-04-01T20:55:25.028Z:
Process stderr: load: token to piece cache size = 1.9446 MB
print_info: arch             = gemma3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2560
print_info: n_layer          = 34
print_info: n_head           = 8


2025-04-01T20:55:25.029Z:
Process stderr: print_info: n_head_kv        = 4
print_info: n_rot            = 256
print_info: n_swa            = 1024
print_info: n_swa_pattern    = 6
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 6.2e-02
print_info: n_ff             = 10240
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.125
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 4B
print_info: model params     = 3.88 B
print_info: general.name     = Gemma 3 4b It
print_info: vocab type       = SPM
print_info: n_vocab          = 262144
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 106 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 248 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 106 '<end_of_turn>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:55:27.177Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/35 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  3932.65 MiB
.

2025-04-01T20:55:27.177Z:
Process stderr: ....................

2025-04-01T20:55:27.177Z:
Process stderr: ...........

2025-04-01T20:55:27.178Z:
Process stderr: .......................

2025-04-01T20:55:27.178Z:
Process stderr: .................

2025-04-01T20:55:27.179Z:
Process stderr: ............


2025-04-01T20:55:27.180Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.125
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T20:55:27.180Z:
Process stderr: llama_context:        CPU  output buffer size =     1.00 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 34, can_shift = 1


2025-04-01T20:55:27.223Z:
Process stderr: init:        CPU KV buffer size =   272.00 MiB
llama_context: KV self size  =  272.00 MiB, K (f16):  136.00 MiB, V (f16):  136.00 MiB


2025-04-01T20:55:27.228Z:
Process stderr: llama_context:        CPU compute buffer size =   522.00 MiB
llama_context: graph nodes  = 1435
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:55:27.651Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<start_of_turn>user
You are a helpful assistant

Hello<end_of_turn>
<start_of_turn>model
Hi there<end_of_turn>
<start_of_turn>user
How are you?<end_of_turn>
<start_of_turn>model


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T20:55:27.652Z:
Process stderr: main: interactive mode on.


2025-04-01T20:55:27.652Z:
Process stderr: sampler seed: 3289430752
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:55:27.838Z:
Received chunk: Okay...

2025-04-01T20:55:27.911Z:
Received chunk: !...

2025-04-01T20:55:27.985Z:
Received chunk:  That...

2025-04-01T20:55:28.058Z:
Received chunk:  was...

2025-04-01T20:55:28.132Z:
Received chunk:  a...

2025-04-01T20:55:28.203Z:
Received chunk:  simple...

2025-04-01T20:55:28.274Z:
Received chunk:  test...

2025-04-01T20:55:28.344Z:
Received chunk: ....

2025-04-01T20:55:28.416Z:
Received chunk:  Is...

2025-04-01T20:55:28.485Z:
Received chunk:  there...

2025-04-01T20:55:28.554Z:
Received chunk:  anything...

2025-04-01T20:55:28.624Z:
Received chunk:  you...

2025-04-01T20:55:28.695Z:
Received chunk: ’...

2025-04-01T20:55:28.767Z:
Received chunk: d...

2025-04-01T20:55:28.840Z:
Received chunk:  like...

2025-04-01T20:55:28.915Z:
Received chunk:  me...

2025-04-01T20:55:28.990Z:
Received chunk:  to...

2025-04-01T20:55:29.064Z:
Received chunk:  do...

2025-04-01T20:55:29.137Z:
Received chunk:  now...

2025-04-01T20:55:29.209Z:
Received chunk: ?...

2025-04-01T20:55:29.279Z:
Received chunk:  Do...

2025-04-01T20:55:29.351Z:
Received chunk:  you...

2025-04-01T20:55:29.423Z:
Received chunk:  want...

2025-04-01T20:55:29.493Z:
Received chunk:  to...

2025-04-01T20:55:29.566Z:
Received chunk: :...

2025-04-01T20:55:29.639Z:
Received chunk: 

...

2025-04-01T20:55:29.711Z:
Received chunk: *...

2025-04-01T20:55:29.785Z:
Received chunk:    ...

2025-04-01T20:55:29.859Z:
Received chunk: **...

2025-04-01T20:55:29.935Z:
Received chunk: Ask...

2025-04-01T20:55:30.009Z:
Received chunk:  me...

2025-04-01T20:55:30.082Z:
Received chunk:  a...

2025-04-01T20:55:30.156Z:
Received chunk:  question...

2025-04-01T20:55:30.227Z:
Received chunk: ?...

2025-04-01T20:55:30.298Z:
Received chunk: **...

2025-04-01T20:55:30.369Z:
Received chunk: 
...

2025-04-01T20:55:30.439Z:
Received chunk: *...

2025-04-01T20:55:30.510Z:
Received chunk:    ...

2025-04-01T20:55:30.583Z:
Received chunk: **...

2025-04-01T20:55:30.655Z:
Received chunk: Give...

2025-04-01T20:55:30.728Z:
Received chunk:  me...

2025-04-01T20:55:30.802Z:
Received chunk:  a...

2025-04-01T20:55:30.878Z:
Received chunk:  task...

2025-04-01T20:55:30.953Z:
Received chunk: ?...

2025-04-01T20:55:31.030Z:
Received chunk: **...

2025-04-01T20:55:31.102Z:
Received chunk:  (...

2025-04-01T20:55:31.175Z:
Received chunk: e...

2025-04-01T20:55:31.246Z:
Received chunk: ....

2025-04-01T20:55:31.316Z:
Received chunk: g...

2025-04-01T20:55:31.386Z:
Received chunk: .,...

2025-04-01T20:55:31.456Z:
Received chunk:  write...

2025-04-01T20:55:31.525Z:
Received chunk:  something...

2025-04-01T20:55:31.595Z:
Received chunk: ,...

2025-04-01T20:55:31.666Z:
Received chunk:  summarize...

2025-04-01T20:55:31.740Z:
Received chunk:  text...

2025-04-01T20:55:31.812Z:
Received chunk: )...

2025-04-01T20:55:31.889Z:
Received chunk: 
...

2025-04-01T20:55:31.967Z:
Received chunk: *...

2025-04-01T20:55:32.048Z:
Received chunk:    ...

2025-04-01T20:55:32.122Z:
Received chunk: **...

2025-04-01T20:55:32.195Z:
Received chunk: Just...

2025-04-01T20:55:32.265Z:
Received chunk:  chat...

2025-04-01T20:55:32.336Z:
Received chunk: ?...

2025-04-01T20:55:32.405Z:
Received chunk: **...

2025-04-01T20:55:32.476Z:
Received chunk: 

> ...

2025-04-01T20:57:40.882Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:57:40.882Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:57:40.882Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:57:40.882Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:57:54.056Z:
Unloading model...

2025-04-01T20:57:54.057Z:
Model unloaded successfully

2025-04-01T20:57:54.057Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:57:54.057Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:57:54.057Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:57:54.058Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T20:57:57.846Z:
Generate response called with message: test...

2025-04-01T20:57:57.846Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T20:57:57.873Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T20:57:57.896Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T20:57:57.917Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T20:57:57.927Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T20:57:57.949Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T20:57:58.000Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T20:57:58.000Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T20:57:58.017Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0


2025-04-01T20:57:58.017Z:
Process stderr: print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T20:57:58.258Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T20:57:58.283Z:
Process stderr: .

2025-04-01T20:57:58.303Z:
Process stderr: .

2025-04-01T20:57:58.323Z:
Process stderr: .

2025-04-01T20:57:58.343Z:
Process stderr: .

2025-04-01T20:57:58.362Z:
Process stderr: .

2025-04-01T20:57:58.381Z:
Process stderr: .

2025-04-01T20:57:58.404Z:
Process stderr: .

2025-04-01T20:57:58.438Z:
Process stderr: .

2025-04-01T20:57:58.445Z:
Process stderr: .

2025-04-01T20:57:58.479Z:
Process stderr: .

2025-04-01T20:57:58.496Z:
Process stderr: .

2025-04-01T20:57:58.520Z:
Process stderr: .

2025-04-01T20:57:58.538Z:
Process stderr: .

2025-04-01T20:57:58.563Z:
Process stderr: .

2025-04-01T20:57:58.581Z:
Process stderr: .

2025-04-01T20:57:58.606Z:
Process stderr: .

2025-04-01T20:57:58.633Z:
Process stderr: .

2025-04-01T20:57:58.651Z:
Process stderr: .

2025-04-01T20:57:58.670Z:
Process stderr: .

2025-04-01T20:57:58.697Z:
Process stderr: .

2025-04-01T20:57:58.715Z:
Process stderr: .

2025-04-01T20:57:58.742Z:
Process stderr: .

2025-04-01T20:57:58.760Z:
Process stderr: .

2025-04-01T20:57:58.783Z:
Process stderr: .

2025-04-01T20:57:58.806Z:
Process stderr: .

2025-04-01T20:57:58.839Z:
Process stderr: .

2025-04-01T20:57:58.862Z:
Process stderr: .

2025-04-01T20:57:58.880Z:
Process stderr: .

2025-04-01T20:57:58.904Z:
Process stderr: .

2025-04-01T20:57:58.920Z:
Process stderr: .

2025-04-01T20:57:58.938Z:
Process stderr: .

2025-04-01T20:57:58.962Z:
Process stderr: .

2025-04-01T20:57:58.978Z:
Process stderr: .

2025-04-01T20:57:58.995Z:
Process stderr: .

2025-04-01T20:57:59.019Z:
Process stderr: .

2025-04-01T20:57:59.035Z:
Process stderr: .

2025-04-01T20:57:59.060Z:
Process stderr: .

2025-04-01T20:57:59.077Z:
Process stderr: .

2025-04-01T20:57:59.097Z:
Process stderr: .

2025-04-01T20:57:59.119Z:
Process stderr: .

2025-04-01T20:57:59.152Z:
Process stderr: .

2025-04-01T20:57:59.159Z:
Process stderr: .

2025-04-01T20:57:59.195Z:
Process stderr: .

2025-04-01T20:57:59.203Z:
Process stderr: .

2025-04-01T20:57:59.238Z:
Process stderr: .

2025-04-01T20:57:59.255Z:
Process stderr: .

2025-04-01T20:57:59.280Z:
Process stderr: .

2025-04-01T20:57:59.297Z:
Process stderr: .

2025-04-01T20:57:59.314Z:
Process stderr: .

2025-04-01T20:57:59.338Z:
Process stderr: .

2025-04-01T20:57:59.354Z:
Process stderr: .

2025-04-01T20:57:59.379Z:
Process stderr: .

2025-04-01T20:57:59.396Z:
Process stderr: .

2025-04-01T20:57:59.413Z:
Process stderr: .

2025-04-01T20:57:59.438Z:
Process stderr: .

2025-04-01T20:57:59.456Z:
Process stderr: .

2025-04-01T20:57:59.477Z:
Process stderr: .

2025-04-01T20:57:59.499Z:
Process stderr: .

2025-04-01T20:57:59.520Z:
Process stderr: .

2025-04-01T20:57:59.543Z:
Process stderr: .

2025-04-01T20:57:59.565Z:
Process stderr: .

2025-04-01T20:57:59.605Z:
Process stderr: .

2025-04-01T20:57:59.614Z:
Process stderr: .

2025-04-01T20:57:59.651Z:
Process stderr: .

2025-04-01T20:57:59.658Z:
Process stderr: .

2025-04-01T20:57:59.695Z:
Process stderr: .....

2025-04-01T20:57:59.696Z:
Process stderr: .............


2025-04-01T20:57:59.698Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T20:57:59.699Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T20:57:59.716Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T20:57:59.721Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T20:57:59.971Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T20:57:59.972Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T20:57:59.973Z:
Process stderr: sampler seed: 1848016002
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T20:58:00.163Z:
Received chunk: <think>...

2025-04-01T20:58:00.249Z:
Received chunk: 

...

2025-04-01T20:58:00.337Z:
Received chunk: </think>...

2025-04-01T20:58:00.421Z:
Received chunk: 

...

2025-04-01T20:58:00.503Z:
Received chunk: It...

2025-04-01T20:58:00.584Z:
Received chunk:  seems...

2025-04-01T20:58:00.665Z:
Received chunk:  like...

2025-04-01T20:58:00.748Z:
Received chunk:  your...

2025-04-01T20:58:00.832Z:
Received chunk:  message...

2025-04-01T20:58:00.912Z:
Received chunk:  might...

2025-04-01T20:58:00.994Z:
Received chunk:  be...

2025-04-01T20:58:01.075Z:
Received chunk:  incomplete...

2025-04-01T20:58:01.157Z:
Received chunk:  or...

2025-04-01T20:58:01.242Z:
Received chunk:  unclear...

2025-04-01T20:58:01.326Z:
Received chunk: ....

2025-04-01T20:58:01.413Z:
Received chunk:  Could...

2025-04-01T20:58:01.496Z:
Received chunk:  you...

2025-04-01T20:58:01.577Z:
Received chunk:  clarify...

2025-04-01T20:58:01.657Z:
Received chunk:  what...

2025-04-01T20:58:01.738Z:
Received chunk:  you...

2025-04-01T20:58:01.818Z:
Received chunk: 'd...

2025-04-01T20:58:01.899Z:
Received chunk:  like...

2025-04-01T20:58:01.980Z:
Received chunk:  to...

2025-04-01T20:58:02.059Z:
Received chunk:  know...

2025-04-01T20:58:02.142Z:
Received chunk: ,...

2025-04-01T20:58:02.224Z:
Received chunk:  ask...

2025-04-01T20:58:02.311Z:
Received chunk: ,...

2025-04-01T20:58:02.397Z:
Received chunk:  or...

2025-04-01T20:58:02.480Z:
Received chunk:  discuss...

2025-04-01T20:58:02.561Z:
Received chunk: ?...

2025-04-01T20:58:02.643Z:
Received chunk:  I...

2025-04-01T20:58:02.724Z:
Received chunk: 'm...

2025-04-01T20:58:02.805Z:
Received chunk:  here...

2025-04-01T20:58:02.886Z:
Received chunk:  to...

2025-04-01T20:58:02.967Z:
Received chunk:  help...

2025-04-01T20:58:03.049Z:
Received chunk: !...

2025-04-01T20:58:03.131Z:
Received chunk: 

> ...

2025-04-01T20:58:17.868Z:
Unloading model...

2025-04-01T20:58:17.868Z:
Terminating running process

2025-04-01T20:58:17.869Z:
Model unloaded successfully

2025-04-01T20:58:17.870Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:58:17.870Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:58:17.871Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T20:58:17.872Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T20:58:18.298Z:
Process exited with code null

2025-04-01T20:58:18.298Z:
Process failed, falling back to simulation

2025-04-01T20:58:18.299Z:
Generating simulated response

2025-04-01T20:58:18.402Z:
Sent simulated chunk: I'm

2025-04-01T20:58:18.496Z:
Sent simulated chunk: a

2025-04-01T20:58:18.589Z:
Sent simulated chunk: locally

2025-04-01T20:58:18.636Z:
Sent simulated chunk: hosted

2025-04-01T20:58:18.682Z:
Sent simulated chunk: AI

2025-04-01T20:58:18.729Z:
Sent simulated chunk: assistant

2025-04-01T20:58:18.775Z:
Sent simulated chunk: running

2025-04-01T20:58:18.869Z:
Sent simulated chunk: directly

2025-04-01T20:58:18.947Z:
Sent simulated chunk: on

2025-04-01T20:58:19.024Z:
Sent simulated chunk: your

2025-04-01T20:58:19.118Z:
Sent simulated chunk: device.

2025-04-01T20:58:19.164Z:
Sent simulated chunk: I

2025-04-01T20:58:19.227Z:
Sent simulated chunk: process

2025-04-01T20:58:19.288Z:
Sent simulated chunk: information

2025-04-01T20:58:19.383Z:
Sent simulated chunk: using

2025-04-01T20:58:19.467Z:
Sent simulated chunk: the

2025-04-01T20:58:19.568Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T20:58:19.600Z:
Sent simulated chunk: model

2025-04-01T20:58:19.661Z:
Sent simulated chunk: loaded

2025-04-01T20:58:19.754Z:
Sent simulated chunk: in

2025-04-01T20:58:19.817Z:
Sent simulated chunk: the

2025-04-01T20:58:19.910Z:
Sent simulated chunk: LM

2025-04-01T20:58:19.956Z:
Sent simulated chunk: Terminal

2025-04-01T20:58:20.002Z:
Sent simulated chunk: application.

2025-04-01T20:58:20.079Z:
Sent simulated chunk: I've

2025-04-01T20:58:20.188Z:
Sent simulated chunk: received

2025-04-01T20:58:20.251Z:
Sent simulated chunk: your

2025-04-01T20:58:20.282Z:
Sent simulated chunk: message:

2025-04-01T20:58:20.358Z:
Sent simulated chunk: "test"

2025-04-01T20:58:20.444Z:
Sent simulated chunk: and

2025-04-01T20:58:20.498Z:
Sent simulated chunk: am

2025-04-01T20:58:20.544Z:
Sent simulated chunk: responding

2025-04-01T20:58:20.606Z:
Sent simulated chunk: without

2025-04-01T20:58:20.685Z:
Sent simulated chunk: requiring

2025-04-01T20:58:20.732Z:
Sent simulated chunk: internet

2025-04-01T20:58:20.778Z:
Sent simulated chunk: access.

(This

2025-04-01T20:58:20.810Z:
Sent simulated chunk: response

2025-04-01T20:58:20.888Z:
Sent simulated chunk: was

2025-04-01T20:58:20.950Z:
Sent simulated chunk: generated

2025-04-01T20:58:21.027Z:
Sent simulated chunk: using

2025-04-01T20:58:21.103Z:
Sent simulated chunk: a

2025-04-01T20:58:21.165Z:
Sent simulated chunk: simulated

2025-04-01T20:58:21.228Z:
Sent simulated chunk: version

2025-04-01T20:58:21.320Z:
Sent simulated chunk: of

2025-04-01T20:58:21.414Z:
Sent simulated chunk: the

2025-04-01T20:58:21.492Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T20:58:21.571Z:
Sent simulated chunk: model)

2025-04-01T20:58:21.571Z:
Simulated response generation completed

2025-04-01T20:59:04.722Z:
Unloading model...

2025-04-01T20:59:04.722Z:
Model unloaded successfully

2025-04-01T20:59:04.722Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:59:04.723Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T20:59:04.723Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T20:59:04.723Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:22:21.226Z:
Unloading model...

2025-04-01T21:22:21.226Z:
Model unloaded successfully

2025-04-01T21:22:21.226Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:22:21.227Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:22:21.227Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:22:21.227Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:22:22.438Z:
Unloading model...

2025-04-01T21:22:22.439Z:
Model unloaded successfully

2025-04-01T21:22:22.439Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:22:22.439Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:22:22.439Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:22:22.439Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:22:47.957Z:
Generate response called with message: Test...

2025-04-01T21:22:47.958Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:22:47.994Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:22:48.017Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-01T21:22:48.039Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T21:22:48.048Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T21:22:48.071Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-01T21:22:48.121Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T21:22:48.121Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T21:22:48.140Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00


2025-04-01T21:22:48.141Z:
Process stderr: print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T21:22:48.381Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-01T21:22:48.405Z:
Process stderr: .

2025-04-01T21:22:48.424Z:
Process stderr: .

2025-04-01T21:22:48.444Z:
Process stderr: .

2025-04-01T21:22:48.464Z:
Process stderr: .

2025-04-01T21:22:48.484Z:
Process stderr: .

2025-04-01T21:22:48.508Z:
Process stderr: .

2025-04-01T21:22:48.523Z:
Process stderr: .

2025-04-01T21:22:48.555Z:
Process stderr: .

2025-04-01T21:22:48.562Z:
Process stderr: .

2025-04-01T21:22:48.594Z:
Process stderr: .

2025-04-01T21:22:48.610Z:
Process stderr: .

2025-04-01T21:22:48.632Z:
Process stderr: .

2025-04-01T21:22:48.648Z:
Process stderr: .

2025-04-01T21:22:48.671Z:
Process stderr: .

2025-04-01T21:22:48.687Z:
Process stderr: .

2025-04-01T21:22:48.703Z:
Process stderr: .

2025-04-01T21:22:48.726Z:
Process stderr: .

2025-04-01T21:22:48.741Z:
Process stderr: .

2025-04-01T21:22:48.757Z:
Process stderr: .

2025-04-01T21:22:48.785Z:
Process stderr: .

2025-04-01T21:22:48.806Z:
Process stderr: .

2025-04-01T21:22:48.829Z:
Process stderr: .

2025-04-01T21:22:48.844Z:
Process stderr: .

2025-04-01T21:22:48.864Z:
Process stderr: .

2025-04-01T21:22:48.883Z:
Process stderr: .

2025-04-01T21:22:48.914Z:
Process stderr: .

2025-04-01T21:22:48.937Z:
Process stderr: .

2025-04-01T21:22:48.953Z:
Process stderr: .

2025-04-01T21:22:48.976Z:
Process stderr: .

2025-04-01T21:22:48.992Z:
Process stderr: .

2025-04-01T21:22:49.009Z:
Process stderr: .

2025-04-01T21:22:49.034Z:
Process stderr: .

2025-04-01T21:22:49.050Z:
Process stderr: .

2025-04-01T21:22:49.068Z:
Process stderr: .

2025-04-01T21:22:49.092Z:
Process stderr: .

2025-04-01T21:22:49.110Z:
Process stderr: .

2025-04-01T21:22:49.136Z:
Process stderr: .

2025-04-01T21:22:49.152Z:
Process stderr: .

2025-04-01T21:22:49.173Z:
Process stderr: .

2025-04-01T21:22:49.196Z:
Process stderr: .

2025-04-01T21:22:49.231Z:
Process stderr: .

2025-04-01T21:22:49.239Z:
Process stderr: .

2025-04-01T21:22:49.275Z:
Process stderr: .

2025-04-01T21:22:49.283Z:
Process stderr: .

2025-04-01T21:22:49.319Z:
Process stderr: .

2025-04-01T21:22:49.337Z:
Process stderr: .

2025-04-01T21:22:49.363Z:
Process stderr: .

2025-04-01T21:22:49.380Z:
Process stderr: .

2025-04-01T21:22:49.397Z:
Process stderr: .

2025-04-01T21:22:49.422Z:
Process stderr: .

2025-04-01T21:22:49.440Z:
Process stderr: .

2025-04-01T21:22:49.475Z:
Process stderr: .

2025-04-01T21:22:49.494Z:
Process stderr: .

2025-04-01T21:22:49.516Z:
Process stderr: .

2025-04-01T21:22:49.546Z:
Process stderr: .

2025-04-01T21:22:49.565Z:
Process stderr: .

2025-04-01T21:22:49.589Z:
Process stderr: .

2025-04-01T21:22:49.615Z:
Process stderr: .

2025-04-01T21:22:49.641Z:
Process stderr: .

2025-04-01T21:22:49.666Z:
Process stderr: .

2025-04-01T21:22:49.691Z:
Process stderr: .

2025-04-01T21:22:49.736Z:
Process stderr: .

2025-04-01T21:22:49.744Z:
Process stderr: .

2025-04-01T21:22:49.784Z:
Process stderr: .

2025-04-01T21:22:49.792Z:
Process stderr: .

2025-04-01T21:22:49.829Z:
Process stderr: ....

2025-04-01T21:22:49.829Z:
Process stderr: .........

2025-04-01T21:22:49.829Z:
Process stderr: .....


2025-04-01T21:22:49.831Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T21:22:49.832Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T21:22:49.849Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T21:22:49.854Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T21:22:50.096Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-01T21:22:50.096Z:
Process stderr: main: interactive mode on.


2025-04-01T21:22:50.097Z:
Process stderr: sampler seed: 2508839950
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T21:22:50.284Z:
Received chunk: <think>...

2025-04-01T21:22:50.364Z:
Received chunk: 

...

2025-04-01T21:22:50.446Z:
Received chunk: </think>...

2025-04-01T21:22:50.528Z:
Received chunk: 

...

2025-04-01T21:22:50.617Z:
Received chunk: It...

2025-04-01T21:22:50.702Z:
Received chunk:  seems...

2025-04-01T21:22:50.785Z:
Received chunk:  like...

2025-04-01T21:22:50.867Z:
Received chunk:  you...

2025-04-01T21:22:50.949Z:
Received chunk: 're...

2025-04-01T21:22:51.034Z:
Received chunk:  testing...

2025-04-01T21:22:51.114Z:
Received chunk:  the...

2025-04-01T21:22:51.195Z:
Received chunk:  assistant...

2025-04-01T21:22:51.276Z:
Received chunk: ....

2025-04-01T21:22:51.356Z:
Received chunk:  How...

2025-04-01T21:22:51.441Z:
Received chunk:  can...

2025-04-01T21:22:51.530Z:
Received chunk:  I...

2025-04-01T21:22:51.617Z:
Received chunk:  assist...

2025-04-01T21:22:51.704Z:
Received chunk:  you...

2025-04-01T21:22:51.788Z:
Received chunk:  today...

2025-04-01T21:22:51.869Z:
Received chunk: ?...

2025-04-01T21:22:51.951Z:
Received chunk: 

> ...

2025-04-01T21:26:01.791Z:
Unloading model...

2025-04-01T21:26:01.791Z:
Terminating running process

2025-04-01T21:26:01.792Z:
Model unloaded successfully

2025-04-01T21:26:01.793Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:26:01.793Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:26:01.793Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:26:01.794Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:26:02.200Z:
Process exited with code null

2025-04-01T21:26:02.200Z:
Process failed, falling back to simulation

2025-04-01T21:26:02.201Z:
Generating simulated response

2025-04-01T21:26:02.269Z:
Sent simulated chunk: I'm

2025-04-01T21:26:02.317Z:
Sent simulated chunk: a

2025-04-01T21:26:02.411Z:
Sent simulated chunk: locally

2025-04-01T21:26:02.503Z:
Sent simulated chunk: hosted

2025-04-01T21:26:02.582Z:
Sent simulated chunk: AI

2025-04-01T21:26:02.628Z:
Sent simulated chunk: assistant

2025-04-01T21:26:02.675Z:
Sent simulated chunk: running

2025-04-01T21:26:02.721Z:
Sent simulated chunk: directly

2025-04-01T21:26:02.798Z:
Sent simulated chunk: on

2025-04-01T21:26:02.832Z:
Unloading model...

2025-04-01T21:26:02.832Z:
Model unloaded successfully

2025-04-01T21:26:02.832Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:26:02.832Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:26:02.832Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:26:02.833Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:26:02.833Z:
Sent simulated chunk: your

2025-04-01T21:26:02.907Z:
Sent simulated chunk: device.

2025-04-01T21:26:02.984Z:
Sent simulated chunk: I

2025-04-01T21:26:03.079Z:
Sent simulated chunk: process

2025-04-01T21:26:03.155Z:
Sent simulated chunk: information

2025-04-01T21:26:03.202Z:
Sent simulated chunk: using

2025-04-01T21:26:03.279Z:
Sent simulated chunk: the

2025-04-01T21:26:03.373Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T21:26:03.467Z:
Sent simulated chunk: model

2025-04-01T21:26:03.560Z:
Sent simulated chunk: loaded

2025-04-01T21:26:03.639Z:
Sent simulated chunk: in

2025-04-01T21:26:03.685Z:
Sent simulated chunk: the

2025-04-01T21:26:03.746Z:
Sent simulated chunk: LM

2025-04-01T21:26:03.808Z:
Sent simulated chunk: Terminal

2025-04-01T21:26:03.886Z:
Sent simulated chunk: application.

2025-04-01T21:26:03.981Z:
Sent simulated chunk: I've

2025-04-01T21:26:04.074Z:
Sent simulated chunk: received

2025-04-01T21:26:04.121Z:
Sent simulated chunk: your

2025-04-01T21:26:04.214Z:
Sent simulated chunk: message:

2025-04-01T21:26:04.291Z:
Sent simulated chunk: "Test"

2025-04-01T21:26:04.337Z:
Sent simulated chunk: and

2025-04-01T21:26:04.431Z:
Sent simulated chunk: am

2025-04-01T21:26:04.494Z:
Sent simulated chunk: responding

2025-04-01T21:26:04.529Z:
Sent simulated chunk: without

2025-04-01T21:26:04.633Z:
Sent simulated chunk: requiring

2025-04-01T21:26:04.726Z:
Sent simulated chunk: internet

2025-04-01T21:26:04.804Z:
Sent simulated chunk: access.

(This

2025-04-01T21:26:04.895Z:
Sent simulated chunk: response

2025-04-01T21:26:05.004Z:
Sent simulated chunk: was

2025-04-01T21:26:05.066Z:
Sent simulated chunk: generated

2025-04-01T21:26:05.128Z:
Sent simulated chunk: using

2025-04-01T21:26:05.222Z:
Sent simulated chunk: a

2025-04-01T21:26:05.298Z:
Sent simulated chunk: simulated

2025-04-01T21:26:05.376Z:
Sent simulated chunk: version

2025-04-01T21:26:05.470Z:
Sent simulated chunk: of

2025-04-01T21:26:05.523Z:
Sent simulated chunk: the

2025-04-01T21:26:05.627Z:
Sent simulated chunk: DeepSeek-R1-Distill-Qwen-7B-Q4_K_M

2025-04-01T21:26:05.720Z:
Sent simulated chunk: model)

2025-04-01T21:26:05.720Z:
Simulated response generation completed

2025-04-01T21:26:24.122Z:
Unloading model...

2025-04-01T21:26:24.122Z:
Model unloaded successfully

2025-04-01T21:26:24.122Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:26:24.123Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:26:24.123Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:26:24.123Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:26:24.667Z:
Unloading model...

2025-04-01T21:26:24.667Z:
Model unloaded successfully

2025-04-01T21:26:24.667Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:26:24.667Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:26:24.668Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:26:24.668Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:29:49.927Z:
Unloading model...

2025-04-01T21:29:49.927Z:
Model unloaded successfully

2025-04-01T21:29:49.927Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:29:49.927Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:29:49.927Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:29:49.928Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:29:50.566Z:
Unloading model...

2025-04-01T21:29:50.566Z:
Model unloaded successfully

2025-04-01T21:29:50.566Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:29:50.567Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:29:50.567Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:29:50.567Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:29:58.130Z:
Generate response called with message: test...

2025-04-01T21:29:58.131Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:29:58.165Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:29:58.193Z:
Process stderr: llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It
llama_model_loader: - kv   3:                           general.finetune str              = it
llama_model_loader: - kv   4:                           general.basename str              = gemma-3
llama_model_loader: - kv   5:                         general.size_label str              = 4B
llama_model_loader: - kv   6:                            general.license str              = gemma
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Google
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...
llama_model_loader: - kv  11:                               general.tags arr[str,1]       = ["image-text-to-text"]
llama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072
llama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560
llama_model_loader: - kv  14:                         gemma3.block_count u32              = 34
llama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240
llama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8
llama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256
llama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256
llama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024
llama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4
llama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear
llama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000
llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default


2025-04-01T21:29:58.230Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...


2025-04-01T21:29:58.315Z:
Process stderr: llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...


2025-04-01T21:29:58.331Z:
Process stderr: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\n{%- if messages[0]['r...
llama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  38:               general.quantization_version u32              = 2
llama_model_loader: - kv  39:                          general.file_type u32              = 7
llama_model_loader: - type  f32:  205 tensors
llama_model_loader: - type q8_0:  239 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 3.84 GiB (8.50 BPW) 


2025-04-01T21:29:58.377Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-01T21:29:58.377Z:
Process stderr: load: special tokens cache size = 6414


2025-04-01T21:29:58.390Z:
Process stderr: load: token to piece cache size = 1.9446 MB
print_info: arch             = gemma3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2560
print_info: n_layer          = 34


2025-04-01T21:29:58.391Z:
Process stderr: print_info: n_head           = 8
print_info: n_head_kv        = 4
print_info: n_rot            = 256
print_info: n_swa            = 1024
print_info: n_swa_pattern    = 6
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 6.2e-02
print_info: n_ff             = 10240
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.125
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 4B
print_info: model params     = 3.88 B
print_info: general.name     = Gemma 3 4b It
print_info: vocab type       = SPM
print_info: n_vocab          = 262144
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 106 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 248 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 106 '<end_of_turn>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T21:29:58.615Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/35 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  3932.65 MiB


2025-04-01T21:29:58.616Z:
Process stderr: .......................

2025-04-01T21:29:58.616Z:
Process stderr: ..........

2025-04-01T21:29:58.617Z:
Process stderr: .....................

2025-04-01T21:29:58.618Z:
Process stderr: ..............................


2025-04-01T21:29:58.619Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.125
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-01T21:29:58.619Z:
Process stderr: llama_context:        CPU  output buffer size =     1.00 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 34, can_shift = 1


2025-04-01T21:29:58.665Z:
Process stderr: init:        CPU KV buffer size =   272.00 MiB
llama_context: KV self size  =  272.00 MiB, K (f16):  136.00 MiB, V (f16):  136.00 MiB


2025-04-01T21:29:58.670Z:
Process stderr: llama_context:        CPU compute buffer size =   522.00 MiB
llama_context: graph nodes  = 1435
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T21:29:59.082Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-01T21:29:59.083Z:
Process stderr: main: chat template example:
<start_of_turn>user
You are a helpful assistant

Hello<end_of_turn>
<start_of_turn>model
Hi there<end_of_turn>
<start_of_turn>user
How are you?<end_of_turn>
<start_of_turn>model


system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-01T21:29:59.084Z:
Process stderr: sampler seed: 636005676
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-01T21:29:59.268Z:
Received chunk: Okay...

2025-04-01T21:29:59.338Z:
Received chunk: !...

2025-04-01T21:29:59.409Z:
Received chunk:  Just...

2025-04-01T21:29:59.478Z:
Received chunk:  testing...

2025-04-01T21:29:59.548Z:
Received chunk:  to...

2025-04-01T21:29:59.618Z:
Received chunk:  see...

2025-04-01T21:29:59.690Z:
Received chunk:  if...

2025-04-01T21:29:59.763Z:
Received chunk:  I...

2025-04-01T21:29:59.835Z:
Received chunk: '...

2025-04-01T21:29:59.907Z:
Received chunk: m...

2025-04-01T21:29:59.979Z:
Received chunk:  working...

2025-04-01T21:30:00.051Z:
Received chunk: ....

2025-04-01T21:30:00.123Z:
Received chunk:  😊...

2025-04-01T21:30:00.195Z:
Received chunk:  ...

2025-04-01T21:30:00.267Z:
Received chunk: 

...

2025-04-01T21:30:00.338Z:
Received chunk: Is...

2025-04-01T21:30:00.409Z:
Received chunk:  there...

2025-04-01T21:30:00.481Z:
Received chunk:  anything...

2025-04-01T21:30:00.552Z:
Received chunk:  you...

2025-04-01T21:30:00.623Z:
Received chunk: ’...

2025-04-01T21:30:00.696Z:
Received chunk: d...

2025-04-01T21:30:00.770Z:
Received chunk:  like...

2025-04-01T21:30:00.844Z:
Received chunk:  me...

2025-04-01T21:30:00.916Z:
Received chunk:  to...

2025-04-01T21:30:00.988Z:
Received chunk:  do...

2025-04-01T21:30:01.059Z:
Received chunk:  now...

2025-04-01T21:30:01.130Z:
Received chunk: ?...

2025-04-01T21:30:01.201Z:
Received chunk:  Do...

2025-04-01T21:30:01.272Z:
Received chunk:  you...

2025-04-01T21:30:01.342Z:
Received chunk:  want...

2025-04-01T21:30:01.412Z:
Received chunk:  me...

2025-04-01T21:30:01.482Z:
Received chunk:  to...

2025-04-01T21:30:01.552Z:
Received chunk: :...

2025-04-01T21:30:01.623Z:
Received chunk: 

...

2025-04-01T21:30:01.695Z:
Received chunk: *...

2025-04-01T21:30:01.768Z:
Received chunk:    ...

2025-04-01T21:30:01.839Z:
Received chunk: Answer...

2025-04-01T21:30:01.912Z:
Received chunk:  a...

2025-04-01T21:30:01.984Z:
Received chunk:  question...

2025-04-01T21:30:02.054Z:
Received chunk: ?...

2025-04-01T21:30:02.124Z:
Received chunk: 
...

2025-04-01T21:30:02.195Z:
Received chunk: *...

2025-04-01T21:30:02.265Z:
Received chunk:    ...

2025-04-01T21:30:02.335Z:
Received chunk: Write...

2025-04-01T21:30:02.405Z:
Received chunk:  something...

2025-04-01T21:30:02.475Z:
Received chunk:  for...

2025-04-01T21:30:02.544Z:
Received chunk:  you...

2025-04-01T21:30:02.614Z:
Received chunk: ?...

2025-04-01T21:30:02.684Z:
Received chunk: 
...

2025-04-01T21:30:02.758Z:
Received chunk: *...

2025-04-01T21:30:02.840Z:
Received chunk:    ...

2025-04-01T21:30:02.905Z:
Received chunk: Play...

2025-04-01T21:30:02.978Z:
Received chunk:  a...

2025-04-01T21:30:03.050Z:
Received chunk:  game...

2025-04-01T21:30:03.121Z:
Received chunk: ?...

2025-04-01T21:30:03.192Z:
Received chunk: 

> ...

2025-04-01T21:31:29.149Z:
Unloading model...

2025-04-01T21:31:29.149Z:
Terminating running process

2025-04-01T21:31:29.149Z:
Model unloaded successfully

2025-04-01T21:31:29.150Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:31:29.151Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:31:29.151Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:31:29.151Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:31:29.437Z:
Process exited with code null

2025-04-01T21:31:29.437Z:
Process failed, falling back to simulation

2025-04-01T21:31:29.438Z:
Generating simulated response

2025-04-01T21:31:29.521Z:
Sent simulated chunk: I'm

2025-04-01T21:31:29.582Z:
Sent simulated chunk: a

2025-04-01T21:31:29.661Z:
Sent simulated chunk: locally

2025-04-01T21:31:29.754Z:
Sent simulated chunk: hosted

2025-04-01T21:31:29.832Z:
Sent simulated chunk: AI

2025-04-01T21:31:29.879Z:
Sent simulated chunk: assistant

2025-04-01T21:31:29.926Z:
Sent simulated chunk: running

2025-04-01T21:31:30.019Z:
Sent simulated chunk: directly

2025-04-01T21:31:30.067Z:
Sent simulated chunk: on

2025-04-01T21:31:30.160Z:
Sent simulated chunk: your

2025-04-01T21:31:30.207Z:
Sent simulated chunk: device.

2025-04-01T21:31:30.268Z:
Sent simulated chunk: I

2025-04-01T21:31:30.330Z:
Sent simulated chunk: process

2025-04-01T21:31:30.409Z:
Sent simulated chunk: information

2025-04-01T21:31:30.503Z:
Sent simulated chunk: using

2025-04-01T21:31:30.597Z:
Sent simulated chunk: the

2025-04-01T21:31:30.690Z:
Sent simulated chunk: gemma-3-4b-it-Q8_0

2025-04-01T21:31:30.784Z:
Sent simulated chunk: model

2025-04-01T21:31:30.878Z:
Sent simulated chunk: loaded

2025-04-01T21:31:30.972Z:
Sent simulated chunk: in

2025-04-01T21:31:31.067Z:
Sent simulated chunk: the

2025-04-01T21:31:31.098Z:
Sent simulated chunk: LM

2025-04-01T21:31:31.189Z:
Sent simulated chunk: Terminal

2025-04-01T21:31:31.237Z:
Sent simulated chunk: application.

2025-04-01T21:31:31.330Z:
Sent simulated chunk: I've

2025-04-01T21:31:31.394Z:
Sent simulated chunk: received

2025-04-01T21:31:31.472Z:
Sent simulated chunk: your

2025-04-01T21:31:31.565Z:
Sent simulated chunk: message:

2025-04-01T21:31:31.616Z:
Sent simulated chunk: "test"

2025-04-01T21:31:31.675Z:
Sent simulated chunk: and

2025-04-01T21:31:31.753Z:
Sent simulated chunk: am

2025-04-01T21:31:31.801Z:
Sent simulated chunk: responding

2025-04-01T21:31:31.879Z:
Sent simulated chunk: without

2025-04-01T21:31:31.958Z:
Sent simulated chunk: requiring

2025-04-01T21:31:32.005Z:
Sent simulated chunk: internet

2025-04-01T21:31:32.083Z:
Sent simulated chunk: access.

(This

2025-04-01T21:31:32.162Z:
Sent simulated chunk: response

2025-04-01T21:31:32.240Z:
Sent simulated chunk: was

2025-04-01T21:31:32.304Z:
Sent simulated chunk: generated

2025-04-01T21:31:32.384Z:
Sent simulated chunk: using

2025-04-01T21:31:32.446Z:
Sent simulated chunk: a

2025-04-01T21:31:32.541Z:
Sent simulated chunk: simulated

2025-04-01T21:31:32.634Z:
Sent simulated chunk: version

2025-04-01T21:31:32.729Z:
Sent simulated chunk: of

2025-04-01T21:31:32.823Z:
Sent simulated chunk: the

2025-04-01T21:31:32.871Z:
Sent simulated chunk: gemma-3-4b-it-Q8_0

2025-04-01T21:31:32.934Z:
Sent simulated chunk: model)

2025-04-01T21:31:32.934Z:
Simulated response generation completed

2025-04-01T21:33:57.947Z:
Unloading model...

2025-04-01T21:33:57.948Z:
Model unloaded successfully

2025-04-01T21:33:57.948Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:33:57.948Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:33:57.948Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:33:57.949Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:33:58.502Z:
Unloading model...

2025-04-01T21:33:58.502Z:
Model unloaded successfully

2025-04-01T21:33:58.503Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:33:58.503Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:33:58.503Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:33:58.504Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:34:27.741Z:
Unloading model...

2025-04-01T21:34:27.741Z:
Model unloaded successfully

2025-04-01T21:34:27.742Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:34:27.742Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:34:27.742Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:34:27.743Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:34:28.191Z:
Unloading model...

2025-04-01T21:34:28.191Z:
Model unloaded successfully

2025-04-01T21:34:28.192Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:34:28.192Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:34:28.193Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:34:28.193Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:36:55.664Z:
Unloading model...

2025-04-01T21:36:55.665Z:
Model unloaded successfully

2025-04-01T21:36:55.665Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:36:55.665Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:36:55.665Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:36:55.666Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:36:56.510Z:
Unloading model...

2025-04-01T21:36:56.510Z:
Model unloaded successfully

2025-04-01T21:36:56.510Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:36:56.511Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:36:56.511Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:36:56.511Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:37:11.930Z:
Generate response called with message: Test...

2025-04-01T21:37:11.930Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:37:11.955Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:37:11.984Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T21:37:12.006Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T21:37:12.015Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T21:37:12.038Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T21:37:12.091Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T21:37:12.108Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512


2025-04-01T21:37:12.109Z:
Process stderr: print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T21:37:17.795Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  7717.68 MiB


2025-04-01T21:37:17.803Z:
Process stderr: ........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T21:37:17.818Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T21:37:17.825Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T21:40:17.562Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:40:17.562Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:40:17.563Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:40:17.563Z:
Model mmproj-model-f16 file verified

2025-04-01T21:40:18.032Z:
Unloading model...

2025-04-01T21:40:18.032Z:
Model unloaded successfully

2025-04-01T21:40:18.032Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:40:18.033Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:40:18.033Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:40:18.033Z:
Model mmproj-model-f16 file verified

2025-04-01T21:40:21.104Z:
Generate response called with message: Test...

2025-04-01T21:40:21.104Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:40:21.203Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:40:21.210Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T21:40:21.210Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T21:40:21.211Z:
Process exited with code 1

2025-04-01T21:40:21.211Z:
Process failed, falling back to simulation

2025-04-01T21:40:21.212Z:
Generating simulated response

2025-04-01T21:40:21.296Z:
Sent simulated chunk: I'm

2025-04-01T21:40:21.374Z:
Sent simulated chunk: a

2025-04-01T21:40:21.437Z:
Sent simulated chunk: locally

2025-04-01T21:40:21.546Z:
Sent simulated chunk: hosted

2025-04-01T21:40:21.593Z:
Sent simulated chunk: AI

2025-04-01T21:40:21.688Z:
Sent simulated chunk: assistant

2025-04-01T21:40:21.766Z:
Sent simulated chunk: running

2025-04-01T21:40:21.828Z:
Sent simulated chunk: directly

2025-04-01T21:40:21.937Z:
Sent simulated chunk: on

2025-04-01T21:40:22.046Z:
Sent simulated chunk: your

2025-04-01T21:40:22.108Z:
Sent simulated chunk: device.

2025-04-01T21:40:22.185Z:
Sent simulated chunk: I

2025-04-01T21:40:22.263Z:
Sent simulated chunk: process

2025-04-01T21:40:22.374Z:
Sent simulated chunk: information

2025-04-01T21:40:22.435Z:
Sent simulated chunk: using

2025-04-01T21:40:22.482Z:
Sent simulated chunk: the

2025-04-01T21:40:22.529Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T21:40:22.607Z:
Sent simulated chunk: model

2025-04-01T21:40:22.669Z:
Sent simulated chunk: loaded

2025-04-01T21:40:22.732Z:
Sent simulated chunk: in

2025-04-01T21:40:22.795Z:
Sent simulated chunk: the

2025-04-01T21:40:22.841Z:
Sent simulated chunk: LM

2025-04-01T21:40:22.903Z:
Sent simulated chunk: Terminal

2025-04-01T21:40:22.950Z:
Sent simulated chunk: application.

2025-04-01T21:40:22.996Z:
Sent simulated chunk: I've

2025-04-01T21:40:23.105Z:
Sent simulated chunk: received

2025-04-01T21:40:23.151Z:
Sent simulated chunk: your

2025-04-01T21:40:23.260Z:
Sent simulated chunk: message:

2025-04-01T21:40:23.306Z:
Sent simulated chunk: "Test"

2025-04-01T21:40:23.353Z:
Sent simulated chunk: and

2025-04-01T21:40:23.446Z:
Sent simulated chunk: am

2025-04-01T21:40:23.508Z:
Sent simulated chunk: responding

2025-04-01T21:40:23.553Z:
Sent simulated chunk: without

2025-04-01T21:40:23.631Z:
Sent simulated chunk: requiring

2025-04-01T21:40:23.693Z:
Sent simulated chunk: internet

2025-04-01T21:40:23.740Z:
Sent simulated chunk: access.

(This

2025-04-01T21:40:23.817Z:
Sent simulated chunk: response

2025-04-01T21:40:23.879Z:
Sent simulated chunk: was

2025-04-01T21:40:23.942Z:
Sent simulated chunk: generated

2025-04-01T21:40:24.021Z:
Sent simulated chunk: using

2025-04-01T21:40:24.099Z:
Sent simulated chunk: a

2025-04-01T21:40:24.207Z:
Sent simulated chunk: simulated

2025-04-01T21:40:24.305Z:
Sent simulated chunk: version

2025-04-01T21:40:24.363Z:
Sent simulated chunk: of

2025-04-01T21:40:24.425Z:
Sent simulated chunk: the

2025-04-01T21:40:24.502Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T21:40:24.549Z:
Sent simulated chunk: model)

2025-04-01T21:40:24.550Z:
Simulated response generation completed

2025-04-01T21:42:50.837Z:
Unloading model...

2025-04-01T21:42:50.837Z:
Model unloaded successfully

2025-04-01T21:42:50.837Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:42:50.838Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:42:50.838Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:42:50.838Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:42:51.392Z:
Unloading model...

2025-04-01T21:42:51.393Z:
Model unloaded successfully

2025-04-01T21:42:51.393Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:42:51.393Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:42:51.393Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:42:51.394Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:43:49.203Z:
Unloading model...

2025-04-01T21:43:49.203Z:
Model unloaded successfully

2025-04-01T21:43:49.204Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:43:49.204Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:43:49.204Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:43:49.205Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:43:49.833Z:
Unloading model...

2025-04-01T21:43:49.833Z:
Model unloaded successfully

2025-04-01T21:43:49.833Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:43:49.833Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:43:49.834Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:43:49.834Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:45:07.571Z:
Unloading model...

2025-04-01T21:45:07.571Z:
Model unloaded successfully

2025-04-01T21:45:07.572Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:45:07.572Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:45:07.573Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:45:07.573Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:45:08.004Z:
Unloading model...

2025-04-01T21:45:08.004Z:
Model unloaded successfully

2025-04-01T21:45:08.005Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:45:08.005Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:45:08.005Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:45:08.005Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:46:09.747Z:
Unloading model...

2025-04-01T21:46:09.747Z:
Model unloaded successfully

2025-04-01T21:46:09.747Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:09.747Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:46:09.747Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:09.748Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:46:10.179Z:
Unloading model...

2025-04-01T21:46:10.179Z:
Model unloaded successfully

2025-04-01T21:46:10.180Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:10.180Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:46:10.180Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:10.181Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:46:45.982Z:
Unloading model...

2025-04-01T21:46:45.982Z:
Model unloaded successfully

2025-04-01T21:46:45.983Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:45.983Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:46:45.983Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:45.984Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:46:46.449Z:
Unloading model...

2025-04-01T21:46:46.449Z:
Model unloaded successfully

2025-04-01T21:46:46.449Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:46.449Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:46:46.450Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:46:46.451Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:46:49.821Z:
Generate response called with message: Test...

2025-04-01T21:46:49.822Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:46:49.852Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:46:49.882Z:
Process stderr: llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 7
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2


2025-04-01T21:46:49.904Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-01T21:46:49.914Z:
Process stderr: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-01T21:46:49.936Z:
Process stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q8_0:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 7.54 GiB (8.50 BPW) 


2025-04-01T21:46:49.987Z:
Process stderr: load: special tokens cache size = 22


2025-04-01T21:46:50.004Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0


2025-04-01T21:46:50.004Z:
Process stderr: print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-01T21:46:55.229Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  7717.68 MiB


2025-04-01T21:46:55.229Z:
Process stderr: ............................

2025-04-01T21:46:55.230Z:
Process stderr: .......................

2025-04-01T21:46:55.230Z:
Process stderr: ..................

2025-04-01T21:46:55.231Z:
Process stderr: ...................


2025-04-01T21:46:55.233Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.58 MiB


2025-04-01T21:46:55.234Z:
Process stderr: init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-01T21:46:55.252Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-01T21:46:55.257Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-01T21:47:35.882Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:47:35.882Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:47:35.883Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:47:35.883Z:
Model mmproj-model-f16 file verified

2025-04-01T21:47:36.360Z:
Unloading model...

2025-04-01T21:47:36.360Z:
Model unloaded successfully

2025-04-01T21:47:36.361Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:47:36.361Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:47:36.361Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:47:36.362Z:
Model mmproj-model-f16 file verified

2025-04-01T21:48:37.940Z:
Unloading model...

2025-04-01T21:48:37.940Z:
Model unloaded successfully

2025-04-01T21:48:37.941Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:48:37.941Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:48:37.942Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:48:37.942Z:
Model mmproj-model-f16 file verified

2025-04-01T21:48:38.368Z:
Unloading model...

2025-04-01T21:48:38.369Z:
Model unloaded successfully

2025-04-01T21:48:38.369Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:48:38.369Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:48:38.369Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:48:38.370Z:
Model mmproj-model-f16 file verified

2025-04-01T21:50:49.518Z:
Unloading model...

2025-04-01T21:50:49.518Z:
Model unloaded successfully

2025-04-01T21:50:49.518Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:50:49.518Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:50:49.519Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:50:49.520Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:50:50.040Z:
Unloading model...

2025-04-01T21:50:50.040Z:
Model unloaded successfully

2025-04-01T21:50:50.040Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:50:50.040Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:50:50.040Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:50:50.041Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:52:12.383Z:
Unloading model...

2025-04-01T21:52:12.383Z:
Model unloaded successfully

2025-04-01T21:52:12.383Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:52:12.383Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:52:12.384Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T21:52:12.384Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T21:53:32.888Z:
Unloading model...

2025-04-01T21:53:32.888Z:
Model unloaded successfully

2025-04-01T21:53:32.888Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:53:32.888Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:53:32.888Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:53:32.888Z:
Model mmproj-model-f16 file verified

2025-04-01T21:53:33.449Z:
Unloading model...

2025-04-01T21:53:33.449Z:
Model unloaded successfully

2025-04-01T21:53:33.449Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:53:33.450Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:53:33.450Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:53:33.450Z:
Model mmproj-model-f16 file verified

2025-04-01T21:53:37.290Z:
Generate response called with message: Test...

2025-04-01T21:53:37.291Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:53:37.317Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:53:37.325Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T21:53:37.325Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T21:53:37.326Z:
Process exited with code 1

2025-04-01T21:53:37.327Z:
Process failed, falling back to simulation

2025-04-01T21:53:37.327Z:
Generating simulated response

2025-04-01T21:53:37.423Z:
Sent simulated chunk: I'm

2025-04-01T21:53:37.486Z:
Sent simulated chunk: a

2025-04-01T21:53:37.564Z:
Sent simulated chunk: locally

2025-04-01T21:53:37.611Z:
Sent simulated chunk: hosted

2025-04-01T21:53:37.689Z:
Sent simulated chunk: AI

2025-04-01T21:53:37.799Z:
Sent simulated chunk: assistant

2025-04-01T21:53:37.846Z:
Sent simulated chunk: running

2025-04-01T21:53:37.925Z:
Sent simulated chunk: directly

2025-04-01T21:53:37.988Z:
Sent simulated chunk: on

2025-04-01T21:53:38.065Z:
Sent simulated chunk: your

2025-04-01T21:53:38.160Z:
Sent simulated chunk: device.

2025-04-01T21:53:38.222Z:
Sent simulated chunk: I

2025-04-01T21:53:38.330Z:
Sent simulated chunk: process

2025-04-01T21:53:38.397Z:
Sent simulated chunk: information

2025-04-01T21:53:38.428Z:
Sent simulated chunk: using

2025-04-01T21:53:38.533Z:
Sent simulated chunk: the

2025-04-01T21:53:38.611Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T21:53:38.719Z:
Sent simulated chunk: model

2025-04-01T21:53:38.781Z:
Sent simulated chunk: loaded

2025-04-01T21:53:38.858Z:
Sent simulated chunk: in

2025-04-01T21:53:38.951Z:
Sent simulated chunk: the

2025-04-01T21:53:38.998Z:
Sent simulated chunk: LM

2025-04-01T21:53:39.045Z:
Sent simulated chunk: Terminal

2025-04-01T21:53:39.138Z:
Sent simulated chunk: application.

2025-04-01T21:53:39.231Z:
Sent simulated chunk: I've

2025-04-01T21:53:39.324Z:
Sent simulated chunk: received

2025-04-01T21:53:39.402Z:
Sent simulated chunk: your

2025-04-01T21:53:39.509Z:
Sent simulated chunk: message:

2025-04-01T21:53:39.617Z:
Sent simulated chunk: "Test"

2025-04-01T21:53:39.665Z:
Sent simulated chunk: and

2025-04-01T21:53:39.759Z:
Sent simulated chunk: am

2025-04-01T21:53:39.852Z:
Sent simulated chunk: responding

2025-04-01T21:53:39.947Z:
Sent simulated chunk: without

2025-04-01T21:53:40.024Z:
Sent simulated chunk: requiring

2025-04-01T21:53:40.069Z:
Sent simulated chunk: internet

2025-04-01T21:53:40.178Z:
Sent simulated chunk: access.

(This

2025-04-01T21:53:40.256Z:
Sent simulated chunk: response

2025-04-01T21:53:40.302Z:
Sent simulated chunk: was

2025-04-01T21:53:40.396Z:
Sent simulated chunk: generated

2025-04-01T21:53:40.474Z:
Sent simulated chunk: using

2025-04-01T21:53:40.520Z:
Sent simulated chunk: a

2025-04-01T21:53:40.583Z:
Sent simulated chunk: simulated

2025-04-01T21:53:40.644Z:
Sent simulated chunk: version

2025-04-01T21:53:40.691Z:
Sent simulated chunk: of

2025-04-01T21:53:40.752Z:
Sent simulated chunk: the

2025-04-01T21:53:40.814Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T21:53:40.860Z:
Sent simulated chunk: model)

2025-04-01T21:53:40.860Z:
Simulated response generation completed

2025-04-01T21:54:30.798Z:
Unloading model...

2025-04-01T21:54:30.799Z:
Model unloaded successfully

2025-04-01T21:54:30.799Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:54:30.799Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:54:30.799Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:54:30.800Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:54:35.504Z:
Unloading model...

2025-04-01T21:54:35.505Z:
Model unloaded successfully

2025-04-01T21:54:35.505Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:54:35.505Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:54:35.505Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:54:35.506Z:
Model mmproj-model-f16 file verified

2025-04-01T21:55:40.448Z:
Unloading model...

2025-04-01T21:55:40.448Z:
Model unloaded successfully

2025-04-01T21:55:40.448Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:55:40.448Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:55:40.449Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:55:40.449Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:55:40.876Z:
Unloading model...

2025-04-01T21:55:40.876Z:
Model unloaded successfully

2025-04-01T21:55:40.876Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:55:40.876Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:55:40.877Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T21:55:40.878Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T21:56:48.304Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:56:48.304Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:56:48.305Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:56:48.305Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:56:50.401Z:
Unloading model...

2025-04-01T21:56:50.401Z:
Model unloaded successfully

2025-04-01T21:56:50.401Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:56:50.401Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:56:50.402Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T21:56:50.402Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T21:57:31.431Z:
Unloading model...

2025-04-01T21:57:31.431Z:
Model unloaded successfully

2025-04-01T21:57:31.431Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:57:31.432Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:57:31.432Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:57:31.432Z:
Model mmproj-model-f16 file verified

2025-04-01T21:57:32.131Z:
Unloading model...

2025-04-01T21:57:32.131Z:
Model unloaded successfully

2025-04-01T21:57:32.131Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:57:32.131Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T21:57:32.131Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T21:57:32.132Z:
Model mmproj-model-f16 file verified

2025-04-01T21:57:34.389Z:
Generate response called with message: Hi...

2025-04-01T21:57:34.389Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T21:57:34.416Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T21:57:34.417Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T21:57:34.417Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T21:57:34.418Z:
Process exited with code 1

2025-04-01T21:57:34.418Z:
Process failed, falling back to simulation

2025-04-01T21:57:34.419Z:
Generating simulated response

2025-04-01T21:57:34.475Z:
Sent simulated chunk: Hello!

2025-04-01T21:57:34.569Z:
Sent simulated chunk: I'm

2025-04-01T21:57:34.663Z:
Sent simulated chunk: a

2025-04-01T21:57:34.757Z:
Sent simulated chunk: local

2025-04-01T21:57:34.849Z:
Sent simulated chunk: AI

2025-04-01T21:57:34.911Z:
Sent simulated chunk: assistant

2025-04-01T21:57:35.020Z:
Sent simulated chunk: running

2025-04-01T21:57:35.082Z:
Sent simulated chunk: on

2025-04-01T21:57:35.159Z:
Sent simulated chunk: your

2025-04-01T21:57:35.207Z:
Sent simulated chunk: device

2025-04-01T21:57:35.253Z:
Sent simulated chunk: through

2025-04-01T21:57:35.362Z:
Sent simulated chunk: the

2025-04-01T21:57:35.408Z:
Sent simulated chunk: LM

2025-04-01T21:57:35.501Z:
Sent simulated chunk: Terminal.

2025-04-01T21:57:35.565Z:
Sent simulated chunk: How

2025-04-01T21:57:35.612Z:
Sent simulated chunk: can

2025-04-01T21:57:35.659Z:
Sent simulated chunk: I

2025-04-01T21:57:35.766Z:
Sent simulated chunk: help

2025-04-01T21:57:35.844Z:
Sent simulated chunk: you

2025-04-01T21:57:35.937Z:
Sent simulated chunk: today?

(This

2025-04-01T21:57:35.984Z:
Sent simulated chunk: response

2025-04-01T21:57:36.030Z:
Sent simulated chunk: was

2025-04-01T21:57:36.123Z:
Sent simulated chunk: generated

2025-04-01T21:57:36.186Z:
Sent simulated chunk: using

2025-04-01T21:57:36.282Z:
Sent simulated chunk: a

2025-04-01T21:57:36.391Z:
Sent simulated chunk: simulated

2025-04-01T21:57:36.438Z:
Sent simulated chunk: version

2025-04-01T21:57:36.535Z:
Sent simulated chunk: of

2025-04-01T21:57:36.600Z:
Sent simulated chunk: the

2025-04-01T21:57:36.686Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T21:57:36.763Z:
Sent simulated chunk: model)

2025-04-01T21:57:36.764Z:
Simulated response generation completed

2025-04-01T22:07:02.342Z:
Unloading model...

2025-04-01T22:07:02.342Z:
Model unloaded successfully

2025-04-01T22:07:02.342Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T22:07:02.342Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:07:02.342Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T22:07:02.343Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T22:07:02.834Z:
Unloading model...

2025-04-01T22:07:02.835Z:
Model unloaded successfully

2025-04-01T22:07:02.835Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T22:07:02.835Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:07:02.835Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T22:07:02.836Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T22:07:49.332Z:
Unloading model...

2025-04-01T22:07:49.332Z:
Model unloaded successfully

2025-04-01T22:07:49.332Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:07:49.332Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:07:49.332Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:07:49.333Z:
Model mmproj-model-f16 file verified

2025-04-01T22:08:42.427Z:
Unloading model...

2025-04-01T22:08:42.427Z:
Model unloaded successfully

2025-04-01T22:08:42.427Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:08:42.428Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:08:42.428Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:08:42.428Z:
Model mmproj-model-f16 file verified

2025-04-01T22:08:46.826Z:
Unloading model...

2025-04-01T22:08:46.826Z:
Model unloaded successfully

2025-04-01T22:08:46.826Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T22:08:46.826Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:08:46.827Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-01T22:08:46.827Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-01T22:18:55.220Z:
Unloading model...

2025-04-01T22:18:55.220Z:
Model unloaded successfully

2025-04-01T22:18:55.220Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:18:55.220Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:18:55.221Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:18:55.221Z:
Model mmproj-model-f16 file verified

2025-04-01T22:18:58.111Z:
Generate response called with message: hi...

2025-04-01T22:18:58.111Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T22:18:58.135Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T22:18:58.136Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T22:18:58.137Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T22:18:58.138Z:
Process exited with code 1

2025-04-01T22:18:58.138Z:
Process failed, falling back to simulation

2025-04-01T22:18:58.139Z:
Generating simulated response

2025-04-01T22:18:58.212Z:
Sent simulated chunk: Hello!

2025-04-01T22:18:58.259Z:
Sent simulated chunk: I'm

2025-04-01T22:18:58.321Z:
Sent simulated chunk: a

2025-04-01T22:18:58.430Z:
Sent simulated chunk: local

2025-04-01T22:18:58.497Z:
Sent simulated chunk: AI

2025-04-01T22:18:58.602Z:
Sent simulated chunk: assistant

2025-04-01T22:18:58.664Z:
Sent simulated chunk: running

2025-04-01T22:18:58.758Z:
Sent simulated chunk: on

2025-04-01T22:18:58.852Z:
Sent simulated chunk: your

2025-04-01T22:18:58.944Z:
Sent simulated chunk: device

2025-04-01T22:18:59.053Z:
Sent simulated chunk: through

2025-04-01T22:18:59.130Z:
Sent simulated chunk: the

2025-04-01T22:18:59.209Z:
Sent simulated chunk: LM

2025-04-01T22:18:59.286Z:
Sent simulated chunk: Terminal.

2025-04-01T22:18:59.379Z:
Sent simulated chunk: How

2025-04-01T22:18:59.441Z:
Sent simulated chunk: can

2025-04-01T22:18:59.535Z:
Sent simulated chunk: I

2025-04-01T22:18:59.599Z:
Sent simulated chunk: help

2025-04-01T22:18:59.646Z:
Sent simulated chunk: you

2025-04-01T22:18:59.739Z:
Sent simulated chunk: today?

(This

2025-04-01T22:18:59.786Z:
Sent simulated chunk: response

2025-04-01T22:18:59.864Z:
Sent simulated chunk: was

2025-04-01T22:18:59.926Z:
Sent simulated chunk: generated

2025-04-01T22:18:59.987Z:
Sent simulated chunk: using

2025-04-01T22:19:00.080Z:
Sent simulated chunk: a

2025-04-01T22:19:00.126Z:
Sent simulated chunk: simulated

2025-04-01T22:19:00.235Z:
Sent simulated chunk: version

2025-04-01T22:19:00.328Z:
Sent simulated chunk: of

2025-04-01T22:19:00.375Z:
Sent simulated chunk: the

2025-04-01T22:19:00.421Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T22:19:00.498Z:
Sent simulated chunk: model)

2025-04-01T22:19:00.498Z:
Simulated response generation completed

2025-04-01T22:56:16.951Z:
Unloading model...

2025-04-01T22:56:16.952Z:
Model unloaded successfully

2025-04-01T22:56:16.952Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:56:16.952Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:56:16.952Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:56:16.952Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T22:56:17.755Z:
Unloading model...

2025-04-01T22:56:17.755Z:
Model unloaded successfully

2025-04-01T22:56:17.755Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:56:17.755Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:56:17.756Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:56:17.757Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T22:56:49.167Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:56:49.167Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:56:49.167Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:56:49.167Z:
Model mmproj-model-f16 file verified

2025-04-01T22:56:49.550Z:
Unloading model...

2025-04-01T22:56:49.550Z:
Model unloaded successfully

2025-04-01T22:56:49.551Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:56:49.551Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:56:49.551Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:56:49.552Z:
Model mmproj-model-f16 file verified

2025-04-01T22:58:47.513Z:
Unloading model...

2025-04-01T22:58:47.513Z:
Model unloaded successfully

2025-04-01T22:58:47.513Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:58:47.513Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:58:47.514Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T22:58:47.514Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T22:58:53.320Z:
Unloading model...

2025-04-01T22:58:53.320Z:
Model unloaded successfully

2025-04-01T22:58:53.320Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:58:53.320Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T22:58:53.320Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T22:58:53.321Z:
Model mmproj-model-f16 file verified

2025-04-01T23:02:10.049Z:
Unloading model...

2025-04-01T23:02:10.050Z:
Model unloaded successfully

2025-04-01T23:02:10.050Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:02:10.050Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:02:10.050Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:02:10.051Z:
Model mmproj-model-f16 file verified

2025-04-01T23:02:10.469Z:
Unloading model...

2025-04-01T23:02:10.469Z:
Model unloaded successfully

2025-04-01T23:02:10.469Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:02:10.470Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:02:10.470Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:02:10.470Z:
Model mmproj-model-f16 file verified

2025-04-01T23:02:39.885Z:
Generate response called with message: Hi...

2025-04-01T23:02:39.886Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T23:02:39.920Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T23:02:39.926Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-01T23:02:39.927Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T23:02:39.928Z:
Process exited with code 1

2025-04-01T23:02:39.928Z:
Process failed, falling back to simulation

2025-04-01T23:02:39.928Z:
Generating simulated response

2025-04-01T23:02:40.018Z:
Sent simulated chunk: Hello!

2025-04-01T23:02:40.079Z:
Sent simulated chunk: I'm

2025-04-01T23:02:40.125Z:
Sent simulated chunk: a

2025-04-01T23:02:40.217Z:
Sent simulated chunk: local

2025-04-01T23:02:40.264Z:
Sent simulated chunk: AI

2025-04-01T23:02:40.310Z:
Sent simulated chunk: assistant

2025-04-01T23:02:40.387Z:
Sent simulated chunk: running

2025-04-01T23:02:40.418Z:
Sent simulated chunk: on

2025-04-01T23:02:40.528Z:
Sent simulated chunk: your

2025-04-01T23:02:40.622Z:
Sent simulated chunk: device

2025-04-01T23:02:40.683Z:
Sent simulated chunk: through

2025-04-01T23:02:40.761Z:
Sent simulated chunk: the

2025-04-01T23:02:40.808Z:
Sent simulated chunk: LM

2025-04-01T23:02:40.870Z:
Sent simulated chunk: Terminal.

2025-04-01T23:02:40.964Z:
Sent simulated chunk: How

2025-04-01T23:02:41.011Z:
Sent simulated chunk: can

2025-04-01T23:02:41.058Z:
Sent simulated chunk: I

2025-04-01T23:02:41.134Z:
Sent simulated chunk: help

2025-04-01T23:02:41.227Z:
Sent simulated chunk: you

2025-04-01T23:02:41.336Z:
Sent simulated chunk: today?

(This

2025-04-01T23:02:41.430Z:
Sent simulated chunk: response

2025-04-01T23:02:41.477Z:
Sent simulated chunk: was

2025-04-01T23:02:41.538Z:
Sent simulated chunk: generated

2025-04-01T23:02:41.617Z:
Sent simulated chunk: using

2025-04-01T23:02:41.726Z:
Sent simulated chunk: a

2025-04-01T23:02:41.804Z:
Sent simulated chunk: simulated

2025-04-01T23:02:41.851Z:
Sent simulated chunk: version

2025-04-01T23:02:41.898Z:
Sent simulated chunk: of

2025-04-01T23:02:41.929Z:
Sent simulated chunk: the

2025-04-01T23:02:41.992Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T23:02:42.100Z:
Sent simulated chunk: model)

2025-04-01T23:02:42.100Z:
Simulated response generation completed

2025-04-01T23:08:35.363Z:
Unloading model...

2025-04-01T23:08:35.363Z:
Model unloaded successfully

2025-04-01T23:08:35.363Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T23:08:35.363Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:08:35.363Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Qwen2.5-Coder-7B-Instruct-GGUF\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf

2025-04-01T23:08:35.364Z:
Model Qwen2.5-Coder-7B-Instruct-Q8_0 file verified

2025-04-01T23:13:55.506Z:
Unloading model...

2025-04-01T23:13:55.506Z:
Model unloaded successfully

2025-04-01T23:13:55.506Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:13:55.506Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:13:55.507Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:13:55.507Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T23:13:56.370Z:
Unloading model...

2025-04-01T23:13:56.371Z:
Model unloaded successfully

2025-04-01T23:13:56.371Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:13:56.371Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:13:56.371Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:13:56.371Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T23:14:22.324Z:
Unloading model...

2025-04-01T23:14:22.325Z:
Model unloaded successfully

2025-04-01T23:14:22.325Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:22.325Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:14:22.325Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:22.326Z:
Model mmproj-model-f16 file verified

2025-04-01T23:14:46.288Z:
Unloading model...

2025-04-01T23:14:46.288Z:
Model unloaded successfully

2025-04-01T23:14:46.288Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:46.288Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:14:46.289Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:46.290Z:
Model mmproj-model-f16 file verified

2025-04-01T23:14:46.724Z:
Unloading model...

2025-04-01T23:14:46.725Z:
Model unloaded successfully

2025-04-01T23:14:46.725Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:46.725Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:14:46.726Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:14:46.726Z:
Model mmproj-model-f16 file verified

2025-04-01T23:15:09.794Z:
Unloading model...

2025-04-01T23:15:09.794Z:
Model unloaded successfully

2025-04-01T23:15:09.794Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:09.795Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:15:09.795Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:09.795Z:
Model mmproj-model-f16 file verified

2025-04-01T23:15:10.189Z:
Unloading model...

2025-04-01T23:15:10.189Z:
Model unloaded successfully

2025-04-01T23:15:10.189Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:10.189Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:15:10.189Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:10.190Z:
Model mmproj-model-f16 file verified

2025-04-01T23:15:24.105Z:
Unloading model...

2025-04-01T23:15:24.105Z:
Model unloaded successfully

2025-04-01T23:15:24.106Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:24.106Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:15:24.106Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:24.107Z:
Model mmproj-model-f16 file verified

2025-04-01T23:15:24.495Z:
Unloading model...

2025-04-01T23:15:24.495Z:
Model unloaded successfully

2025-04-01T23:15:24.495Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:24.495Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:15:24.496Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:24.496Z:
Model mmproj-model-f16 file verified

2025-04-01T23:15:56.889Z:
Unloading model...

2025-04-01T23:15:56.889Z:
Model unloaded successfully

2025-04-01T23:15:56.889Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:56.889Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:15:56.890Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:15:56.891Z:
Model mmproj-model-f16 file verified

2025-04-01T23:16:24.426Z:
Unloading model...

2025-04-01T23:16:24.427Z:
Model unloaded successfully

2025-04-01T23:16:24.427Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:16:24.427Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:16:24.427Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:16:24.428Z:
Model mmproj-model-f16 file verified

2025-04-01T23:16:24.838Z:
Unloading model...

2025-04-01T23:16:24.838Z:
Model unloaded successfully

2025-04-01T23:16:24.838Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:16:24.839Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:16:24.839Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:16:24.839Z:
Model mmproj-model-f16 file verified

2025-04-01T23:18:25.186Z:
Unloading model...

2025-04-01T23:18:25.186Z:
Model unloaded successfully

2025-04-01T23:18:25.186Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:25.186Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:18:25.186Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:25.187Z:
Model mmproj-model-f16 file verified

2025-04-01T23:18:37.915Z:
Unloading model...

2025-04-01T23:18:37.915Z:
Model unloaded successfully

2025-04-01T23:18:37.915Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:37.915Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:18:37.915Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:37.916Z:
Model mmproj-model-f16 file verified

2025-04-01T23:18:38.402Z:
Unloading model...

2025-04-01T23:18:38.402Z:
Model unloaded successfully

2025-04-01T23:18:38.403Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:38.403Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:18:38.403Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:18:38.403Z:
Model mmproj-model-f16 file verified

2025-04-01T23:18:58.983Z:
Unloading model...

2025-04-01T23:18:58.983Z:
Model unloaded successfully

2025-04-01T23:18:58.983Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:18:58.983Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:18:58.983Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:18:58.984Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T23:19:08.309Z:
Unloading model...

2025-04-01T23:19:08.309Z:
Model unloaded successfully

2025-04-01T23:19:08.310Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:19:08.310Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:19:08.310Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:19:08.311Z:
Model mmproj-model-f16 file verified

2025-04-01T23:26:56.193Z:
Unloading model...

2025-04-01T23:26:56.193Z:
Model unloaded successfully

2025-04-01T23:26:56.193Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:26:56.194Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:26:56.194Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:26:56.195Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T23:26:56.714Z:
Unloading model...

2025-04-01T23:26:56.714Z:
Model unloaded successfully

2025-04-01T23:26:56.714Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:26:56.714Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:26:56.715Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-01T23:26:56.715Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-01T23:28:33.793Z:
Unloading model...

2025-04-01T23:28:33.793Z:
Model unloaded successfully

2025-04-01T23:28:33.793Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:28:33.794Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:28:33.794Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:28:33.794Z:
Model mmproj-model-f16 file verified

2025-04-01T23:28:34.373Z:
Unloading model...

2025-04-01T23:28:34.373Z:
Model unloaded successfully

2025-04-01T23:28:34.373Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:28:34.374Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:28:34.374Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:28:34.374Z:
Model mmproj-model-f16 file verified

2025-04-01T23:35:33.730Z:
Unloading model...

2025-04-01T23:35:33.730Z:
Model unloaded successfully

2025-04-01T23:35:33.730Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:35:33.731Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:35:33.731Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:35:33.731Z:
Model mmproj-model-f16 file verified

2025-04-01T23:43:39.338Z:
Unloading model...

2025-04-01T23:43:39.338Z:
Model unloaded successfully

2025-04-01T23:43:39.338Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:43:39.339Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:43:39.339Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:43:39.339Z:
Model mmproj-model-f16 file verified

2025-04-01T23:43:40.148Z:
Unloading model...

2025-04-01T23:43:40.149Z:
Model unloaded successfully

2025-04-01T23:43:40.149Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:43:40.149Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:43:40.149Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:43:40.149Z:
Model mmproj-model-f16 file verified

2025-04-01T23:45:14.337Z:
Unloading model...

2025-04-01T23:45:14.337Z:
Model unloaded successfully

2025-04-01T23:45:14.337Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:45:14.337Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:45:14.337Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:45:14.338Z:
Model mmproj-model-f16 file verified

2025-04-01T23:45:14.751Z:
Unloading model...

2025-04-01T23:45:14.752Z:
Model unloaded successfully

2025-04-01T23:45:14.752Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:45:14.752Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:45:14.752Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:45:14.753Z:
Model mmproj-model-f16 file verified

2025-04-01T23:47:19.303Z:
Unloading model...

2025-04-01T23:47:19.303Z:
Model unloaded successfully

2025-04-01T23:47:19.303Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:47:19.303Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:47:19.303Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:47:19.304Z:
Model mmproj-model-f16 file verified

2025-04-01T23:49:12.448Z:
Unloading model...

2025-04-01T23:49:12.448Z:
Model unloaded successfully

2025-04-01T23:49:12.448Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:12.449Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:49:12.449Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:12.450Z:
Model mmproj-model-f16 file verified

2025-04-01T23:49:12.827Z:
Unloading model...

2025-04-01T23:49:12.827Z:
Model unloaded successfully

2025-04-01T23:49:12.827Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:12.827Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:49:12.828Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:12.828Z:
Model mmproj-model-f16 file verified

2025-04-01T23:49:54.357Z:
Unloading model...

2025-04-01T23:49:54.358Z:
Model unloaded successfully

2025-04-01T23:49:54.358Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:54.358Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:49:54.358Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:54.358Z:
Model mmproj-model-f16 file verified

2025-04-01T23:49:54.613Z:
Unloading model...

2025-04-01T23:49:54.613Z:
Model unloaded successfully

2025-04-01T23:49:54.613Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:54.614Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:49:54.614Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:49:54.614Z:
Model mmproj-model-f16 file verified

2025-04-01T23:50:09.583Z:
Unloading model...

2025-04-01T23:50:09.583Z:
Model unloaded successfully

2025-04-01T23:50:09.583Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:09.584Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:50:09.584Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:09.584Z:
Model mmproj-model-f16 file verified

2025-04-01T23:50:36.526Z:
Unloading model...

2025-04-01T23:50:36.526Z:
Model unloaded successfully

2025-04-01T23:50:36.527Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:36.527Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:50:36.528Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:36.528Z:
Model mmproj-model-f16 file verified

2025-04-01T23:50:36.730Z:
Unloading model...

2025-04-01T23:50:36.730Z:
Model unloaded successfully

2025-04-01T23:50:36.730Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:36.730Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:50:36.731Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:50:36.731Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:12.355Z:
Unloading model...

2025-04-01T23:51:12.355Z:
Model unloaded successfully

2025-04-01T23:51:12.356Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:12.356Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:12.356Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:12.357Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:12.776Z:
Unloading model...

2025-04-01T23:51:12.776Z:
Model unloaded successfully

2025-04-01T23:51:12.776Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:12.776Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:12.777Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:12.777Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:35.063Z:
Unloading model...

2025-04-01T23:51:35.063Z:
Model unloaded successfully

2025-04-01T23:51:35.064Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:35.064Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:35.064Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:35.064Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:35.414Z:
Unloading model...

2025-04-01T23:51:35.414Z:
Model unloaded successfully

2025-04-01T23:51:35.414Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:35.414Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:35.415Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:35.415Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:52.355Z:
Unloading model...

2025-04-01T23:51:52.355Z:
Model unloaded successfully

2025-04-01T23:51:52.356Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:52.356Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:52.356Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:52.356Z:
Model mmproj-model-f16 file verified

2025-04-01T23:51:52.577Z:
Unloading model...

2025-04-01T23:51:52.577Z:
Model unloaded successfully

2025-04-01T23:51:52.577Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:52.577Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:51:52.577Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:51:52.578Z:
Model mmproj-model-f16 file verified

2025-04-01T23:52:09.029Z:
Unloading model...

2025-04-01T23:52:09.029Z:
Model unloaded successfully

2025-04-01T23:52:09.029Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:52:09.029Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:52:09.030Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:52:09.030Z:
Model mmproj-model-f16 file verified

2025-04-01T23:52:24.758Z:
Unloading model...

2025-04-01T23:52:24.758Z:
Model unloaded successfully

2025-04-01T23:52:24.758Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:52:24.758Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:52:24.759Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:52:24.759Z:
Model mmproj-model-f16 file verified

2025-04-01T23:52:36.934Z:
Generate response called with message: Hi...

2025-04-01T23:52:36.934Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Hi
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-01T23:52:36.960Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-01T23:52:36.961Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)


2025-04-01T23:52:36.961Z:
Process stderr: print_info: file size   = 811.79 MiB (16.22 BPW) 
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-01T23:52:36.962Z:
Process exited with code 1

2025-04-01T23:52:36.962Z:
Process failed, falling back to simulation

2025-04-01T23:52:36.963Z:
Generating simulated response

2025-04-01T23:52:37.071Z:
Sent simulated chunk: Hello!

2025-04-01T23:52:37.117Z:
Sent simulated chunk: I'm

2025-04-01T23:52:37.195Z:
Sent simulated chunk: a

2025-04-01T23:52:37.304Z:
Sent simulated chunk: local

2025-04-01T23:52:37.383Z:
Sent simulated chunk: AI

2025-04-01T23:52:37.445Z:
Sent simulated chunk: assistant

2025-04-01T23:52:37.537Z:
Sent simulated chunk: running

2025-04-01T23:52:37.589Z:
Sent simulated chunk: on

2025-04-01T23:52:37.631Z:
Sent simulated chunk: your

2025-04-01T23:52:37.708Z:
Sent simulated chunk: device

2025-04-01T23:52:37.786Z:
Sent simulated chunk: through

2025-04-01T23:52:37.865Z:
Sent simulated chunk: the

2025-04-01T23:52:37.942Z:
Sent simulated chunk: LM

2025-04-01T23:52:37.989Z:
Sent simulated chunk: Terminal.

2025-04-01T23:52:38.066Z:
Sent simulated chunk: How

2025-04-01T23:52:38.176Z:
Sent simulated chunk: can

2025-04-01T23:52:38.285Z:
Sent simulated chunk: I

2025-04-01T23:52:38.332Z:
Sent simulated chunk: help

2025-04-01T23:52:38.411Z:
Sent simulated chunk: you

2025-04-01T23:52:38.457Z:
Sent simulated chunk: today?

(This

2025-04-01T23:52:38.518Z:
Sent simulated chunk: response

2025-04-01T23:52:38.568Z:
Sent simulated chunk: was

2025-04-01T23:52:38.627Z:
Sent simulated chunk: generated

2025-04-01T23:52:38.721Z:
Sent simulated chunk: using

2025-04-01T23:52:38.829Z:
Sent simulated chunk: a

2025-04-01T23:52:38.890Z:
Sent simulated chunk: simulated

2025-04-01T23:52:38.966Z:
Sent simulated chunk: version

2025-04-01T23:52:39.044Z:
Sent simulated chunk: of

2025-04-01T23:52:39.122Z:
Sent simulated chunk: the

2025-04-01T23:52:39.171Z:
Sent simulated chunk: mmproj-model-f16

2025-04-01T23:52:39.216Z:
Sent simulated chunk: model)

2025-04-01T23:52:39.217Z:
Simulated response generation completed

2025-04-01T23:55:09.392Z:
Unloading model...

2025-04-01T23:55:09.392Z:
Model unloaded successfully

2025-04-01T23:55:09.392Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:55:09.392Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:55:09.392Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:55:09.393Z:
Model mmproj-model-f16 file verified

2025-04-01T23:55:10.057Z:
Unloading model...

2025-04-01T23:55:10.057Z:
Model unloaded successfully

2025-04-01T23:55:10.057Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:55:10.057Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-01T23:55:10.057Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-01T23:55:10.058Z:
Model mmproj-model-f16 file verified

2025-04-02T00:00:14.998Z:
Unloading model...

2025-04-02T00:00:14.998Z:
Model unloaded successfully

2025-04-02T00:00:14.999Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:00:14.999Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T00:00:14.999Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:00:14.999Z:
Model mmproj-model-f16 file verified

2025-04-02T00:03:38.351Z:
Unloading model...

2025-04-02T00:03:38.351Z:
Model unloaded successfully

2025-04-02T00:03:38.351Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:03:38.351Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T00:03:38.353Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:03:38.353Z:
Model mmproj-model-f16 file verified

2025-04-02T00:03:38.792Z:
Unloading model...

2025-04-02T00:03:38.792Z:
Model unloaded successfully

2025-04-02T00:03:38.792Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:03:38.792Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T00:03:38.792Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:03:38.793Z:
Model mmproj-model-f16 file verified

2025-04-02T00:08:01.717Z:
Unloading model...

2025-04-02T00:08:01.718Z:
Model unloaded successfully

2025-04-02T00:08:01.718Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:08:01.718Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T00:08:01.718Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:08:01.718Z:
Model mmproj-model-f16 file verified

2025-04-02T00:49:44.729Z:
Unloading model...

2025-04-02T00:49:44.730Z:
Model unloaded successfully

2025-04-02T00:49:44.730Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:49:44.730Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T00:49:44.730Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T00:49:44.731Z:
Model mmproj-model-f16 file verified

2025-04-02T01:39:23.749Z:
Unloading model...

2025-04-02T01:39:23.750Z:
Model unloaded successfully

2025-04-02T01:39:23.750Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T01:39:23.750Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T01:39:23.750Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T01:39:23.751Z:
Model mmproj-model-f16 file verified

2025-04-02T01:39:40.539Z:
Generate response called with message: Hi friend what orthodontists are near me...

2025-04-02T01:39:40.540Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Hi friend what orthodontists are near me
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T01:39:40.566Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T01:39:40.567Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-02T01:39:40.568Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T01:39:40.569Z:
Process exited with code 1

2025-04-02T01:39:40.569Z:
Process failed, falling back to simulation

2025-04-02T01:39:40.569Z:
Generating simulated response

2025-04-02T01:39:40.659Z:
Sent simulated chunk: Hello!

2025-04-02T01:39:40.721Z:
Sent simulated chunk: I'm

2025-04-02T01:39:40.767Z:
Sent simulated chunk: a

2025-04-02T01:39:40.845Z:
Sent simulated chunk: local

2025-04-02T01:39:40.907Z:
Sent simulated chunk: AI

2025-04-02T01:39:41.016Z:
Sent simulated chunk: assistant

2025-04-02T01:39:41.047Z:
Sent simulated chunk: running

2025-04-02T01:39:41.141Z:
Sent simulated chunk: on

2025-04-02T01:39:41.187Z:
Sent simulated chunk: your

2025-04-02T01:39:41.249Z:
Sent simulated chunk: device

2025-04-02T01:39:41.311Z:
Sent simulated chunk: through

2025-04-02T01:39:41.373Z:
Sent simulated chunk: the

2025-04-02T01:39:41.419Z:
Sent simulated chunk: LM

2025-04-02T01:39:41.465Z:
Sent simulated chunk: Terminal.

2025-04-02T01:39:41.557Z:
Sent simulated chunk: How

2025-04-02T01:39:41.649Z:
Sent simulated chunk: can

2025-04-02T01:39:41.726Z:
Sent simulated chunk: I

2025-04-02T01:39:41.803Z:
Sent simulated chunk: help

2025-04-02T01:39:41.848Z:
Sent simulated chunk: you

2025-04-02T01:39:41.894Z:
Sent simulated chunk: today?

(This

2025-04-02T01:39:41.973Z:
Sent simulated chunk: response

2025-04-02T01:39:42.064Z:
Sent simulated chunk: was

2025-04-02T01:39:42.158Z:
Sent simulated chunk: generated

2025-04-02T01:39:42.222Z:
Sent simulated chunk: using

2025-04-02T01:39:42.269Z:
Sent simulated chunk: a

2025-04-02T01:39:42.347Z:
Sent simulated chunk: simulated

2025-04-02T01:39:42.441Z:
Sent simulated chunk: version

2025-04-02T01:39:42.518Z:
Sent simulated chunk: of

2025-04-02T01:39:42.579Z:
Sent simulated chunk: the

2025-04-02T01:39:42.641Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T01:39:42.688Z:
Sent simulated chunk: model)

2025-04-02T01:39:42.688Z:
Simulated response generation completed

2025-04-02T02:12:34.062Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:12:34.062Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:12:34.063Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:12:34.063Z:
Model mmproj-model-f16 file verified

2025-04-02T02:12:34.769Z:
Unloading model...

2025-04-02T02:12:34.769Z:
Model unloaded successfully

2025-04-02T02:12:34.769Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:12:34.770Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:12:34.770Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:12:34.770Z:
Model mmproj-model-f16 file verified

2025-04-02T02:12:51.526Z:
Generate response called with message: Whats a cooking recipe for 5 types of underated cookies...

2025-04-02T02:12:51.526Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: Whats a cooking recipe for 5 types of underated cookies
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:12:51.552Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:12:51.554Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-02T02:12:51.554Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T02:12:51.555Z:
Process exited with code 1

2025-04-02T02:12:51.555Z:
Process failed, falling back to simulation

2025-04-02T02:12:51.555Z:
Generating simulated response

2025-04-02T02:12:51.665Z:
Sent simulated chunk: I'm

2025-04-02T02:12:51.744Z:
Sent simulated chunk: a

2025-04-02T02:12:51.837Z:
Sent simulated chunk: locally

2025-04-02T02:12:51.914Z:
Sent simulated chunk: hosted

2025-04-02T02:12:51.992Z:
Sent simulated chunk: AI

2025-04-02T02:12:52.030Z:
Sent simulated chunk: assistant

2025-04-02T02:12:52.117Z:
Sent simulated chunk: running

2025-04-02T02:12:52.193Z:
Sent simulated chunk: directly

2025-04-02T02:12:52.286Z:
Sent simulated chunk: on

2025-04-02T02:12:52.364Z:
Sent simulated chunk: your

2025-04-02T02:12:52.473Z:
Sent simulated chunk: device.

2025-04-02T02:12:52.535Z:
Sent simulated chunk: I

2025-04-02T02:12:52.597Z:
Sent simulated chunk: process

2025-04-02T02:12:52.691Z:
Sent simulated chunk: information

2025-04-02T02:12:52.799Z:
Sent simulated chunk: using

2025-04-02T02:12:52.845Z:
Sent simulated chunk: the

2025-04-02T02:12:52.907Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:12:52.952Z:
Sent simulated chunk: model

2025-04-02T02:12:53.031Z:
Sent simulated chunk: loaded

2025-04-02T02:12:53.083Z:
Sent simulated chunk: in

2025-04-02T02:12:53.170Z:
Sent simulated chunk: the

2025-04-02T02:12:53.280Z:
Sent simulated chunk: LM

2025-04-02T02:12:53.375Z:
Sent simulated chunk: Terminal

2025-04-02T02:12:53.421Z:
Sent simulated chunk: application.

2025-04-02T02:12:53.514Z:
Sent simulated chunk: I've

2025-04-02T02:12:53.561Z:
Sent simulated chunk: received

2025-04-02T02:12:53.607Z:
Sent simulated chunk: your

2025-04-02T02:12:53.669Z:
Sent simulated chunk: message:

2025-04-02T02:12:53.701Z:
Sent simulated chunk: "Whats

2025-04-02T02:12:53.779Z:
Sent simulated chunk: a

2025-04-02T02:12:53.841Z:
Sent simulated chunk: cooking

2025-04-02T02:12:53.934Z:
Sent simulated chunk: recipe

2025-04-02T02:12:53.980Z:
Sent simulated chunk: for

2025-04-02T02:12:54.074Z:
Sent simulated chunk: 5

2025-04-02T02:12:54.182Z:
Sent simulated chunk: types

2025-04-02T02:12:54.276Z:
Sent simulated chunk: of

2025-04-02T02:12:54.384Z:
Sent simulated chunk: underated

2025-04-02T02:12:54.478Z:
Sent simulated chunk: cookies"

2025-04-02T02:12:54.572Z:
Sent simulated chunk: and

2025-04-02T02:12:54.650Z:
Sent simulated chunk: am

2025-04-02T02:12:54.743Z:
Sent simulated chunk: responding

2025-04-02T02:12:54.820Z:
Sent simulated chunk: without

2025-04-02T02:12:54.882Z:
Sent simulated chunk: requiring

2025-04-02T02:12:54.929Z:
Sent simulated chunk: internet

2025-04-02T02:12:55.023Z:
Sent simulated chunk: access.

(This

2025-04-02T02:12:55.116Z:
Sent simulated chunk: response

2025-04-02T02:12:55.179Z:
Sent simulated chunk: was

2025-04-02T02:12:55.288Z:
Sent simulated chunk: generated

2025-04-02T02:12:55.365Z:
Sent simulated chunk: using

2025-04-02T02:12:55.456Z:
Sent simulated chunk: a

2025-04-02T02:12:55.566Z:
Sent simulated chunk: simulated

2025-04-02T02:12:55.658Z:
Sent simulated chunk: version

2025-04-02T02:12:55.704Z:
Sent simulated chunk: of

2025-04-02T02:12:55.799Z:
Sent simulated chunk: the

2025-04-02T02:12:55.876Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:12:55.939Z:
Sent simulated chunk: model)

2025-04-02T02:12:55.940Z:
Simulated response generation completed

2025-04-02T02:13:11.270Z:
Unloading model...

2025-04-02T02:13:11.270Z:
Model unloaded successfully

2025-04-02T02:13:11.270Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:13:11.270Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:13:11.270Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:13:11.271Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:13:14.790Z:
Unloading model...

2025-04-02T02:13:14.790Z:
Model unloaded successfully

2025-04-02T02:13:14.790Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:13:14.790Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:13:14.791Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:13:14.791Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:13:19.351Z:
Generate response called with message: Whats a cooking recipe for 5 types of underated cookies
...

2025-04-02T02:13:19.352Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: Whats a cooking recipe for 5 types of underated cookies

ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:13:19.376Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:13:19.410Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:13:19.433Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:13:19.442Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:13:19.465Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:13:19.522Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:13:19.523Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:13:19.540Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear


2025-04-02T02:13:19.540Z:
Process stderr: print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:13:21.169Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:13:21.196Z:
Process stderr: .

2025-04-02T02:13:21.214Z:
Process stderr: .

2025-04-02T02:13:21.235Z:
Process stderr: .

2025-04-02T02:13:21.255Z:
Process stderr: .

2025-04-02T02:13:21.277Z:
Process stderr: .

2025-04-02T02:13:21.297Z:
Process stderr: .

2025-04-02T02:13:21.320Z:
Process stderr: .

2025-04-02T02:13:21.354Z:
Process stderr: .

2025-04-02T02:13:21.362Z:
Process stderr: .

2025-04-02T02:13:21.395Z:
Process stderr: .

2025-04-02T02:13:21.413Z:
Process stderr: .

2025-04-02T02:13:21.436Z:
Process stderr: .

2025-04-02T02:13:21.452Z:
Process stderr: .

2025-04-02T02:13:21.477Z:
Process stderr: .

2025-04-02T02:13:21.492Z:
Process stderr: .

2025-04-02T02:13:21.509Z:
Process stderr: .

2025-04-02T02:13:21.533Z:
Process stderr: .

2025-04-02T02:13:21.549Z:
Process stderr: .

2025-04-02T02:13:21.565Z:
Process stderr: .

2025-04-02T02:13:21.590Z:
Process stderr: .

2025-04-02T02:13:21.608Z:
Process stderr: .

2025-04-02T02:13:21.632Z:
Process stderr: .

2025-04-02T02:13:21.648Z:
Process stderr: .

2025-04-02T02:13:21.668Z:
Process stderr: .

2025-04-02T02:13:21.688Z:
Process stderr: .

2025-04-02T02:13:21.720Z:
Process stderr: .

2025-04-02T02:13:21.744Z:
Process stderr: .

2025-04-02T02:13:21.760Z:
Process stderr: .

2025-04-02T02:13:21.784Z:
Process stderr: .

2025-04-02T02:13:21.800Z:
Process stderr: .

2025-04-02T02:13:21.817Z:
Process stderr: .

2025-04-02T02:13:21.841Z:
Process stderr: .

2025-04-02T02:13:21.856Z:
Process stderr: .

2025-04-02T02:13:21.873Z:
Process stderr: .

2025-04-02T02:13:21.896Z:
Process stderr: .

2025-04-02T02:13:21.913Z:
Process stderr: .

2025-04-02T02:13:21.936Z:
Process stderr: .

2025-04-02T02:13:21.952Z:
Process stderr: .

2025-04-02T02:13:21.972Z:
Process stderr: .

2025-04-02T02:13:21.993Z:
Process stderr: .

2025-04-02T02:13:22.026Z:
Process stderr: .

2025-04-02T02:13:22.033Z:
Process stderr: .

2025-04-02T02:13:22.068Z:
Process stderr: .

2025-04-02T02:13:22.075Z:
Process stderr: .

2025-04-02T02:13:22.109Z:
Process stderr: .

2025-04-02T02:13:22.126Z:
Process stderr: .

2025-04-02T02:13:22.151Z:
Process stderr: .

2025-04-02T02:13:22.167Z:
Process stderr: .

2025-04-02T02:13:22.185Z:
Process stderr: .

2025-04-02T02:13:22.209Z:
Process stderr: .

2025-04-02T02:13:22.226Z:
Process stderr: .

2025-04-02T02:13:22.254Z:
Process stderr: .

2025-04-02T02:13:22.265Z:
Process stderr: .

2025-04-02T02:13:22.283Z:
Process stderr: .

2025-04-02T02:13:22.308Z:
Process stderr: .

2025-04-02T02:13:22.325Z:
Process stderr: .

2025-04-02T02:13:22.345Z:
Process stderr: .

2025-04-02T02:13:22.367Z:
Process stderr: .

2025-04-02T02:13:22.389Z:
Process stderr: .

2025-04-02T02:13:22.410Z:
Process stderr: .

2025-04-02T02:13:22.431Z:
Process stderr: .

2025-04-02T02:13:22.468Z:
Process stderr: .

2025-04-02T02:13:22.475Z:
Process stderr: .

2025-04-02T02:13:22.510Z:
Process stderr: .

2025-04-02T02:13:22.517Z:
Process stderr: .

2025-04-02T02:13:22.552Z:
Process stderr: ....

2025-04-02T02:13:22.552Z:
Process stderr: .........

2025-04-02T02:13:22.553Z:
Process stderr: .....


2025-04-02T02:13:22.555Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:13:22.555Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:13:22.572Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:13:22.577Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:13:22.829Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T02:13:22.830Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:13:22.831Z:
Process stderr: sampler seed: 4146995258
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000


2025-04-02T02:13:22.831Z:
Process stderr: sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:13:23.110Z:
Received chunk: <think>...

2025-04-02T02:13:23.196Z:
Received chunk: 
...

2025-04-02T02:13:23.284Z:
Received chunk: Okay...

2025-04-02T02:13:23.373Z:
Received chunk: ,...

2025-04-02T02:13:23.465Z:
Received chunk:  so...

2025-04-02T02:13:23.552Z:
Received chunk:  I...

2025-04-02T02:13:23.638Z:
Received chunk:  need...

2025-04-02T02:13:23.724Z:
Received chunk:  to...

2025-04-02T02:13:23.811Z:
Received chunk:  come...

2025-04-02T02:13:23.897Z:
Received chunk:  up...

2025-04-02T02:13:23.982Z:
Received chunk:  with...

2025-04-02T02:13:24.069Z:
Received chunk:  five...

2025-04-02T02:13:24.155Z:
Received chunk:  different...

2025-04-02T02:13:24.243Z:
Received chunk:  cookie...

2025-04-02T02:13:24.334Z:
Received chunk:  recipes...

2025-04-02T02:13:24.424Z:
Received chunk:  that...

2025-04-02T02:13:24.513Z:
Received chunk:  are...

2025-04-02T02:13:24.600Z:
Received chunk:  usually...

2025-04-02T02:13:24.686Z:
Received chunk:  not...

2025-04-02T02:13:24.772Z:
Received chunk:  talked...

2025-04-02T02:13:24.858Z:
Received chunk:  about...

2025-04-02T02:13:24.943Z:
Received chunk:  much...

2025-04-02T02:13:25.034Z:
Received chunk:  but...

2025-04-02T02:13:25.128Z:
Received chunk:  are...

2025-04-02T02:13:25.218Z:
Received chunk:  actually...

2025-04-02T02:13:25.307Z:
Received chunk:  good...

2025-04-02T02:13:25.402Z:
Received chunk: ....

2025-04-02T02:13:25.486Z:
Received chunk:  Hmm...

2025-04-02T02:13:25.574Z:
Received chunk: ,...

2025-04-02T02:13:25.659Z:
Received chunk:  where...

2025-04-02T02:13:25.747Z:
Received chunk:  do...

2025-04-02T02:13:25.833Z:
Received chunk:  I...

2025-04-02T02:13:25.918Z:
Received chunk:  start...

2025-04-02T02:13:26.004Z:
Received chunk: ?...

2025-04-02T02:13:26.090Z:
Received chunk:  Maybe...

2025-04-02T02:13:26.174Z:
Received chunk:  I...

2025-04-02T02:13:26.262Z:
Received chunk:  should...

2025-04-02T02:13:26.354Z:
Received chunk:  think...

2025-04-02T02:13:26.447Z:
Received chunk:  about...

2025-04-02T02:13:26.536Z:
Received chunk:  cookies...

2025-04-02T02:13:26.625Z:
Received chunk:  that...

2025-04-02T02:13:26.712Z:
Received chunk:  people...

2025-04-02T02:13:26.798Z:
Received chunk:  either...

2025-04-02T02:13:26.884Z:
Received chunk:  don...

2025-04-02T02:13:26.970Z:
Received chunk: 't...

2025-04-02T02:13:27.055Z:
Received chunk:  make...

2025-04-02T02:13:27.141Z:
Received chunk:  often...

2025-04-02T02:13:27.228Z:
Received chunk:  or...

2025-04-02T02:13:27.317Z:
Received chunk:  aren...

2025-04-02T02:13:27.408Z:
Received chunk: 't...

2025-04-02T02:13:27.499Z:
Received chunk:  sure...

2025-04-02T02:13:27.587Z:
Received chunk:  how...

2025-04-02T02:13:27.675Z:
Received chunk:  to...

2025-04-02T02:13:27.762Z:
Received chunk:  make...

2025-04-02T02:13:27.848Z:
Received chunk:  because...

2025-04-02T02:13:27.934Z:
Received chunk:  they...

2025-04-02T02:13:28.020Z:
Received chunk: 're...

2025-04-02T02:13:28.107Z:
Received chunk:  a...

2025-04-02T02:13:28.193Z:
Received chunk:  bit...

2025-04-02T02:13:28.281Z:
Received chunk:  tricky...

2025-04-02T02:13:28.368Z:
Received chunk: .

...

2025-04-02T02:13:28.460Z:
Received chunk: First...

2025-04-02T02:13:28.551Z:
Received chunk: ,...

2025-04-02T02:13:28.637Z:
Received chunk:  I...

2025-04-02T02:13:28.723Z:
Received chunk:  remember...

2025-04-02T02:13:28.809Z:
Received chunk:  hearing...

2025-04-02T02:13:28.896Z:
Received chunk:  about...

2025-04-02T02:13:28.981Z:
Received chunk:  something...

2025-04-02T02:13:29.066Z:
Received chunk:  called...

2025-04-02T02:13:29.152Z:
Received chunk:  "...

2025-04-02T02:13:29.238Z:
Received chunk: No...

2025-04-02T02:13:29.327Z:
Received chunk: -B...

2025-04-02T02:13:29.416Z:
Received chunk: ake...

2025-04-02T02:13:29.506Z:
Received chunk:  Cookies...

2025-04-02T02:13:29.595Z:
Received chunk: ."...

2025-04-02T02:13:29.681Z:
Received chunk:  They...

2025-04-02T02:13:29.767Z:
Received chunk:  sound...

2025-04-02T02:13:29.852Z:
Received chunk:  appealing...

2025-04-02T02:13:29.939Z:
Received chunk:  because...

2025-04-02T02:13:30.025Z:
Received chunk:  you...

2025-04-02T02:13:30.111Z:
Received chunk:  don...

2025-04-02T02:13:30.197Z:
Received chunk: 't...

2025-04-02T02:13:30.288Z:
Received chunk:  have...

2025-04-02T02:13:30.374Z:
Received chunk:  to...

2025-04-02T02:13:30.465Z:
Received chunk:  bake...

2025-04-02T02:13:30.554Z:
Received chunk:  them...

2025-04-02T02:13:30.641Z:
Received chunk:  with...

2025-04-02T02:13:30.727Z:
Received chunk:  traditional...

2025-04-02T02:13:30.815Z:
Received chunk:  methods...

2025-04-02T02:13:30.902Z:
Received chunk: ....

2025-04-02T02:13:30.988Z:
Received chunk:  Maybe...

2025-04-02T02:13:31.073Z:
Received chunk:  that...

2025-04-02T02:13:31.159Z:
Received chunk: 's...

2025-04-02T02:13:31.244Z:
Received chunk:  a...

2025-04-02T02:13:31.334Z:
Received chunk:  good...

2025-04-02T02:13:31.428Z:
Received chunk:  one...

2025-04-02T02:13:31.516Z:
Received chunk:  since...

2025-04-02T02:13:31.604Z:
Received chunk:  many...

2025-04-02T02:13:31.690Z:
Received chunk:  people...

2025-04-02T02:13:31.779Z:
Received chunk:  avoid...

2025-04-02T02:13:31.865Z:
Received chunk:  baking...

2025-04-02T02:13:31.951Z:
Received chunk:  unless...

2025-04-02T02:13:32.036Z:
Received chunk:  it...

2025-04-02T02:13:32.123Z:
Received chunk: 's...

2025-04-02T02:13:32.209Z:
Received chunk:  necessary...

2025-04-02T02:13:32.298Z:
Received chunk: ....

2025-04-02T02:13:32.386Z:
Received chunk:  The...

2025-04-02T02:13:32.482Z:
Received chunk:  user...

2025-04-02T02:13:32.574Z:
Received chunk:  mentioned...

2025-04-02T02:13:32.660Z:
Received chunk:  a...

2025-04-02T02:13:32.746Z:
Received chunk:  Double...

2025-04-02T02:13:32.833Z:
Received chunk:  Chocolate...

2025-04-02T02:13:32.919Z:
Received chunk:  Chip...

2025-04-02T02:13:33.005Z:
Received chunk:  No...

2025-04-02T02:13:33.091Z:
Received chunk: -B...

2025-04-02T02:13:33.176Z:
Received chunk: ake...

2025-04-02T02:13:33.263Z:
Received chunk:  Cookie...

2025-04-02T02:13:33.352Z:
Received chunk:  Bar...

2025-04-02T02:13:33.445Z:
Received chunk:  in...

2025-04-02T02:13:33.538Z:
Received chunk:  their...

2025-04-02T02:13:33.625Z:
Received chunk:  example...

2025-04-02T02:13:33.710Z:
Received chunk: ,...

2025-04-02T02:13:33.796Z:
Received chunk:  which...

2025-04-02T02:13:33.882Z:
Received chunk:  is...

2025-04-02T02:13:33.968Z:
Received chunk:  perfect...

2025-04-02T02:13:34.054Z:
Received chunk: .

...

2025-04-02T02:13:34.140Z:
Received chunk: Next...

2025-04-02T02:13:34.225Z:
Received chunk: ,...

2025-04-02T02:13:34.312Z:
Received chunk:  there...

2025-04-02T02:13:34.402Z:
Received chunk:  are...

2025-04-02T02:13:34.495Z:
Received chunk:  those...

2025-04-02T02:13:34.584Z:
Received chunk:  cookies...

2025-04-02T02:13:34.670Z:
Received chunk:  that...

2025-04-02T02:13:34.756Z:
Received chunk:  come...

2025-04-02T02:13:34.843Z:
Received chunk:  in...

2025-04-02T02:13:34.930Z:
Received chunk:  unusual...

2025-04-02T02:13:35.016Z:
Received chunk:  shapes...

2025-04-02T02:13:35.103Z:
Received chunk: ....

2025-04-02T02:13:35.189Z:
Received chunk:  I...

2025-04-02T02:13:35.275Z:
Received chunk:  know...

2025-04-02T02:13:35.363Z:
Received chunk:  someone...

2025-04-02T02:13:35.457Z:
Received chunk:  who...

2025-04-02T02:13:35.555Z:
Received chunk:  loves...

2025-04-02T02:13:35.642Z:
Received chunk:  mac...

2025-04-02T02:13:35.729Z:
Received chunk: ar...

2025-04-02T02:13:35.817Z:
Received chunk: ons...

2025-04-02T02:13:35.904Z:
Received chunk: ,...

2025-04-02T02:13:35.990Z:
Received chunk:  but...

2025-04-02T02:13:36.076Z:
Received chunk:  they...

2025-04-02T02:13:36.163Z:
Received chunk: 're...

2025-04-02T02:13:36.249Z:
Received chunk:  really...

2025-04-02T02:13:36.336Z:
Received chunk:  delicate...

2025-04-02T02:13:36.425Z:
Received chunk:  and...

2025-04-02T02:13:36.518Z:
Received chunk:  can...

2025-04-02T02:13:36.607Z:
Received chunk:  be...

2025-04-02T02:13:36.693Z:
Received chunk:  hard...

2025-04-02T02:13:36.779Z:
Received chunk:  to...

2025-04-02T02:13:36.865Z:
Received chunk:  make...

2025-04-02T02:13:36.952Z:
Received chunk: ....

2025-04-02T02:13:37.038Z:
Received chunk:  Maybe...

2025-04-02T02:13:37.124Z:
Received chunk:  "...

2025-04-02T02:13:37.210Z:
Received chunk: Mac...

2025-04-02T02:13:37.296Z:
Received chunk: aron...

2025-04-02T02:13:37.386Z:
Received chunk:  Cookies...

2025-04-02T02:13:37.477Z:
Received chunk: "...

2025-04-02T02:13:37.571Z:
Received chunk:  would...

2025-04-02T02:13:37.657Z:
Received chunk:  fit...

2025-04-02T02:13:37.743Z:
Received chunk:  here...

2025-04-02T02:13:37.828Z:
Received chunk:  since...

2025-04-02T02:13:37.915Z:
Received chunk:  they...

2025-04-02T02:13:38.001Z:
Received chunk: 're...

2025-04-02T02:13:38.087Z:
Received chunk:  under...

2025-04-02T02:13:38.173Z:
Received chunk:  the...

2025-04-02T02:13:38.259Z:
Received chunk:  radar...

2025-04-02T02:13:38.346Z:
Received chunk:  but...

2025-04-02T02:13:38.435Z:
Received chunk:  looked...

2025-04-02T02:13:38.529Z:
Received chunk:  nice...

2025-04-02T02:13:38.619Z:
Received chunk:  in...

2025-04-02T02:13:38.707Z:
Received chunk:  the...

2025-04-02T02:13:38.793Z:
Received chunk:  example...

2025-04-02T02:13:38.883Z:
Received chunk: .

...

2025-04-02T02:13:38.969Z:
Received chunk: Another...

2025-04-02T02:13:39.055Z:
Received chunk:  idea...

2025-04-02T02:13:39.143Z:
Received chunk:  could...

2025-04-02T02:13:39.228Z:
Received chunk:  be...

2025-04-02T02:13:39.314Z:
Received chunk:  something...

2025-04-02T02:13:39.403Z:
Received chunk:  traditional...

2025-04-02T02:13:39.496Z:
Received chunk:  yet...

2025-04-02T02:13:39.591Z:
Received chunk:  unique...

2025-04-02T02:13:39.682Z:
Received chunk: ....

2025-04-02T02:13:39.769Z:
Received chunk:  Double...

2025-04-02T02:13:39.856Z:
Received chunk: -...

2025-04-02T02:13:39.941Z:
Received chunk: Chocolate...

2025-04-02T02:13:40.028Z:
Received chunk:  Chip...

2025-04-02T02:13:40.115Z:
Received chunk:  cookies...

2025-04-02T02:13:40.201Z:
Received chunk:  are...

2025-04-02T02:13:40.288Z:
Received chunk:  common...

2025-04-02T02:13:40.377Z:
Received chunk: ,...

2025-04-02T02:13:40.465Z:
Received chunk:  so...

2025-04-02T02:13:40.558Z:
Received chunk:  maybe...

2025-04-02T02:13:40.647Z:
Received chunk:  a...

2025-04-02T02:13:40.732Z:
Received chunk:  variation...

2025-04-02T02:13:40.818Z:
Received chunk:  of...

2025-04-02T02:13:40.906Z:
Received chunk:  that...

2025-04-02T02:13:40.992Z:
Received chunk:  with...

2025-04-02T02:13:41.078Z:
Received chunk:  different...

2025-04-02T02:13:41.163Z:
Received chunk:  toppings...

2025-04-02T02:13:41.249Z:
Received chunk:  or...

2025-04-02T02:13:41.335Z:
Received chunk:  ingredients...

2025-04-02T02:13:41.423Z:
Received chunk:  would...

2025-04-02T02:13:41.513Z:
Received chunk:  work...

2025-04-02T02:13:41.605Z:
Received chunk: ....

2025-04-02T02:13:41.691Z:
Received chunk:  The...

2025-04-02T02:13:41.777Z:
Received chunk:  user...

2025-04-02T02:13:41.863Z:
Received chunk:  suggested...

2025-04-02T02:13:41.950Z:
Received chunk:  Double...

2025-04-02T02:13:42.036Z:
Received chunk: -...

2025-04-02T02:13:42.122Z:
Received chunk: Chocolate...

2025-04-02T02:13:42.207Z:
Received chunk:  Chip...

2025-04-02T02:13:42.293Z:
Received chunk:  No...

2025-04-02T02:13:42.381Z:
Received chunk: -B...

2025-04-02T02:13:42.471Z:
Received chunk: ake...

2025-04-02T02:13:42.562Z:
Received chunk:  Cookie...

2025-04-02T02:13:42.652Z:
Received chunk:  Bars...

2025-04-02T02:13:42.738Z:
Received chunk:  as...

2025-04-02T02:13:42.823Z:
Received chunk:  an...

2025-04-02T02:13:42.908Z:
Received chunk:  example...

2025-04-02T02:13:42.994Z:
Received chunk: ,...

2025-04-02T02:13:43.079Z:
Received chunk:  which...

2025-04-02T02:13:43.166Z:
Received chunk:  is...

2025-04-02T02:13:43.252Z:
Received chunk:  already...

2025-04-02T02:13:43.338Z:
Received chunk:  covered...

2025-04-02T02:13:43.426Z:
Received chunk:  my...

2025-04-02T02:13:43.516Z:
Received chunk:  first...

2025-04-02T02:13:43.609Z:
Received chunk:  point...

2025-04-02T02:13:43.696Z:
Received chunk: .

...

2025-04-02T02:13:43.783Z:
Received chunk: I...

2025-04-02T02:13:43.870Z:
Received chunk:  should...

2025-04-02T02:13:43.956Z:
Received chunk:  think...

2025-04-02T02:13:44.043Z:
Received chunk:  about...

2025-04-02T02:13:44.129Z:
Received chunk:  cookies...

2025-04-02T02:13:44.214Z:
Received chunk:  that...

2025-04-02T02:13:44.300Z:
Received chunk:  have...

2025-04-02T02:13:44.387Z:
Received chunk:  a...

2025-04-02T02:13:44.474Z:
Received chunk:  unique...

2025-04-02T02:13:44.565Z:
Received chunk:  twist...

2025-04-02T02:13:44.655Z:
Received chunk:  on...

2025-04-02T02:13:44.742Z:
Received chunk:  flavor...

2025-04-02T02:13:44.829Z:
Received chunk:  or...

2025-04-02T02:13:44.915Z:
Received chunk:  texture...

2025-04-02T02:13:45.001Z:
Received chunk: ....

2025-04-02T02:13:45.087Z:
Received chunk:  How...

2025-04-02T02:13:45.175Z:
Received chunk:  about...

2025-04-02T02:13:45.261Z:
Received chunk:  "...

2025-04-02T02:13:45.348Z:
Received chunk: Al...

2025-04-02T02:13:45.437Z:
Received chunk: mond...

2025-04-02T02:13:45.529Z:
Received chunk: -F...

2025-04-02T02:13:45.620Z:
Received chunk: illed...

2025-04-02T02:13:45.706Z:
Received chunk:  Cookies...

2025-04-02T02:13:45.793Z:
Received chunk: "?...

2025-04-02T02:13:45.882Z:
Received chunk:  They...

2025-04-02T02:13:45.968Z:
Received chunk: 're...

2025-04-02T02:13:46.054Z:
Received chunk:  not...

2025-04-02T02:13:46.140Z:
Received chunk:  too...

2025-04-02T02:13:46.227Z:
Received chunk:  common...

2025-04-02T02:13:46.314Z:
Received chunk:  and...

2025-04-02T02:13:46.403Z:
Received chunk:  can...

2025-04-02T02:13:46.497Z:
Received chunk:  be...

2025-04-02T02:13:46.581Z:
Received chunk:  made...

2025-04-02T02:13:46.672Z:
Received chunk:  with...

2025-04-02T02:13:46.758Z:
Received chunk:  just...

2025-04-02T02:13:46.845Z:
Received chunk:  almond...

2025-04-02T02:13:46.931Z:
Received chunk:  sl...

2025-04-02T02:13:47.017Z:
Received chunk: ivers...

2025-04-02T02:13:47.104Z:
Received chunk:  instead...

2025-04-02T02:13:47.191Z:
Received chunk:  of...

2025-04-02T02:13:47.277Z:
Received chunk:  whole...

2025-04-02T02:13:47.363Z:
Received chunk:  almonds...

2025-04-02T02:13:47.451Z:
Received chunk: ,...

2025-04-02T02:13:47.541Z:
Received chunk:  making...

2025-04-02T02:13:47.632Z:
Received chunk:  them...

2025-04-02T02:13:47.720Z:
Received chunk:  healthier...

2025-04-02T02:13:47.807Z:
Received chunk: ....

2025-04-02T02:13:47.893Z:
Received chunk:  Or...

2025-04-02T02:13:47.979Z:
Received chunk:  maybe...

2025-04-02T02:13:48.065Z:
Received chunk:  something...

2025-04-02T02:13:48.153Z:
Received chunk:  like...

2025-04-02T02:13:48.239Z:
Received chunk:  "...

2025-04-02T02:13:48.326Z:
Received chunk: C...

2025-04-02T02:13:48.413Z:
Received chunk: aul...

2025-04-02T02:13:48.506Z:
Received chunk: iflower...

2025-04-02T02:13:48.594Z:
Received chunk:  Cr...

2025-04-02T02:13:48.685Z:
Received chunk: is...

2025-04-02T02:13:48.772Z:
Received chunk: ps...

2025-04-02T02:13:48.858Z:
Received chunk: "...

2025-04-02T02:13:48.945Z:
Received chunk:  if...

2025-04-02T02:13:49.032Z:
Received chunk:  I...

2025-04-02T02:13:49.117Z:
Received chunk: 'm...

2025-04-02T02:13:49.204Z:
Received chunk:  including...

2025-04-02T02:13:49.290Z:
Received chunk:  more...

2025-04-02T02:13:49.377Z:
Received chunk:  health...

2025-04-02T02:13:49.466Z:
Received chunk: -focused...

2025-04-02T02:13:49.558Z:
Received chunk:  options...

2025-04-02T02:13:49.649Z:
Received chunk: .

...

2025-04-02T02:13:49.737Z:
Received chunk: Wait...

2025-04-02T02:13:49.823Z:
Received chunk: ,...

2025-04-02T02:13:49.910Z:
Received chunk:  the...

2025-04-02T02:13:49.996Z:
Received chunk:  user...

2025-04-02T02:13:50.082Z:
Received chunk:  already...

2025-04-02T02:13:50.168Z:
Received chunk:  mentioned...

2025-04-02T02:13:50.254Z:
Received chunk:  Double...

2025-04-02T02:13:50.341Z:
Received chunk: -...

2025-04-02T02:13:50.428Z:
Received chunk: Chocolate...

2025-04-02T02:13:50.519Z:
Received chunk:  Chip...

2025-04-02T02:13:50.611Z:
Received chunk:  in...

2025-04-02T02:13:50.699Z:
Received chunk:  their...

2025-04-02T02:13:50.787Z:
Received chunk:  example...

2025-04-02T02:13:50.874Z:
Received chunk: ,...

2025-04-02T02:13:50.960Z:
Received chunk:  so...

2025-04-02T02:13:51.048Z:
Received chunk:  maybe...

2025-04-02T02:13:51.133Z:
Received chunk:  another...

2025-04-02T02:13:51.219Z:
Received chunk:  variation...

2025-04-02T02:13:51.305Z:
Received chunk:  is...

2025-04-02T02:13:51.391Z:
Received chunk:  better...

2025-04-02T02:13:51.480Z:
Received chunk: ....

2025-04-02T02:13:51.571Z:
Received chunk:  Maybe...

2025-04-02T02:13:51.662Z:
Received chunk:  "...

2025-04-02T02:13:51.748Z:
Received chunk: O...

2025-04-02T02:13:51.836Z:
Received chunk: reo...

2025-04-02T02:13:51.922Z:
Received chunk:  Stack...

2025-04-02T02:13:52.009Z:
Received chunk:  Cookies...

2025-04-02T02:13:52.095Z:
Received chunk: ,"...

2025-04-02T02:13:52.181Z:
Received chunk:  which...

2025-04-02T02:13:52.267Z:
Received chunk:  use...

2025-04-02T02:13:52.356Z:
Received chunk:  O...

2025-04-02T02:13:52.443Z:
Received chunk: reo...

2025-04-02T02:13:52.532Z:
Received chunk:  cookies...

2025-04-02T02:13:52.624Z:
Received chunk:  as...

2025-04-02T02:13:52.713Z:
Received chunk:  a...

2025-04-02T02:13:52.800Z:
Received chunk:  base...

2025-04-02T02:13:52.888Z:
Received chunk:  and...

2025-04-02T02:13:52.974Z:
Received chunk:  can...

2025-04-02T02:13:53.061Z:
Received chunk:  be...

2025-04-02T02:13:53.148Z:
Received chunk:  layered...

2025-04-02T02:13:53.234Z:
Received chunk:  with...

2025-04-02T02:13:53.321Z:
Received chunk:  cream...

2025-04-02T02:13:53.409Z:
Received chunk:  cheese...

2025-04-02T02:13:53.498Z:
Received chunk:  frosting...

2025-04-02T02:13:53.590Z:
Received chunk:  or...

2025-04-02T02:13:53.691Z:
Received chunk:  chocolate...

2025-04-02T02:13:53.782Z:
Received chunk:  gan...

2025-04-02T02:13:53.870Z:
Received chunk: ache...

2025-04-02T02:13:53.958Z:
Received chunk: ....

2025-04-02T02:13:54.044Z:
Received chunk:  That...

2025-04-02T02:13:54.130Z:
Received chunk:  sounds...

2025-04-02T02:13:54.217Z:
Received chunk:  unique...

2025-04-02T02:13:54.304Z:
Received chunk:  and...

2025-04-02T02:13:54.390Z:
Received chunk:  easy...

2025-04-02T02:13:54.479Z:
Received chunk:  to...

2025-04-02T02:13:54.568Z:
Received chunk:  make...

2025-04-02T02:13:54.659Z:
Received chunk:  without...

2025-04-02T02:13:54.746Z:
Received chunk:  traditional...

2025-04-02T02:13:54.834Z:
Received chunk:  baking...

2025-04-02T02:13:54.921Z:
Received chunk:  steps...

2025-04-02T02:13:55.007Z:
Received chunk: .

...

2025-04-02T02:13:55.094Z:
Received chunk: I...

2025-04-02T02:13:55.181Z:
Received chunk:  need...

2025-04-02T02:13:55.267Z:
Received chunk:  five...

2025-04-02T02:13:55.354Z:
Received chunk:  different...

2025-04-02T02:13:55.441Z:
Received chunk:  types...

2025-04-02T02:13:55.530Z:
Received chunk: ....

2025-04-02T02:13:55.622Z:
Received chunk:  Let...

2025-04-02T02:13:55.711Z:
Received chunk:  me...

2025-04-02T02:13:55.797Z:
Received chunk:  list...

2025-04-02T02:13:55.884Z:
Received chunk:  them...

2025-04-02T02:13:55.970Z:
Received chunk:  out...

2025-04-02T02:13:56.059Z:
Received chunk: :

...

2025-04-02T02:13:56.147Z:
Received chunk: 1...

2025-04-02T02:13:56.234Z:
Received chunk: ....

2025-04-02T02:13:56.320Z:
Received chunk:  Double...

2025-04-02T02:13:56.407Z:
Received chunk:  Chocolate...

2025-04-02T02:13:56.497Z:
Received chunk:  Chip...

2025-04-02T02:13:56.588Z:
Received chunk:  No...

2025-04-02T02:13:56.679Z:
Received chunk: -B...

2025-04-02T02:13:56.766Z:
Received chunk: ake...

2025-04-02T02:13:56.853Z:
Received chunk:  Cookie...

2025-04-02T02:13:56.940Z:
Received chunk:  Bars...

2025-04-02T02:13:57.026Z:
Received chunk: 
...

2025-04-02T02:13:57.113Z:
Received chunk: 2...

2025-04-02T02:13:57.200Z:
Received chunk: ....

2025-04-02T02:13:57.286Z:
Received chunk:  Al...

2025-04-02T02:13:57.373Z:
Received chunk: mond...

2025-04-02T02:13:57.462Z:
Received chunk: -F...

2025-04-02T02:13:57.554Z:
Received chunk: illed...

2025-04-02T02:13:57.646Z:
Received chunk:  Cookies...

2025-04-02T02:13:57.736Z:
Received chunk:  (...

2025-04-02T02:13:57.823Z:
Received chunk: using...

2025-04-02T02:13:57.911Z:
Received chunk:  sl...

2025-04-02T02:13:57.998Z:
Received chunk: ivers...

2025-04-02T02:13:58.085Z:
Received chunk: )
...

2025-04-02T02:13:58.170Z:
Received chunk: 3...

2025-04-02T02:13:58.256Z:
Received chunk: ....

2025-04-02T02:13:58.344Z:
Received chunk:  Double...

2025-04-02T02:13:58.430Z:
Received chunk:  Ore...

2025-04-02T02:13:58.520Z:
Received chunk: o...

2025-04-02T02:13:58.610Z:
Received chunk:  Stack...

2025-04-02T02:13:58.700Z:
Received chunk:  Cookies...

2025-04-02T02:13:58.787Z:
Received chunk: 
...

2025-04-02T02:13:58.874Z:
Received chunk: 4...

2025-04-02T02:13:58.961Z:
Received chunk: ....

2025-04-02T02:13:59.048Z:
Received chunk:  Mac...

2025-04-02T02:13:59.135Z:
Received chunk: aron...

2025-04-02T02:13:59.223Z:
Received chunk:  Cookies...

2025-04-02T02:13:59.308Z:
Received chunk: 
...

2025-04-02T02:13:59.395Z:
Received chunk: 5...

2025-04-02T02:13:59.483Z:
Received chunk: ....

2025-04-02T02:13:59.578Z:
Received chunk:  Chocolate...

2025-04-02T02:13:59.668Z:
Received chunk:  Gan...

2025-04-02T02:13:59.757Z:
Received chunk: ache...

2025-04-02T02:13:59.844Z:
Received chunk:  Cookies...

2025-04-02T02:13:59.932Z:
Received chunk: 

...

2025-04-02T02:14:00.023Z:
Received chunk: Wait...

2025-04-02T02:14:00.109Z:
Received chunk: ,...

2025-04-02T02:14:00.195Z:
Received chunk:  the...

2025-04-02T02:14:00.281Z:
Received chunk:  user...

2025-04-02T02:14:00.367Z:
Received chunk: 's...

2025-04-02T02:14:00.461Z:
Received chunk:  example...

2025-04-02T02:14:00.543Z:
Received chunk:  included...

2025-04-02T02:14:00.636Z:
Received chunk:  Double...

2025-04-02T02:14:00.725Z:
Received chunk:  Chocolate...

2025-04-02T02:14:00.812Z:
Received chunk:  Chip...

2025-04-02T02:14:00.898Z:
Received chunk:  and...

2025-04-02T02:14:00.987Z:
Received chunk:  Mac...

2025-04-02T02:14:01.075Z:
Received chunk: aron...

2025-04-02T02:14:01.164Z:
Received chunk: ,...

2025-04-02T02:14:01.250Z:
Received chunk:  so...

2025-04-02T02:14:01.337Z:
Received chunk:  I...

2025-04-02T02:14:01.423Z:
Received chunk:  should...

2025-04-02T02:14:01.511Z:
Received chunk:  include...

2025-04-02T02:14:01.604Z:
Received chunk:  different...

2025-04-02T02:14:01.697Z:
Received chunk:  ones...

2025-04-02T02:14:01.785Z:
Received chunk:  to...

2025-04-02T02:14:01.871Z:
Received chunk:  avoid...

2025-04-02T02:14:01.957Z:
Received chunk:  duplication...

2025-04-02T02:14:02.047Z:
Received chunk: .

...

2025-04-02T02:14:02.135Z:
Received chunk: Alternatively...

2025-04-02T02:14:02.222Z:
Received chunk: ,...

2025-04-02T02:14:02.311Z:
Received chunk:  maybe...

2025-04-02T02:14:02.401Z:
Received chunk:  "...

2025-04-02T02:14:02.490Z:
Received chunk: Co...

2025-04-02T02:14:02.583Z:
Received chunk: conut...

2025-04-02T02:14:02.679Z:
Received chunk:  Mac...

2025-04-02T02:14:02.771Z:
Received chunk: adam...

2025-04-02T02:14:02.858Z:
Received chunk: ia...

2025-04-02T02:14:02.945Z:
Received chunk:  Nut...

2025-04-02T02:14:03.031Z:
Received chunk:  Cookies...

2025-04-02T02:14:03.119Z:
Received chunk: "...

2025-04-02T02:14:03.206Z:
Received chunk:  could...

2025-04-02T02:14:03.293Z:
Received chunk:  be...

2025-04-02T02:14:03.381Z:
Received chunk:  another...

2025-04-02T02:14:03.475Z:
Received chunk:  option...

2025-04-02T02:14:03.556Z:
Received chunk:  since...

2025-04-02T02:14:03.649Z:
Received chunk:  they...

2025-04-02T02:14:03.740Z:
Received chunk: 're...

2025-04-02T02:14:03.826Z:
Received chunk:  not...

2025-04-02T02:14:03.914Z:
Received chunk:  as...

2025-04-02T02:14:04.000Z:
Received chunk:  mainstream...

2025-04-02T02:14:04.088Z:
Received chunk:  but...

2025-04-02T02:14:04.174Z:
Received chunk:  are...

2025-04-02T02:14:04.261Z:
Received chunk:  rich...

2025-04-02T02:14:04.349Z:
Received chunk:  and...

2025-04-02T02:14:04.435Z:
Received chunk:  flavorful...

2025-04-02T02:14:04.523Z:
Received chunk: .

...

2025-04-02T02:14:04.615Z:
Received chunk: Let...

2025-04-02T02:14:04.707Z:
Received chunk:  me...

2025-04-02T02:14:04.794Z:
Received chunk:  try...

2025-04-02T02:14:04.881Z:
Received chunk:  again...

2025-04-02T02:14:04.968Z:
Received chunk: :

...

2025-04-02T02:14:05.056Z:
Received chunk: 1...

2025-04-02T02:14:05.143Z:
Received chunk: ....

2025-04-02T02:14:05.230Z:
Received chunk:  No...

2025-04-02T02:14:05.318Z:
Received chunk: -B...

2025-04-02T02:14:05.405Z:
Received chunk: ake...

2025-04-02T02:14:05.494Z:
Received chunk:  Chocolate...

2025-04-02T02:14:05.583Z:
Received chunk:  Chip...

2025-04-02T02:14:05.675Z:
Received chunk:  Cookies...

2025-04-02T02:14:05.765Z:
Received chunk: 
...

2025-04-02T02:14:05.852Z:
Received chunk: 2...

2025-04-02T02:14:05.939Z:
Received chunk: ....

2025-04-02T02:14:06.025Z:
Received chunk:  Al...

2025-04-02T02:14:06.113Z:
Received chunk: mond...

2025-04-02T02:14:06.201Z:
Received chunk: -F...

2025-04-02T02:14:06.287Z:
Received chunk: illed...

2025-04-02T02:14:06.375Z:
Received chunk:  Sn...

2025-04-02T02:14:06.462Z:
Received chunk: icker...

2025-04-02T02:14:06.550Z:
Received chunk: d...

2025-04-02T02:14:06.641Z:
Received chunk: oodles...

2025-04-02T02:14:06.733Z:
Received chunk: 
...

2025-04-02T02:14:06.820Z:
Received chunk: 3...

2025-04-02T02:14:06.908Z:
Received chunk: ....

2025-04-02T02:14:06.995Z:
Received chunk:  Double...

2025-04-02T02:14:07.082Z:
Received chunk:  Ore...

2025-04-02T02:14:07.169Z:
Received chunk: o...

2025-04-02T02:14:07.255Z:
Received chunk:  Stack...

2025-04-02T02:14:07.343Z:
Received chunk:  Cookies...

2025-04-02T02:14:07.429Z:
Received chunk: 
...

2025-04-02T02:14:07.518Z:
Received chunk: 4...

2025-04-02T02:14:07.607Z:
Received chunk: ....

2025-04-02T02:14:07.698Z:
Received chunk:  Mac...

2025-04-02T02:14:07.790Z:
Received chunk: aron...

2025-04-02T02:14:07.877Z:
Received chunk:  Cookies...

2025-04-02T02:14:07.964Z:
Received chunk: 
...

2025-04-02T02:14:08.051Z:
Received chunk: 5...

2025-04-02T02:14:08.138Z:
Received chunk: ....

2025-04-02T02:14:08.225Z:
Received chunk:  Coconut...

2025-04-02T02:14:08.312Z:
Received chunk:  Mac...

2025-04-02T02:14:08.399Z:
Received chunk: adam...

2025-04-02T02:14:08.487Z:
Received chunk: ia...

2025-04-02T02:14:08.576Z:
Received chunk:  Nut...

2025-04-02T02:14:08.669Z:
Received chunk:  Cookies...

2025-04-02T02:14:08.760Z:
Received chunk: 

...

2025-04-02T02:14:08.848Z:
Received chunk: Yes...

2025-04-02T02:14:08.935Z:
Received chunk: ,...

2025-04-02T02:14:09.022Z:
Received chunk:  that...

2025-04-02T02:14:09.110Z:
Received chunk:  sounds...

2025-04-02T02:14:09.198Z:
Received chunk:  better...

2025-04-02T02:14:09.286Z:
Received chunk: ....

2025-04-02T02:14:09.373Z:
Received chunk:  Each...

2025-04-02T02:14:09.460Z:
Received chunk:  of...

2025-04-02T02:14:09.548Z:
Received chunk:  these...

2025-04-02T02:14:09.640Z:
Received chunk:  is...

2025-04-02T02:14:09.734Z:
Received chunk:  under...

2025-04-02T02:14:09.822Z:
Received chunk:  the...

2025-04-02T02:14:09.910Z:
Received chunk:  radar...

2025-04-02T02:14:09.996Z:
Received chunk:  but...

2025-04-02T02:14:10.084Z:
Received chunk:  has...

2025-04-02T02:14:10.170Z:
Received chunk:  its...

2025-04-02T02:14:10.258Z:
Received chunk:  own...

2025-04-02T02:14:10.346Z:
Received chunk:  unique...

2025-04-02T02:14:10.433Z:
Received chunk:  appeal...

2025-04-02T02:14:10.523Z:
Received chunk: .

...

2025-04-02T02:14:10.610Z:
Received chunk: Now...

2025-04-02T02:14:10.703Z:
Received chunk: ,...

2025-04-02T02:14:10.794Z:
Received chunk:  thinking...

2025-04-02T02:14:10.881Z:
Received chunk:  about...

2025-04-02T02:14:10.968Z:
Received chunk:  each...

2025-04-02T02:14:11.054Z:
Received chunk:  recipe...

2025-04-02T02:14:11.140Z:
Received chunk: :

...

2025-04-02T02:14:11.228Z:
Received chunk: 1...

2025-04-02T02:14:11.315Z:
Received chunk: ....

2025-04-02T02:14:11.402Z:
Received chunk:  **...

2025-04-02T02:14:11.488Z:
Received chunk: No...

2025-04-02T02:14:11.577Z:
Received chunk: -B...

2025-04-02T02:14:11.670Z:
Received chunk: ake...

2025-04-02T02:14:11.762Z:
Received chunk:  Chocolate...

2025-04-02T02:14:11.850Z:
Received chunk:  Chip...

2025-04-02T02:14:11.937Z:
Received chunk:  Cookies...

2025-04-02T02:14:12.024Z:
Received chunk: **:...

2025-04-02T02:14:12.110Z:
Received chunk:  This...

2025-04-02T02:14:12.198Z:
Received chunk:  would...

2025-04-02T02:14:12.286Z:
Received chunk:  be...

2025-04-02T02:14:12.374Z:
Received chunk:  great...

2025-04-02T02:14:12.461Z:
Received chunk:  for...

2025-04-02T02:14:12.549Z:
Received chunk:  people...

2025-04-02T02:14:12.641Z:
Received chunk:  who...

2025-04-02T02:14:12.735Z:
Received chunk:  don...

2025-04-02T02:14:12.823Z:
Received chunk: 't...

2025-04-02T02:14:12.911Z:
Received chunk:  want...

2025-04-02T02:14:12.999Z:
Received chunk:  to...

2025-04-02T02:14:13.086Z:
Received chunk:  deal...

2025-04-02T02:14:13.174Z:
Received chunk:  with...

2025-04-02T02:14:13.263Z:
Received chunk:  oven...

2025-04-02T02:14:13.351Z:
Received chunk:  temps...

2025-04-02T02:14:13.438Z:
Received chunk:  or...

2025-04-02T02:14:13.532Z:
Received chunk:  cleanup...

2025-04-02T02:14:13.614Z:
Received chunk: ....

2025-04-02T02:14:13.706Z:
Received chunk:  Using...

2025-04-02T02:14:13.803Z:
Received chunk:  instant...

2025-04-02T02:14:13.893Z:
Received chunk:  pudding...

2025-04-02T02:14:13.981Z:
Received chunk:  mix...

2025-04-02T02:14:14.067Z:
Received chunk:  and...

2025-04-02T02:14:14.155Z:
Received chunk:  crunchy...

2025-04-02T02:14:14.242Z:
Received chunk:  chips...

2025-04-02T02:14:14.331Z:
Received chunk:  makes...

2025-04-02T02:14:14.417Z:
Received chunk:  them...

2025-04-02T02:14:14.504Z:
Received chunk:  simple...

2025-04-02T02:14:14.593Z:
Received chunk:  yet...

2025-04-02T02:14:14.684Z:
Received chunk:  delicious...

2025-04-02T02:14:14.776Z:
Received chunk: .

...

2025-04-02T02:14:14.864Z:
Received chunk: 2...

2025-04-02T02:14:14.952Z:
Received chunk: ....

2025-04-02T02:14:15.039Z:
Received chunk:  **...

2025-04-02T02:14:15.126Z:
Received chunk: Al...

2025-04-02T02:14:15.213Z:
Received chunk: mond...

2025-04-02T02:14:15.301Z:
Received chunk: -F...

2025-04-02T02:14:15.390Z:
Received chunk: illed...

2025-04-02T02:14:15.477Z:
Received chunk:  Sn...

2025-04-02T02:14:15.565Z:
Received chunk: icker...

2025-04-02T02:14:15.655Z:
Received chunk: d...

2025-04-02T02:14:15.747Z:
Received chunk: oodles...

2025-04-02T02:14:15.838Z:
Received chunk: **:...

2025-04-02T02:14:15.926Z:
Received chunk:  Comb...

2025-04-02T02:14:16.013Z:
Received chunk: ining...

2025-04-02T02:14:16.099Z:
Received chunk:  almonds...

2025-04-02T02:14:16.187Z:
Received chunk:  with...

2025-04-02T02:14:16.275Z:
Received chunk:  a...

2025-04-02T02:14:16.364Z:
Received chunk:  classic...

2025-04-02T02:14:16.451Z:
Received chunk:  sn...

2025-04-02T02:14:16.546Z:
Received chunk: icker...

2025-04-02T02:14:16.628Z:
Received chunk: d...

2025-04-02T02:14:16.720Z:
Received chunk: oodle...

2025-04-02T02:14:16.812Z:
Received chunk:  base...

2025-04-02T02:14:16.900Z:
Received chunk:  offers...

2025-04-02T02:14:16.988Z:
Received chunk:  a...

2025-04-02T02:14:17.076Z:
Received chunk:  unique...

2025-04-02T02:14:17.163Z:
Received chunk:  taste...

2025-04-02T02:14:17.250Z:
Received chunk: ....

2025-04-02T02:14:17.337Z:
Received chunk:  They...

2025-04-02T02:14:17.425Z:
Received chunk:  can...

2025-04-02T02:14:17.512Z:
Received chunk:  be...

2025-04-02T02:14:17.602Z:
Received chunk:  made...

2025-04-02T02:14:17.694Z:
Received chunk:  by...

2025-04-02T02:14:17.788Z:
Received chunk:  adding...

2025-04-02T02:14:17.875Z:
Received chunk:  almond...

2025-04-02T02:14:17.963Z:
Received chunk:  sl...

2025-04-02T02:14:18.051Z:
Received chunk: ivers...

2025-04-02T02:14:18.139Z:
Received chunk:  to...

2025-04-02T02:14:18.227Z:
Received chunk:  the...

2025-04-02T02:14:18.314Z:
Received chunk:  dough...

2025-04-02T02:14:18.404Z:
Received chunk:  before...

2025-04-02T02:14:18.490Z:
Received chunk:  baking...

2025-04-02T02:14:18.578Z:
Received chunk: .

...

2025-04-02T02:14:18.669Z:
Received chunk: 3...

2025-04-02T02:14:18.762Z:
Received chunk: ....

2025-04-02T02:14:18.852Z:
Received chunk:  **...

2025-04-02T02:14:18.939Z:
Received chunk: Double...

2025-04-02T02:14:19.026Z:
Received chunk:  Ore...

2025-04-02T02:14:19.112Z:
Received chunk: o...

2025-04-02T02:14:19.199Z:
Received chunk:  Stack...

2025-04-02T02:14:19.286Z:
Received chunk:  Cookies...

2025-04-02T02:14:19.373Z:
Received chunk: **:...

2025-04-02T02:14:19.461Z:
Received chunk:  Using...

2025-04-02T02:14:19.549Z:
Received chunk:  two...

2025-04-02T02:14:19.638Z:
Received chunk:  layers...

2025-04-02T02:14:19.730Z:
Received chunk:  of...

2025-04-02T02:14:19.821Z:
Received chunk:  O...

2025-04-02T02:14:19.909Z:
Received chunk: reo...

2025-04-02T02:14:19.995Z:
Received chunk:  cookies...

2025-04-02T02:14:20.083Z:
Received chunk: ,...

2025-04-02T02:14:20.170Z:
Received chunk:  you...

2025-04-02T02:14:20.257Z:
Received chunk:  can...

2025-04-02T02:14:20.346Z:
Received chunk:  create...

2025-04-02T02:14:20.434Z:
Received chunk:  thick...

2025-04-02T02:14:20.521Z:
Received chunk: ,...

2025-04-02T02:14:20.611Z:
Received chunk:  chew...

2025-04-02T02:14:20.704Z:
Received chunk: y...

2025-04-02T02:14:20.797Z:
Received chunk:  cookies...

2025-04-02T02:14:20.888Z:
Received chunk:  that...

2025-04-02T02:14:20.978Z:
Received chunk:  are...

2025-04-02T02:14:21.069Z:
Received chunk:  easy...

2025-04-02T02:14:21.158Z:
Received chunk:  to...

2025-04-02T02:14:21.247Z:
Received chunk:  stack...

2025-04-02T02:14:21.339Z:
Received chunk:  and...

2025-04-02T02:14:21.427Z:
Received chunk:  enjoy...

2025-04-02T02:14:21.516Z:
Received chunk: .

...

2025-04-02T02:14:21.608Z:
Received chunk: 4...

2025-04-02T02:14:21.707Z:
Received chunk: ....

2025-04-02T02:14:21.808Z:
Received chunk:  **...

2025-04-02T02:14:21.903Z:
Received chunk: Mac...

2025-04-02T02:14:21.999Z:
Received chunk: aron...

2025-04-02T02:14:22.095Z:
Received chunk:  Cookies...

2025-04-02T02:14:22.191Z:
Received chunk: **:...

2025-04-02T02:14:22.286Z:
Received chunk:  These...

2025-04-02T02:14:22.381Z:
Received chunk:  are...

2025-04-02T02:14:22.476Z:
Received chunk:  delicate...

2025-04-02T02:14:22.573Z:
Received chunk:  and...

2025-04-02T02:14:22.656Z:
Received chunk:  require...

2025-04-02T02:14:22.751Z:
Received chunk:  precise...

2025-04-02T02:14:22.841Z:
Received chunk:  baking...

2025-04-02T02:14:22.928Z:
Received chunk:  temperatures...

2025-04-02T02:14:23.016Z:
Received chunk: ,...

2025-04-02T02:14:23.104Z:
Received chunk:  making...

2025-04-02T02:14:23.191Z:
Received chunk:  them...

2025-04-02T02:14:23.279Z:
Received chunk:  a...

2025-04-02T02:14:23.366Z:
Received chunk:  bit...

2025-04-02T02:14:23.453Z:
Received chunk:  challenging...

2025-04-02T02:14:23.540Z:
Received chunk:  but...

2025-04-02T02:14:23.630Z:
Received chunk:  rewarding...

2025-04-02T02:14:23.722Z:
Received chunk: .

...

2025-04-02T02:14:23.814Z:
Received chunk: 5...

2025-04-02T02:14:23.902Z:
Received chunk: ....

2025-04-02T02:14:23.990Z:
Received chunk:  **...

2025-04-02T02:14:24.079Z:
Received chunk: Co...

2025-04-02T02:14:24.166Z:
Received chunk: conut...

2025-04-02T02:14:24.253Z:
Received chunk:  Mac...

2025-04-02T02:14:24.340Z:
Received chunk: adam...

2025-04-02T02:14:24.428Z:
Received chunk: ia...

2025-04-02T02:14:24.517Z:
Received chunk:  Nut...

2025-04-02T02:14:24.606Z:
Received chunk:  Cookies...

2025-04-02T02:14:24.702Z:
Received chunk: **:...

2025-04-02T02:14:24.787Z:
Received chunk:  Rich...

2025-04-02T02:14:24.882Z:
Received chunk:  and...

2025-04-02T02:14:24.973Z:
Received chunk:  nut...

2025-04-02T02:14:25.061Z:
Received chunk: ty...

2025-04-02T02:14:25.150Z:
Received chunk:  with...

2025-04-02T02:14:25.240Z:
Received chunk:  coconut...

2025-04-02T02:14:25.329Z:
Received chunk:  flavor...

2025-04-02T02:14:25.417Z:
Received chunk: ,...

2025-04-02T02:14:25.505Z:
Received chunk:  these...

2025-04-02T02:14:25.595Z:
Received chunk:  would...

2025-04-02T02:14:25.685Z:
Received chunk:  be...

2025-04-02T02:14:25.780Z:
Received chunk:  great...

2025-04-02T02:14:25.872Z:
Received chunk:  for...

2025-04-02T02:14:25.960Z:
Received chunk:  those...

2025-04-02T02:14:26.048Z:
Received chunk:  who...

2025-04-02T02:14:26.136Z:
Received chunk:  love...

2025-04-02T02:14:26.222Z:
Received chunk:  both...

2025-04-02T02:14:26.310Z:
Received chunk:  mac...

2025-04-02T02:14:26.399Z:
Received chunk: adam...

2025-04-02T02:14:26.487Z:
Received chunk: ia...

2025-04-02T02:14:26.576Z:
Received chunk:  nuts...

2025-04-02T02:14:26.666Z:
Received chunk:  and...

2025-04-02T02:14:26.759Z:
Received chunk:  coconut...

2025-04-02T02:14:26.853Z:
Received chunk: .

...

2025-04-02T02:14:26.941Z:
Received chunk: I...

2025-04-02T02:14:27.029Z:
Received chunk:  think...

2025-04-02T02:14:27.117Z:
Received chunk:  these...

2025-04-02T02:14:27.204Z:
Received chunk:  five...

2025-04-02T02:14:27.291Z:
Received chunk:  options...

2025-04-02T02:14:27.378Z:
Received chunk:  cover...

2025-04-02T02:14:27.466Z:
Received chunk:  different...

2025-04-02T02:14:27.553Z:
Received chunk:  aspects...

2025-04-02T02:14:27.643Z:
Received chunk: —...

2025-04-02T02:14:27.735Z:
Received chunk: no...

2025-04-02T02:14:27.829Z:
Received chunk: -b...

2025-04-02T02:14:27.918Z:
Received chunk: ake...

2025-04-02T02:14:28.006Z:
Received chunk: ,...

2025-04-02T02:14:28.094Z:
Received chunk:  unique...

2025-04-02T02:14:28.182Z:
Received chunk:  shapes...

2025-04-02T02:14:28.268Z:
Received chunk: ,...

2025-04-02T02:14:28.356Z:
Received chunk:  almond...

2025-04-02T02:14:28.444Z:
Received chunk:  twist...

2025-04-02T02:14:28.532Z:
Received chunk: ,...

2025-04-02T02:14:28.620Z:
Received chunk:  ore...

2025-04-02T02:14:28.717Z:
Received chunk: o...

2025-04-02T02:14:28.803Z:
Received chunk:  stacking...

2025-04-02T02:14:28.894Z:
Received chunk: ,...

2025-04-02T02:14:28.982Z:
Received chunk:  and...

2025-04-02T02:14:29.070Z:
Received chunk:  coconut...

2025-04-02T02:14:29.157Z:
Received chunk:  flavors...

2025-04-02T02:14:29.246Z:
Received chunk: ....

2025-04-02T02:14:29.334Z:
Received chunk:  Each...

2025-04-02T02:14:29.421Z:
Received chunk:  offers...

2025-04-02T02:14:29.509Z:
Received chunk:  something...

2025-04-02T02:14:29.601Z:
Received chunk:  slightly...

2025-04-02T02:14:29.693Z:
Received chunk:  different...

2025-04-02T02:14:29.789Z:
Received chunk:  and...

2025-04-02T02:14:29.886Z:
Received chunk:  is...

2025-04-02T02:14:29.975Z:
Received chunk:  not...

2025-04-02T02:14:30.063Z:
Received chunk:  commonly...

2025-04-02T02:14:30.151Z:
Received chunk:  over...

2025-04-02T02:14:30.238Z:
Received chunk: em...

2025-04-02T02:14:30.325Z:
Received chunk: phas...

2025-04-02T02:14:30.412Z:
Received chunk: ized...

2025-04-02T02:14:30.498Z:
Received chunk:  in...

2025-04-02T02:14:30.586Z:
Received chunk:  regular...

2025-04-02T02:14:30.677Z:
Received chunk:  cookie...

2025-04-02T02:14:30.770Z:
Received chunk:  making...

2025-04-02T02:14:30.863Z:
Received chunk: .
...

2025-04-02T02:14:30.953Z:
Received chunk: </think>...

2025-04-02T02:14:31.040Z:
Received chunk: 

...

2025-04-02T02:14:31.133Z:
Received chunk: Here...

2025-04-02T02:14:31.224Z:
Received chunk:  are...

2025-04-02T02:14:31.311Z:
Received chunk:  five...

2025-04-02T02:14:31.398Z:
Received chunk:  unique...

2025-04-02T02:14:31.486Z:
Received chunk:  and...

2025-04-02T02:14:31.573Z:
Received chunk:  under...

2025-04-02T02:14:31.662Z:
Received chunk: app...

2025-04-02T02:14:31.754Z:
Received chunk: rec...

2025-04-02T02:14:31.848Z:
Received chunk: iated...

2025-04-02T02:14:31.935Z:
Received chunk:  cookie...

2025-04-02T02:14:32.024Z:
Received chunk:  recipes...

2025-04-02T02:14:32.114Z:
Received chunk:  that...

2025-04-02T02:14:32.203Z:
Received chunk:  offer...

2025-04-02T02:14:32.294Z:
Received chunk:  a...

2025-04-02T02:14:32.385Z:
Received chunk:  variety...

2025-04-02T02:14:32.475Z:
Received chunk:  of...

2025-04-02T02:14:32.566Z:
Received chunk:  tastes...

2025-04-02T02:14:32.658Z:
Received chunk:  and...

2025-04-02T02:14:32.755Z:
Received chunk:  methods...

2025-04-02T02:14:32.851Z:
Received chunk: :

...

2025-04-02T02:14:32.943Z:
Received chunk: 1...

2025-04-02T02:14:33.034Z:
Received chunk: ....

2025-04-02T02:14:33.124Z:
Received chunk:  **...

2025-04-02T02:14:33.215Z:
Received chunk: No...

2025-04-02T02:14:33.305Z:
Received chunk: -B...

2025-04-02T02:14:33.396Z:
Received chunk: ake...

2025-04-02T02:14:33.487Z:
Received chunk:  Chocolate...

2025-04-02T02:14:33.579Z:
Received chunk:  Chip...

2025-04-02T02:14:33.671Z:
Received chunk:  Cookies...

2025-04-02T02:14:33.768Z:
Received chunk: **
...

2025-04-02T02:14:33.863Z:
Received chunk:   ...

2025-04-02T02:14:33.954Z:
Received chunk:  -...

2025-04-02T02:14:34.044Z:
Received chunk:  Perfect...

2025-04-02T02:14:34.136Z:
Received chunk:  for...

2025-04-02T02:14:34.226Z:
Received chunk:  those...

2025-04-02T02:14:34.316Z:
Received chunk:  who...

2025-04-02T02:14:34.408Z:
Received chunk:  prefer...

2025-04-02T02:14:34.495Z:
Received chunk:  simplicity...

2025-04-02T02:14:34.583Z:
Received chunk: ,...

2025-04-02T02:14:34.672Z:
Received chunk:  these...

2025-04-02T02:14:34.765Z:
Received chunk:  cookies...

2025-04-02T02:14:34.858Z:
Received chunk:  use...

2025-04-02T02:14:34.946Z:
Received chunk:  instant...

2025-04-02T02:14:35.034Z:
Received chunk:  pudding...

2025-04-02T02:14:35.122Z:
Received chunk:  mix...

2025-04-02T02:14:35.209Z:
Received chunk:  combined...

2025-04-02T02:14:35.297Z:
Received chunk:  with...

2025-04-02T02:14:35.384Z:
Received chunk:  chocolate...

2025-04-02T02:14:35.471Z:
Received chunk:  chip...

2025-04-02T02:14:35.558Z:
Received chunk:  pieces...

2025-04-02T02:14:35.646Z:
Received chunk:  for...

2025-04-02T02:14:35.737Z:
Received chunk:  a...

2025-04-02T02:14:35.832Z:
Received chunk:  quick...

2025-04-02T02:14:35.922Z:
Received chunk:  and...

2025-04-02T02:14:36.010Z:
Received chunk:  delicious...

2025-04-02T02:14:36.097Z:
Received chunk:  treat...

2025-04-02T02:14:36.186Z:
Received chunk: .

...

2025-04-02T02:14:36.274Z:
Received chunk: 2...

2025-04-02T02:14:36.361Z:
Received chunk: ....

2025-04-02T02:14:36.448Z:
Received chunk:  **...

2025-04-02T02:14:36.537Z:
Received chunk: Al...

2025-04-02T02:14:36.629Z:
Received chunk: mond...

2025-04-02T02:14:36.713Z:
Received chunk: -F...

2025-04-02T02:14:36.807Z:
Received chunk: illed...

2025-04-02T02:14:36.900Z:
Received chunk:  Sn...

2025-04-02T02:14:36.988Z:
Received chunk: icker...

2025-04-02T02:14:37.076Z:
Received chunk: d...

2025-04-02T02:14:37.164Z:
Received chunk: oodles...

2025-04-02T02:14:37.252Z:
Received chunk: **
...

2025-04-02T02:14:37.340Z:
Received chunk:   ...

2025-04-02T02:14:37.427Z:
Received chunk:  -...

2025-04-02T02:14:37.514Z:
Received chunk:  Enh...

2025-04-02T02:14:37.602Z:
Received chunk: ance...

2025-04-02T02:14:37.692Z:
Received chunk:  the...

2025-04-02T02:14:37.783Z:
Received chunk:  classic...

2025-04-02T02:14:37.874Z:
Received chunk:  sn...

2025-04-02T02:14:37.963Z:
Received chunk: icker...

2025-04-02T02:14:38.051Z:
Received chunk: d...

2025-04-02T02:14:38.138Z:
Received chunk: oodle...

2025-04-02T02:14:38.225Z:
Received chunk:  flavor...

2025-04-02T02:14:38.313Z:
Received chunk:  by...

2025-04-02T02:14:38.401Z:
Received chunk:  adding...

2025-04-02T02:14:38.489Z:
Received chunk:  almond...

2025-04-02T02:14:38.576Z:
Received chunk:  sl...

2025-04-02T02:14:38.665Z:
Received chunk: ivers...

2025-04-02T02:14:38.755Z:
Received chunk:  to...

2025-04-02T02:14:38.848Z:
Received chunk:  the...

2025-04-02T02:14:38.939Z:
Received chunk:  dough...

2025-04-02T02:14:39.026Z:
Received chunk: ,...

2025-04-02T02:14:39.115Z:
Received chunk:  creating...

2025-04-02T02:14:39.203Z:
Received chunk:  a...

2025-04-02T02:14:39.290Z:
Received chunk:  unique...

2025-04-02T02:14:39.377Z:
Received chunk:  texture...

2025-04-02T02:14:39.467Z:
Received chunk:  and...

2025-04-02T02:14:39.554Z:
Received chunk:  taste...

2025-04-02T02:14:39.649Z:
Received chunk:  that...

2025-04-02T02:14:39.733Z:
Received chunk: 's...

2025-04-02T02:14:39.828Z:
Received chunk:  easy...

2025-04-02T02:14:39.923Z:
Received chunk:  to...

2025-04-02T02:14:40.013Z:
Received chunk:  make...

2025-04-02T02:14:40.100Z:
Received chunk: .

...

2025-04-02T02:14:40.188Z:
Received chunk: 3...

2025-04-02T02:14:40.276Z:
Received chunk: ....

2025-04-02T02:14:40.364Z:
Received chunk:  **...

2025-04-02T02:14:40.454Z:
Received chunk: Double...

2025-04-02T02:14:40.542Z:
Received chunk:  Ore...

2025-04-02T02:14:40.631Z:
Received chunk: o...

2025-04-02T02:14:40.722Z:
Received chunk:  Stack...

2025-04-02T02:14:40.818Z:
Received chunk:  Cookies...

2025-04-02T02:14:40.913Z:
Received chunk: **
...

2025-04-02T02:14:41.002Z:
Received chunk:   ...

2025-04-02T02:14:41.089Z:
Received chunk:  -...

2025-04-02T02:14:41.177Z:
Received chunk:  Create...

2025-04-02T02:14:41.265Z:
Received chunk:  thick...

2025-04-02T02:14:41.353Z:
Received chunk: ,...

2025-04-02T02:14:41.440Z:
Received chunk:  chew...

2025-04-02T02:14:41.528Z:
Received chunk: y...

2025-04-02T02:14:41.616Z:
Received chunk:  cookies...

2025-04-02T02:14:41.705Z:
Received chunk:  by...

2025-04-02T02:14:41.795Z:
Received chunk:  sandwich...

2025-04-02T02:14:41.889Z:
Received chunk: ing...

2025-04-02T02:14:41.978Z:
Received chunk:  two...

2025-04-02T02:14:42.066Z:
Received chunk:  layers...

2025-04-02T02:14:42.154Z:
Received chunk:  of...

2025-04-02T02:14:42.242Z:
Received chunk:  Ore...

2025-04-02T02:14:42.333Z:
Received chunk: os...

2025-04-02T02:14:42.420Z:
Received chunk:  together...

2025-04-02T02:14:42.511Z:
Received chunk: ,...

2025-04-02T02:14:42.600Z:
Received chunk:  ideal...

2025-04-02T02:14:42.690Z:
Received chunk:  for...

2025-04-02T02:14:42.784Z:
Received chunk:  stacking...

2025-04-02T02:14:42.877Z:
Received chunk:  and...

2025-04-02T02:14:42.970Z:
Received chunk:  enjoying...

2025-04-02T02:14:43.067Z:
Received chunk:  multiple...

2025-04-02T02:14:43.155Z:
Received chunk:  layers...

2025-04-02T02:14:43.242Z:
Received chunk:  at...

2025-04-02T02:14:43.331Z:
Received chunk:  once...

2025-04-02T02:14:43.418Z:
Received chunk: .

...

2025-04-02T02:14:43.506Z:
Received chunk: 4...

2025-04-02T02:14:43.594Z:
Received chunk: ....

2025-04-02T02:14:43.683Z:
Received chunk:  **...

2025-04-02T02:14:43.773Z:
Received chunk: Mac...

2025-04-02T02:14:43.868Z:
Received chunk: aron...

2025-04-02T02:14:43.959Z:
Received chunk:  Cookies...

2025-04-02T02:14:44.047Z:
Received chunk: **
...

2025-04-02T02:14:44.135Z:
Received chunk:   ...

2025-04-02T02:14:44.223Z:
Received chunk:  -...

2025-04-02T02:14:44.309Z:
Received chunk:  Achie...

2025-04-02T02:14:44.398Z:
Received chunk: ve...

2025-04-02T02:14:44.486Z:
Received chunk:  delicate...

2025-04-02T02:14:44.574Z:
Received chunk:  and...

2025-04-02T02:14:44.662Z:
Received chunk:  elegant...

2025-04-02T02:14:44.751Z:
Received chunk:  cookies...

2025-04-02T02:14:44.844Z:
Received chunk:  with...

2025-04-02T02:14:44.937Z:
Received chunk:  precise...

2025-04-02T02:14:45.025Z:
Received chunk:  baking...

2025-04-02T02:14:45.113Z:
Received chunk:  temperatures...

2025-04-02T02:14:45.201Z:
Received chunk: ,...

2025-04-02T02:14:45.288Z:
Received chunk:  offering...

2025-04-02T02:14:45.377Z:
Received chunk:  a...

2025-04-02T02:14:45.464Z:
Received chunk:  rewarding...

2025-04-02T02:14:45.552Z:
Received chunk:  experience...

2025-04-02T02:14:45.638Z:
Received chunk:  for...

2025-04-02T02:14:45.728Z:
Received chunk:  those...

2025-04-02T02:14:45.819Z:
Received chunk:  who...

2025-04-02T02:14:45.912Z:
Received chunk:  enjoy...

2025-04-02T02:14:46.002Z:
Received chunk:  intricate...

2025-04-02T02:14:46.090Z:
Received chunk:  cookie...

2025-04-02T02:14:46.181Z:
Received chunk: -making...

2025-04-02T02:14:46.273Z:
Received chunk: .

...

2025-04-02T02:14:46.365Z:
Received chunk: 5...

2025-04-02T02:14:46.455Z:
Received chunk: ....

2025-04-02T02:14:46.543Z:
Received chunk:  **...

2025-04-02T02:14:46.633Z:
Received chunk: Co...

2025-04-02T02:14:46.723Z:
Received chunk: conut...

2025-04-02T02:14:46.816Z:
Received chunk:  Mac...

2025-04-02T02:14:46.909Z:
Received chunk: adam...

2025-04-02T02:14:46.999Z:
Received chunk: ia...

2025-04-02T02:14:47.087Z:
Received chunk:  Nut...

2025-04-02T02:14:47.176Z:
Received chunk:  Cookies...

2025-04-02T02:14:47.266Z:
Received chunk: **
...

2025-04-02T02:14:47.355Z:
Received chunk:   ...

2025-04-02T02:14:47.443Z:
Received chunk:  -...

2025-04-02T02:14:47.532Z:
Received chunk:  Combine...

2025-04-02T02:14:47.620Z:
Received chunk:  the...

2025-04-02T02:14:47.709Z:
Received chunk:  rich...

2025-04-02T02:14:47.803Z:
Received chunk:  flavors...

2025-04-02T02:14:47.898Z:
Received chunk:  of...

2025-04-02T02:14:47.989Z:
Received chunk:  mac...

2025-04-02T02:14:48.077Z:
Received chunk: adam...

2025-04-02T02:14:48.165Z:
Received chunk: ia...

2025-04-02T02:14:48.253Z:
Received chunk:  nuts...

2025-04-02T02:14:48.342Z:
Received chunk:  and...

2025-04-02T02:14:48.430Z:
Received chunk:  coconut...

2025-04-02T02:14:48.518Z:
Received chunk:  for...

2025-04-02T02:14:48.606Z:
Received chunk:  a...

2025-04-02T02:14:48.700Z:
Received chunk:  nut...

2025-04-02T02:14:48.784Z:
Received chunk: ty...

2025-04-02T02:14:48.877Z:
Received chunk: ,...

2025-04-02T02:14:48.967Z:
Received chunk:  coconut...

2025-04-02T02:14:49.056Z:
Received chunk: -inf...

2025-04-02T02:14:49.145Z:
Received chunk: used...

2025-04-02T02:14:49.233Z:
Received chunk:  treat...

2025-04-02T02:14:49.322Z:
Received chunk:  that...

2025-04-02T02:14:49.410Z:
Received chunk: 's...

2025-04-02T02:14:49.499Z:
Received chunk:  both...

2025-04-02T02:14:49.587Z:
Received chunk:  satisfying...

2025-04-02T02:14:49.676Z:
Received chunk:  and...

2025-04-02T02:14:49.767Z:
Received chunk:  indul...

2025-04-02T02:14:49.861Z:
Received chunk: gent...

2025-04-02T02:14:49.952Z:
Received chunk: .

...

2025-04-02T02:14:50.043Z:
Received chunk: Each...

2025-04-02T02:14:50.131Z:
Received chunk:  recipe...

2025-04-02T02:14:50.219Z:
Received chunk:  offers...

2025-04-02T02:14:50.306Z:
Received chunk:  a...

2025-04-02T02:14:50.395Z:
Received chunk:  distinct...

2025-04-02T02:14:50.484Z:
Received chunk:  twist...

2025-04-02T02:14:50.572Z:
Received chunk: ,...

2025-04-02T02:14:50.660Z:
Received chunk:  catering...

2025-04-02T02:14:50.749Z:
Received chunk:  to...

2025-04-02T02:14:50.842Z:
Received chunk:  different...

2025-04-02T02:14:50.934Z:
Received chunk:  preferences...

2025-04-02T02:14:51.024Z:
Received chunk:  and...

2025-04-02T02:14:51.112Z:
Received chunk:  skill...

2025-04-02T02:14:51.201Z:
Received chunk:  levels...

2025-04-02T02:14:51.289Z:
Received chunk:  in...

2025-04-02T02:14:51.377Z:
Received chunk:  the...

2025-04-02T02:14:51.464Z:
Received chunk:  kitchen...

2025-04-02T02:14:51.551Z:
Received chunk: ....

2025-04-02T02:14:51.639Z:
Received chunk: 

> ...

2025-04-02T02:19:41.900Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:19:41.900Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:19:41.901Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:19:41.901Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:19:42.805Z:
Unloading model...

2025-04-02T02:19:42.806Z:
Model unloaded successfully

2025-04-02T02:19:42.806Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:19:42.806Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:19:42.806Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:19:42.807Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:20:01.041Z:
Generate response called with message: give me a list of 5 universally loved cookie recipes...

2025-04-02T02:20:01.041Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a list of 5 universally loved cookie recipes
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:20:01.066Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:20:01.089Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:20:01.111Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:20:01.120Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:20:01.143Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:20:01.197Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:20:01.198Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:20:01.219Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:20:01.435Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:20:01.458Z:
Process stderr: .

2025-04-02T02:20:01.478Z:
Process stderr: .

2025-04-02T02:20:01.497Z:
Process stderr: .

2025-04-02T02:20:01.516Z:
Process stderr: .

2025-04-02T02:20:01.535Z:
Process stderr: .

2025-04-02T02:20:01.554Z:
Process stderr: .

2025-04-02T02:20:01.574Z:
Process stderr: .

2025-04-02T02:20:01.606Z:
Process stderr: .

2025-04-02T02:20:01.613Z:
Process stderr: .

2025-04-02T02:20:01.646Z:
Process stderr: .

2025-04-02T02:20:01.664Z:
Process stderr: .

2025-04-02T02:20:01.687Z:
Process stderr: .

2025-04-02T02:20:01.703Z:
Process stderr: .

2025-04-02T02:20:01.726Z:
Process stderr: .

2025-04-02T02:20:01.742Z:
Process stderr: .

2025-04-02T02:20:01.759Z:
Process stderr: .

2025-04-02T02:20:01.782Z:
Process stderr: .

2025-04-02T02:20:01.797Z:
Process stderr: .

2025-04-02T02:20:01.813Z:
Process stderr: .

2025-04-02T02:20:01.836Z:
Process stderr: .

2025-04-02T02:20:01.852Z:
Process stderr: .

2025-04-02T02:20:01.878Z:
Process stderr: .

2025-04-02T02:20:01.893Z:
Process stderr: .

2025-04-02T02:20:01.913Z:
Process stderr: .

2025-04-02T02:20:01.933Z:
Process stderr: .

2025-04-02T02:20:01.964Z:
Process stderr: .

2025-04-02T02:20:01.987Z:
Process stderr: .

2025-04-02T02:20:02.003Z:
Process stderr: .

2025-04-02T02:20:02.026Z:
Process stderr: .

2025-04-02T02:20:02.041Z:
Process stderr: .

2025-04-02T02:20:02.057Z:
Process stderr: .

2025-04-02T02:20:02.080Z:
Process stderr: .

2025-04-02T02:20:02.095Z:
Process stderr: .

2025-04-02T02:20:02.112Z:
Process stderr: .

2025-04-02T02:20:02.135Z:
Process stderr: .

2025-04-02T02:20:02.151Z:
Process stderr: .

2025-04-02T02:20:02.174Z:
Process stderr: .

2025-04-02T02:20:02.189Z:
Process stderr: .

2025-04-02T02:20:02.209Z:
Process stderr: .

2025-04-02T02:20:02.229Z:
Process stderr: .

2025-04-02T02:20:02.261Z:
Process stderr: .

2025-04-02T02:20:02.267Z:
Process stderr: .

2025-04-02T02:20:02.300Z:
Process stderr: .

2025-04-02T02:20:02.307Z:
Process stderr: .

2025-04-02T02:20:02.339Z:
Process stderr: .

2025-04-02T02:20:02.357Z:
Process stderr: .

2025-04-02T02:20:02.383Z:
Process stderr: .

2025-04-02T02:20:02.399Z:
Process stderr: .

2025-04-02T02:20:02.417Z:
Process stderr: .

2025-04-02T02:20:02.442Z:
Process stderr: .

2025-04-02T02:20:02.460Z:
Process stderr: .

2025-04-02T02:20:02.486Z:
Process stderr: .

2025-04-02T02:20:02.503Z:
Process stderr: .

2025-04-02T02:20:02.521Z:
Process stderr: .

2025-04-02T02:20:02.548Z:
Process stderr: .

2025-04-02T02:20:02.566Z:
Process stderr: .

2025-04-02T02:20:02.587Z:
Process stderr: .

2025-04-02T02:20:02.610Z:
Process stderr: .

2025-04-02T02:20:02.632Z:
Process stderr: .

2025-04-02T02:20:02.655Z:
Process stderr: .

2025-04-02T02:20:02.678Z:
Process stderr: .

2025-04-02T02:20:02.719Z:
Process stderr: .

2025-04-02T02:20:02.727Z:
Process stderr: .

2025-04-02T02:20:02.766Z:
Process stderr: .

2025-04-02T02:20:02.774Z:
Process stderr: .

2025-04-02T02:20:02.813Z:
Process stderr: ....

2025-04-02T02:20:02.814Z:
Process stderr: ........

2025-04-02T02:20:02.814Z:
Process stderr: ......


2025-04-02T02:20:02.817Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:20:02.817Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:20:02.837Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:20:02.843Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:20:03.122Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T02:20:03.122Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:20:03.123Z:
Process stderr: sampler seed: 3131550314
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:20:03.426Z:
Received chunk: <think>...

2025-04-02T02:20:03.519Z:
Received chunk: 
...

2025-04-02T02:20:03.613Z:
Received chunk: Okay...

2025-04-02T02:20:03.707Z:
Received chunk: ,...

2025-04-02T02:20:03.799Z:
Received chunk:  so...

2025-04-02T02:20:03.889Z:
Received chunk:  I...

2025-04-02T02:20:03.980Z:
Received chunk:  need...

2025-04-02T02:20:04.070Z:
Received chunk:  to...

2025-04-02T02:20:04.161Z:
Received chunk:  come...

2025-04-02T02:20:04.253Z:
Received chunk:  up...

2025-04-02T02:20:04.349Z:
Received chunk:  with...

2025-04-02T02:20:04.447Z:
Received chunk:  five...

2025-04-02T02:20:04.546Z:
Received chunk:  universally...

2025-04-02T02:20:04.644Z:
Received chunk:  loved...

2025-04-02T02:20:04.743Z:
Received chunk:  cookie...

2025-04-02T02:20:04.841Z:
Received chunk:  recipes...

2025-04-02T02:20:04.939Z:
Received chunk: ....

2025-04-02T02:20:05.036Z:
Received chunk:  Hmm...

2025-04-02T02:20:05.133Z:
Received chunk: ,...

2025-04-02T02:20:05.236Z:
Received chunk:  where...

2025-04-02T02:20:05.333Z:
Received chunk:  do...

2025-04-02T02:20:05.436Z:
Received chunk:  I...

2025-04-02T02:20:05.534Z:
Received chunk:  start...

2025-04-02T02:20:05.633Z:
Received chunk: ?...

2025-04-02T02:20:05.731Z:
Received chunk:  Well...

2025-04-02T02:20:05.829Z:
Received chunk: ,...

2025-04-02T02:20:05.925Z:
Received chunk:  I...

2025-04-02T02:20:06.011Z:
Received chunk:  know...

2025-04-02T02:20:06.095Z:
Received chunk:  some...

2025-04-02T02:20:06.181Z:
Received chunk:  popular...

2025-04-02T02:20:06.268Z:
Received chunk:  cookies...

2025-04-02T02:20:06.357Z:
Received chunk:  like...

2025-04-02T02:20:06.448Z:
Received chunk:  chocolate...

2025-04-02T02:20:06.534Z:
Received chunk:  chip...

2025-04-02T02:20:06.620Z:
Received chunk:  and...

2025-04-02T02:20:06.706Z:
Received chunk:  oat...

2025-04-02T02:20:06.792Z:
Received chunk: meal...

2025-04-02T02:20:06.878Z:
Received chunk:  rais...

2025-04-02T02:20:06.963Z:
Received chunk: in...

2025-04-02T02:20:07.049Z:
Received chunk: ,...

2025-04-02T02:20:07.135Z:
Received chunk:  but...

2025-04-02T02:20:07.222Z:
Received chunk:  maybe...

2025-04-02T02:20:07.310Z:
Received chunk:  there...

2025-04-02T02:20:07.399Z:
Received chunk:  are...

2025-04-02T02:20:07.487Z:
Received chunk:  others...

2025-04-02T02:20:07.573Z:
Received chunk:  that...

2025-04-02T02:20:07.659Z:
Received chunk:  are...

2025-04-02T02:20:07.744Z:
Received chunk:  just...

2025-04-02T02:20:07.829Z:
Received chunk:  as...

2025-04-02T02:20:07.915Z:
Received chunk:  beloved...

2025-04-02T02:20:08.002Z:
Received chunk: .

...

2025-04-02T02:20:08.087Z:
Received chunk: First...

2025-04-02T02:20:08.173Z:
Received chunk:  off...

2025-04-02T02:20:08.261Z:
Received chunk: ,...

2025-04-02T02:20:08.349Z:
Received chunk:  chocolate...

2025-04-02T02:20:08.439Z:
Received chunk:  chip...

2025-04-02T02:20:08.526Z:
Received chunk:  is...

2025-04-02T02:20:08.612Z:
Received chunk:  a...

2025-04-02T02:20:08.699Z:
Received chunk:  given...

2025-04-02T02:20:08.785Z:
Received chunk: —it...

2025-04-02T02:20:08.870Z:
Received chunk: 's...

2025-04-02T02:20:08.955Z:
Received chunk:  pretty...

2025-04-02T02:20:09.041Z:
Received chunk:  much...

2025-04-02T02:20:09.127Z:
Received chunk:  everyone...

2025-04-02T02:20:09.212Z:
Received chunk: 's...

2025-04-02T02:20:09.302Z:
Received chunk:  favorite...

2025-04-02T02:20:09.393Z:
Received chunk: ....

2025-04-02T02:20:09.482Z:
Received chunk:  But...

2025-04-02T02:20:09.569Z:
Received chunk:  then...

2025-04-02T02:20:09.656Z:
Received chunk:  again...

2025-04-02T02:20:09.745Z:
Received chunk: ,...

2025-04-02T02:20:09.832Z:
Received chunk:  there...

2025-04-02T02:20:09.919Z:
Received chunk:  are...

2025-04-02T02:20:10.008Z:
Received chunk:  so...

2025-04-02T02:20:10.094Z:
Received chunk:  many...

2025-04-02T02:20:10.181Z:
Received chunk:  variations...

2025-04-02T02:20:10.270Z:
Received chunk:  on...

2025-04-02T02:20:10.359Z:
Received chunk:  it...

2025-04-02T02:20:10.453Z:
Received chunk: ....

2025-04-02T02:20:10.541Z:
Received chunk:  Maybe...

2025-04-02T02:20:10.628Z:
Received chunk:  something...

2025-04-02T02:20:10.714Z:
Received chunk:  classic...

2025-04-02T02:20:10.800Z:
Received chunk:  first...

2025-04-02T02:20:10.887Z:
Received chunk: ....

2025-04-02T02:20:10.973Z:
Received chunk:  Next...

2025-04-02T02:20:11.058Z:
Received chunk: ,...

2025-04-02T02:20:11.144Z:
Received chunk:  oat...

2025-04-02T02:20:11.229Z:
Received chunk: meal...

2025-04-02T02:20:11.318Z:
Received chunk:  rais...

2025-04-02T02:20:11.409Z:
Received chunk: in...

2025-04-02T02:20:11.501Z:
Received chunk:  is...

2025-04-02T02:20:11.587Z:
Received chunk:  another...

2025-04-02T02:20:11.673Z:
Received chunk:  staple...

2025-04-02T02:20:11.759Z:
Received chunk: ....

2025-04-02T02:20:11.844Z:
Received chunk:  People...

2025-04-02T02:20:11.930Z:
Received chunk:  love...

2025-04-02T02:20:12.017Z:
Received chunk:  the...

2025-04-02T02:20:12.104Z:
Received chunk:  chew...

2025-04-02T02:20:12.189Z:
Received chunk: y...

2025-04-02T02:20:12.275Z:
Received chunk:  texture...

2025-04-02T02:20:12.367Z:
Received chunk:  and...

2025-04-02T02:20:12.459Z:
Received chunk:  the...

2025-04-02T02:20:12.550Z:
Received chunk:  spices...

2025-04-02T02:20:12.635Z:
Received chunk: ....

2025-04-02T02:20:12.721Z:
Received chunk:  That...

2025-04-02T02:20:12.807Z:
Received chunk:  could...

2025-04-02T02:20:12.892Z:
Received chunk:  be...

2025-04-02T02:20:12.978Z:
Received chunk:  number...

2025-04-02T02:20:13.064Z:
Received chunk:  two...

2025-04-02T02:20:13.149Z:
Received chunk: .

...

2025-04-02T02:20:13.235Z:
Received chunk: Now...

2025-04-02T02:20:13.323Z:
Received chunk: ,...

2025-04-02T02:20:13.415Z:
Received chunk:  what...

2025-04-02T02:20:13.507Z:
Received chunk:  else...

2025-04-02T02:20:13.594Z:
Received chunk: ?...

2025-04-02T02:20:13.680Z:
Received chunk:  I...

2025-04-02T02:20:13.767Z:
Received chunk: 've...

2025-04-02T02:20:13.852Z:
Received chunk:  heard...

2025-04-02T02:20:13.938Z:
Received chunk:  of...

2025-04-02T02:20:14.025Z:
Received chunk:  sn...

2025-04-02T02:20:14.111Z:
Received chunk: icker...

2025-04-02T02:20:14.196Z:
Received chunk: d...

2025-04-02T02:20:14.283Z:
Received chunk: oodles...

2025-04-02T02:20:14.373Z:
Received chunk:  before...

2025-04-02T02:20:14.465Z:
Received chunk: —they...

2025-04-02T02:20:14.555Z:
Received chunk:  have...

2025-04-02T02:20:14.640Z:
Received chunk:  that...

2025-04-02T02:20:14.726Z:
Received chunk:  nice...

2025-04-02T02:20:14.813Z:
Received chunk:  crack...

2025-04-02T02:20:14.899Z:
Received chunk: ly...

2025-04-02T02:20:14.985Z:
Received chunk:  exterior...

2025-04-02T02:20:15.072Z:
Received chunk:  and...

2025-04-02T02:20:15.159Z:
Received chunk:  soft...

2025-04-02T02:20:15.245Z:
Received chunk:  inside...

2025-04-02T02:20:15.332Z:
Received chunk: ....

2025-04-02T02:20:15.423Z:
Received chunk:  They...

2025-04-02T02:20:15.513Z:
Received chunk: 're...

2025-04-02T02:20:15.600Z:
Received chunk:  pretty...

2025-04-02T02:20:15.685Z:
Received chunk:  popular...

2025-04-02T02:20:15.771Z:
Received chunk:  too...

2025-04-02T02:20:15.857Z:
Received chunk: ....

2025-04-02T02:20:15.943Z:
Received chunk:  So...

2025-04-02T02:20:16.029Z:
Received chunk:  maybe...

2025-04-02T02:20:16.116Z:
Received chunk:  that...

2025-04-02T02:20:16.202Z:
Received chunk: 's...

2025-04-02T02:20:16.289Z:
Received chunk:  a...

2025-04-02T02:20:16.379Z:
Received chunk:  third...

2025-04-02T02:20:16.472Z:
Received chunk:  one...

2025-04-02T02:20:16.561Z:
Received chunk: ....

2025-04-02T02:20:16.647Z:
Received chunk:  

...

2025-04-02T02:20:16.734Z:
Received chunk: Looking...

2025-04-02T02:20:16.821Z:
Received chunk:  for...

2025-04-02T02:20:16.908Z:
Received chunk:  something...

2025-04-02T02:20:16.995Z:
Received chunk:  different...

2025-04-02T02:20:17.081Z:
Received chunk: ......

2025-04-02T02:20:17.166Z:
Received chunk:  How...

2025-04-02T02:20:17.252Z:
Received chunk:  about...

2025-04-02T02:20:17.339Z:
Received chunk:  sugar...

2025-04-02T02:20:17.432Z:
Received chunk:  cookies...

2025-04-02T02:20:17.524Z:
Received chunk: ?...

2025-04-02T02:20:17.611Z:
Received chunk:  They...

2025-04-02T02:20:17.698Z:
Received chunk: 're...

2025-04-02T02:20:17.785Z:
Received chunk:  used...

2025-04-02T02:20:17.871Z:
Received chunk:  in...

2025-04-02T02:20:17.957Z:
Received chunk:  Christmas...

2025-04-02T02:20:18.044Z:
Received chunk:  decorations...

2025-04-02T02:20:18.130Z:
Received chunk: ,...

2025-04-02T02:20:18.215Z:
Received chunk:  so...

2025-04-02T02:20:18.302Z:
Received chunk:  they...

2025-04-02T02:20:18.392Z:
Received chunk:  must...

2025-04-02T02:20:18.484Z:
Received chunk:  be...

2025-04-02T02:20:18.573Z:
Received chunk:  loved...

2025-04-02T02:20:18.660Z:
Received chunk:  by...

2025-04-02T02:20:18.747Z:
Received chunk:  a...

2025-04-02T02:20:18.834Z:
Received chunk:  lot...

2025-04-02T02:20:18.920Z:
Received chunk:  of...

2025-04-02T02:20:19.006Z:
Received chunk:  people...

2025-04-02T02:20:19.092Z:
Received chunk: ....

2025-04-02T02:20:19.178Z:
Received chunk:  People...

2025-04-02T02:20:19.264Z:
Received chunk:  make...

2025-04-02T02:20:19.353Z:
Received chunk:  them...

2025-04-02T02:20:19.444Z:
Received chunk:  every...

2025-04-02T02:20:19.536Z:
Received chunk:  holiday...

2025-04-02T02:20:19.624Z:
Received chunk:  season...

2025-04-02T02:20:19.711Z:
Received chunk: ,...

2025-04-02T02:20:19.797Z:
Received chunk:  right...

2025-04-02T02:20:19.884Z:
Received chunk: ?

...

2025-04-02T02:20:19.971Z:
Received chunk: Wait...

2025-04-02T02:20:20.057Z:
Received chunk: ,...

2025-04-02T02:20:20.143Z:
Received chunk:  but...

2025-04-02T02:20:20.229Z:
Received chunk:  I...

2025-04-02T02:20:20.315Z:
Received chunk:  should...

2025-04-02T02:20:20.405Z:
Received chunk:  check...

2025-04-02T02:20:20.495Z:
Received chunk:  if...

2025-04-02T02:20:20.584Z:
Received chunk:  these...

2025-04-02T02:20:20.672Z:
Received chunk:  are...

2025-04-02T02:20:20.759Z:
Received chunk:  indeed...

2025-04-02T02:20:20.845Z:
Received chunk:  universally...

2025-04-02T02:20:20.931Z:
Received chunk:  loved...

2025-04-02T02:20:21.017Z:
Received chunk:  or...

2025-04-02T02:20:21.103Z:
Received chunk:  if...

2025-04-02T02:20:21.189Z:
Received chunk:  there...

2025-04-02T02:20:21.275Z:
Received chunk: 's...

2025-04-02T02:20:21.363Z:
Received chunk:  another...

2025-04-02T02:20:21.454Z:
Received chunk:  cookie...

2025-04-02T02:20:21.543Z:
Received chunk:  missing...

2025-04-02T02:20:21.630Z:
Received chunk: ....

2025-04-02T02:20:21.722Z:
Received chunk:  Maybe...

2025-04-02T02:20:21.812Z:
Received chunk:  something...

2025-04-02T02:20:21.898Z:
Received chunk:  like...

2025-04-02T02:20:21.985Z:
Received chunk:  chocolate...

2025-04-02T02:20:22.073Z:
Received chunk:  drop...

2025-04-02T02:20:22.159Z:
Received chunk:  cookies...

2025-04-02T02:20:22.246Z:
Received chunk: ?...

2025-04-02T02:20:22.333Z:
Received chunk:  They...

2025-04-02T02:20:22.422Z:
Received chunk:  have...

2025-04-02T02:20:22.514Z:
Received chunk:  those...

2025-04-02T02:20:22.603Z:
Received chunk:  little...

2025-04-02T02:20:22.690Z:
Received chunk:  round...

2025-04-02T02:20:22.776Z:
Received chunk:  shapes...

2025-04-02T02:20:22.862Z:
Received chunk:  and...

2025-04-02T02:20:22.948Z:
Received chunk:  the...

2025-04-02T02:20:23.035Z:
Received chunk:  crispy...

2025-04-02T02:20:23.122Z:
Received chunk:  outside...

2025-04-02T02:20:23.207Z:
Received chunk:  with...

2025-04-02T02:20:23.294Z:
Received chunk:  chew...

2025-04-02T02:20:23.382Z:
Received chunk: y...

2025-04-02T02:20:23.472Z:
Received chunk:  ins...

2025-04-02T02:20:23.565Z:
Received chunk: ides...

2025-04-02T02:20:23.653Z:
Received chunk: —...

2025-04-02T02:20:23.739Z:
Received chunk: could...

2025-04-02T02:20:23.826Z:
Received chunk:  that...

2025-04-02T02:20:23.911Z:
Received chunk:  be...

2025-04-02T02:20:23.997Z:
Received chunk:  another...

2025-04-02T02:20:24.083Z:
Received chunk:  option...

2025-04-02T02:20:24.169Z:
Received chunk: ?

...

2025-04-02T02:20:24.256Z:
Received chunk: Or...

2025-04-02T02:20:24.342Z:
Received chunk:  perhaps...

2025-04-02T02:20:24.431Z:
Received chunk: DBC...

2025-04-02T02:20:24.522Z:
Received chunk: up...

2025-04-02T02:20:24.615Z:
Received chunk: cakes...

2025-04-02T02:20:24.704Z:
Received chunk: ?...

2025-04-02T02:20:24.793Z:
Received chunk:  No...

2025-04-02T02:20:24.880Z:
Received chunk: ,...

2025-04-02T02:20:24.969Z:
Received chunk:  wait...

2025-04-02T02:20:25.056Z:
Received chunk: ,...

2025-04-02T02:20:25.143Z:
Received chunk:  the...

2025-04-02T02:20:25.229Z:
Received chunk:  user...

2025-04-02T02:20:25.325Z:
Received chunk:  asked...

2025-04-02T02:20:25.411Z:
Received chunk:  for...

2025-04-02T02:20:25.502Z:
Received chunk:  cookies...

2025-04-02T02:20:25.593Z:
Received chunk:  specifically...

2025-04-02T02:20:25.681Z:
Received chunk: ....

2025-04-02T02:20:25.767Z:
Received chunk:  Okay...

2025-04-02T02:20:25.853Z:
Received chunk: ,...

2025-04-02T02:20:25.939Z:
Received chunk:  so...

2025-04-02T02:20:26.025Z:
Received chunk:  sticking...

2025-04-02T02:20:26.110Z:
Received chunk:  to...

2025-04-02T02:20:26.195Z:
Received chunk:  cookies...

2025-04-02T02:20:26.281Z:
Received chunk: .

...

2025-04-02T02:20:26.369Z:
Received chunk: So...

2025-04-02T02:20:26.460Z:
Received chunk:  compiling...

2025-04-02T02:20:26.549Z:
Received chunk:  these...

2025-04-02T02:20:26.637Z:
Received chunk: :...

2025-04-02T02:20:26.726Z:
Received chunk:  chocolate...

2025-04-02T02:20:26.812Z:
Received chunk:  chip...

2025-04-02T02:20:26.899Z:
Received chunk: ,...

2025-04-02T02:20:26.985Z:
Received chunk:  oat...

2025-04-02T02:20:27.071Z:
Received chunk: meal...

2025-04-02T02:20:27.157Z:
Received chunk:  rais...

2025-04-02T02:20:27.243Z:
Received chunk: in...

2025-04-02T02:20:27.336Z:
Received chunk: ,...

2025-04-02T02:20:27.419Z:
Received chunk:  sn...

2025-04-02T02:20:27.512Z:
Received chunk: icker...

2025-04-02T02:20:27.603Z:
Received chunk: d...

2025-04-02T02:20:27.691Z:
Received chunk: oodles...

2025-04-02T02:20:27.779Z:
Received chunk: ,...

2025-04-02T02:20:27.866Z:
Received chunk:  sugar...

2025-04-02T02:20:27.953Z:
Received chunk:  cookies...

2025-04-02T02:20:28.041Z:
Received chunk: ,...

2025-04-02T02:20:28.126Z:
Received chunk:  and...

2025-04-02T02:20:28.213Z:
Received chunk:  maybe...

2025-04-02T02:20:28.300Z:
Received chunk:  something...

2025-04-02T02:20:28.390Z:
Received chunk:  else...

2025-04-02T02:20:28.481Z:
Received chunk:  like...

2025-04-02T02:20:28.572Z:
Received chunk:  double...

2025-04-02T02:20:28.659Z:
Received chunk:  chocolate...

2025-04-02T02:20:28.747Z:
Received chunk:  or...

2025-04-02T02:20:28.835Z:
Received chunk:  mac...

2025-04-02T02:20:28.921Z:
Received chunk: adam...

2025-04-02T02:20:29.008Z:
Received chunk: ia...

2025-04-02T02:20:29.094Z:
Received chunk:  nut...

2025-04-02T02:20:29.179Z:
Received chunk: ?...

2025-04-02T02:20:29.265Z:
Received chunk:  Wait...

2025-04-02T02:20:29.355Z:
Received chunk: ,...

2025-04-02T02:20:29.442Z:
Received chunk:  but...

2025-04-02T02:20:29.533Z:
Received chunk:  are...

2025-04-02T02:20:29.622Z:
Received chunk:  those...

2025-04-02T02:20:29.709Z:
Received chunk:  as...

2025-04-02T02:20:29.797Z:
Received chunk:  universally...

2025-04-02T02:20:29.884Z:
Received chunk:  loved...

2025-04-02T02:20:29.971Z:
Received chunk: ?

...

2025-04-02T02:20:30.058Z:
Received chunk: Alternatively...

2025-04-02T02:20:30.144Z:
Received chunk: ,...

2025-04-02T02:20:30.231Z:
Received chunk:  maybe...

2025-04-02T02:20:30.317Z:
Received chunk:  a...

2025-04-02T02:20:30.406Z:
Received chunk:  classic...

2025-04-02T02:20:30.495Z:
Received chunk:  like...

2025-04-02T02:20:30.588Z:
Received chunk:  but...

2025-04-02T02:20:30.674Z:
Received chunk: term...

2025-04-02T02:20:30.761Z:
Received chunk: ilk...

2025-04-02T02:20:30.847Z:
Received chunk:  pancakes...

2025-04-02T02:20:30.933Z:
Received chunk: —but...

2025-04-02T02:20:31.019Z:
Received chunk:  no...

2025-04-02T02:20:31.104Z:
Received chunk: ,...

2025-04-02T02:20:31.192Z:
Received chunk:  that...

2025-04-02T02:20:31.278Z:
Received chunk: 's...

2025-04-02T02:20:31.363Z:
Received chunk:  breakfast...

2025-04-02T02:20:31.452Z:
Received chunk: ....

2025-04-02T02:20:31.543Z:
Received chunk:  Maybe...

2025-04-02T02:20:31.635Z:
Received chunk:  coffee...

2025-04-02T02:20:31.721Z:
Received chunk:  cake...

2025-04-02T02:20:31.809Z:
Received chunk: ?...

2025-04-02T02:20:31.895Z:
Received chunk:  Hmm...

2025-04-02T02:20:31.981Z:
Received chunk: .

...

2025-04-02T02:20:32.069Z:
Received chunk: Wait...

2025-04-02T02:20:32.156Z:
Received chunk: ,...

2025-04-02T02:20:32.243Z:
Received chunk:  perhaps...

2025-04-02T02:20:32.329Z:
Received chunk:  the...

2025-04-02T02:20:32.417Z:
Received chunk:  user...

2025-04-02T02:20:32.509Z:
Received chunk:  just...

2025-04-02T02:20:32.601Z:
Received chunk:  wants...

2025-04-02T02:20:32.688Z:
Received chunk:  five...

2025-04-02T02:20:32.776Z:
Received chunk:  well...

2025-04-02T02:20:32.863Z:
Received chunk: -known...

2025-04-02T02:20:32.949Z:
Received chunk:  ones...

2025-04-02T02:20:33.037Z:
Received chunk: ....

2025-04-02T02:20:33.123Z:
Received chunk:  So...

2025-04-02T02:20:33.210Z:
Received chunk:  sticking...

2025-04-02T02:20:33.296Z:
Received chunk:  with...

2025-04-02T02:20:33.385Z:
Received chunk:  the...

2025-04-02T02:20:33.471Z:
Received chunk:  first...

2025-04-02T02:20:33.563Z:
Received chunk:  four...

2025-04-02T02:20:33.654Z:
Received chunk:  plus...

2025-04-02T02:20:33.741Z:
Received chunk:  another...

2025-04-02T02:20:33.829Z:
Received chunk:  solid...

2025-04-02T02:20:33.916Z:
Received chunk:  option...

2025-04-02T02:20:34.003Z:
Received chunk: ....

2025-04-02T02:20:34.089Z:
Received chunk:  Let...

2025-04-02T02:20:34.174Z:
Received chunk:  me...

2025-04-02T02:20:34.261Z:
Received chunk:  think...

2025-04-02T02:20:34.349Z:
Received chunk: —...

2025-04-02T02:20:34.438Z:
Received chunk: maybe...

2025-04-02T02:20:34.529Z:
Received chunk:  but...

2025-04-02T02:20:34.621Z:
Received chunk: term...

2025-04-02T02:20:34.708Z:
Received chunk: ilk...

2025-04-02T02:20:34.795Z:
Received chunk:  cookies...

2025-04-02T02:20:34.883Z:
Received chunk: ?...

2025-04-02T02:20:34.969Z:
Received chunk:  They...

2025-04-02T02:20:35.057Z:
Received chunk: 're...

2025-04-02T02:20:35.143Z:
Received chunk:  rich...

2025-04-02T02:20:35.229Z:
Received chunk:  and...

2025-04-02T02:20:35.315Z:
Received chunk:  have...

2025-04-02T02:20:35.408Z:
Received chunk:  that...

2025-04-02T02:20:35.491Z:
Received chunk:  butter...

2025-04-02T02:20:35.585Z:
Received chunk: y...

2025-04-02T02:20:35.675Z:
Received chunk:  flavor...

2025-04-02T02:20:35.763Z:
Received chunk: ....

2025-04-02T02:20:35.850Z:
Received chunk:  Or...

2025-04-02T02:20:35.936Z:
Received chunk:  maybe...

2025-04-02T02:20:36.023Z:
Received chunk:  something...

2025-04-02T02:20:36.110Z:
Received chunk:  like...

2025-04-02T02:20:36.197Z:
Received chunk:  chew...

2025-04-02T02:20:36.285Z:
Received chunk: y...

2025-04-02T02:20:36.371Z:
Received chunk:  oat...

2025-04-02T02:20:36.459Z:
Received chunk: meal...

2025-04-02T02:20:36.551Z:
Received chunk:  rais...

2025-04-02T02:20:36.641Z:
Received chunk: in...

2025-04-02T02:20:36.728Z:
Received chunk:  is...

2025-04-02T02:20:36.814Z:
Received chunk:  already...

2025-04-02T02:20:36.901Z:
Received chunk:  covered...

2025-04-02T02:20:36.987Z:
Received chunk: .

...

2025-04-02T02:20:37.073Z:
Received chunk: Alternatively...

2025-04-02T02:20:37.160Z:
Received chunk: ,...

2025-04-02T02:20:37.245Z:
Received chunk:  could...

2025-04-02T02:20:37.332Z:
Received chunk:  it...

2025-04-02T02:20:37.425Z:
Received chunk:  be...

2025-04-02T02:20:37.508Z:
Received chunk:  sn...

2025-04-02T02:20:37.601Z:
Received chunk: icker...

2025-04-02T02:20:37.689Z:
Received chunk: d...

2025-04-02T02:20:37.775Z:
Received chunk: oodles...

2025-04-02T02:20:37.863Z:
Received chunk: ,...

2025-04-02T02:20:37.951Z:
Received chunk:  sugar...

2025-04-02T02:20:38.038Z:
Received chunk:  cookies...

2025-04-02T02:20:38.124Z:
Received chunk: ,...

2025-04-02T02:20:38.211Z:
Received chunk:  chocolate...

2025-04-02T02:20:38.297Z:
Received chunk:  chip...

2025-04-02T02:20:38.384Z:
Received chunk: ,...

2025-04-02T02:20:38.472Z:
Received chunk:  oat...

2025-04-02T02:20:38.563Z:
Received chunk: meal...

2025-04-02T02:20:38.657Z:
Received chunk:  rais...

2025-04-02T02:20:38.746Z:
Received chunk: in...

2025-04-02T02:20:38.832Z:
Received chunk: ,...

2025-04-02T02:20:38.919Z:
Received chunk:  and...

2025-04-02T02:20:39.007Z:
Received chunk:  double...

2025-04-02T02:20:39.095Z:
Received chunk:  chocolate...

2025-04-02T02:20:39.181Z:
Received chunk:  chip...

2025-04-02T02:20:39.268Z:
Received chunk: ?...

2025-04-02T02:20:39.354Z:
Received chunk:  Yeah...

2025-04-02T02:20:39.446Z:
Received chunk: ,...

2025-04-02T02:20:39.530Z:
Received chunk:  those...

2025-04-02T02:20:39.622Z:
Received chunk:  sound...

2025-04-02T02:20:39.713Z:
Received chunk:  good...

2025-04-02T02:20:39.801Z:
Received chunk: .

...

2025-04-02T02:20:39.889Z:
Received chunk: But...

2025-04-02T02:20:39.976Z:
Received chunk:  I...

2025-04-02T02:20:40.064Z:
Received chunk: 'm...

2025-04-02T02:20:40.151Z:
Received chunk:  not...

2025-04-02T02:20:40.237Z:
Received chunk:  entirely...

2025-04-02T02:20:40.324Z:
Received chunk:  sure...

2025-04-02T02:20:40.410Z:
Received chunk:  if...

2025-04-02T02:20:40.507Z:
Received chunk:  they...

2025-04-02T02:20:40.600Z:
Received chunk:  are...

2025-04-02T02:20:40.693Z:
Received chunk:  all...

2025-04-02T02:20:40.781Z:
Received chunk:  as...

2025-04-02T02:20:40.868Z:
Received chunk:  universally...

2025-04-02T02:20:40.956Z:
Received chunk:  loved...

2025-04-02T02:20:41.043Z:
Received chunk:  as...

2025-04-02T02:20:41.128Z:
Received chunk:  the...

2025-04-02T02:20:41.215Z:
Received chunk:  first...

2025-04-02T02:20:41.301Z:
Received chunk:  two...

2025-04-02T02:20:41.389Z:
Received chunk:  options...

2025-04-02T02:20:41.476Z:
Received chunk: ....

2025-04-02T02:20:41.564Z:
Received chunk:  Maybe...

2025-04-02T02:20:41.657Z:
Received chunk:  stick...

2025-04-02T02:20:41.748Z:
Received chunk:  with...

2025-04-02T02:20:41.835Z:
Received chunk:  the...

2025-04-02T02:20:41.921Z:
Received chunk:  more...

2025-04-02T02:20:42.009Z:
Received chunk:  traditional...

2025-04-02T02:20:42.099Z:
Received chunk:  ones...

2025-04-02T02:20:42.185Z:
Received chunk:  first...

2025-04-02T02:20:42.275Z:
Received chunk:  to...

2025-04-02T02:20:42.364Z:
Received chunk:  ensure...

2025-04-02T02:20:42.460Z:
Received chunk:  popularity...

2025-04-02T02:20:42.544Z:
Received chunk: .
...

2025-04-02T02:20:42.640Z:
Received chunk: </think>...

2025-04-02T02:20:42.733Z:
Received chunk: 

...

2025-04-02T02:20:42.823Z:
Received chunk: Here...

2025-04-02T02:20:42.911Z:
Received chunk:  is...

2025-04-02T02:20:42.999Z:
Received chunk:  a...

2025-04-02T02:20:43.089Z:
Received chunk:  curated...

2025-04-02T02:20:43.176Z:
Received chunk:  list...

2025-04-02T02:20:43.262Z:
Received chunk:  of...

2025-04-02T02:20:43.349Z:
Received chunk:  five...

2025-04-02T02:20:43.437Z:
Received chunk:  universally...

2025-04-02T02:20:43.526Z:
Received chunk:  loved...

2025-04-02T02:20:43.617Z:
Received chunk:  cookie...

2025-04-02T02:20:43.711Z:
Received chunk:  recipes...

2025-04-02T02:20:43.797Z:
Received chunk: ,...

2025-04-02T02:20:43.883Z:
Received chunk:  based...

2025-04-02T02:20:43.970Z:
Received chunk:  on...

2025-04-02T02:20:44.057Z:
Received chunk:  their...

2025-04-02T02:20:44.145Z:
Received chunk:  popularity...

2025-04-02T02:20:44.231Z:
Received chunk:  and...

2025-04-02T02:20:44.318Z:
Received chunk:  widespread...

2025-04-02T02:20:44.404Z:
Received chunk:  acceptance...

2025-04-02T02:20:44.491Z:
Received chunk: :

...

2025-04-02T02:20:44.581Z:
Received chunk: 1...

2025-04-02T02:20:44.673Z:
Received chunk: ....

2025-04-02T02:20:44.765Z:
Received chunk:  **...

2025-04-02T02:20:44.852Z:
Received chunk: Classic...

2025-04-02T02:20:44.939Z:
Received chunk:  Chocolate...

2025-04-02T02:20:45.026Z:
Received chunk:  Chip...

2025-04-02T02:20:45.115Z:
Received chunk:  Cookies...

2025-04-02T02:20:45.202Z:
Received chunk: **...

2025-04-02T02:20:45.289Z:
Received chunk:   
...

2025-04-02T02:20:45.375Z:
Received chunk:   ...

2025-04-02T02:20:45.463Z:
Received chunk:  -...

2025-04-02T02:20:45.554Z:
Received chunk:  A...

2025-04-02T02:20:45.646Z:
Received chunk:  staple...

2025-04-02T02:20:45.739Z:
Received chunk:  in...

2025-04-02T02:20:45.827Z:
Received chunk:  every...

2025-04-02T02:20:45.914Z:
Received chunk:  baker...

2025-04-02T02:20:46.000Z:
Received chunk: 's...

2025-04-02T02:20:46.088Z:
Received chunk:  repertoire...

2025-04-02T02:20:46.175Z:
Received chunk: ,...

2025-04-02T02:20:46.261Z:
Received chunk:  these...

2025-04-02T02:20:46.349Z:
Received chunk:  cookies...

2025-04-02T02:20:46.436Z:
Received chunk:  are...

2025-04-02T02:20:46.524Z:
Received chunk:  beloved...

2025-04-02T02:20:46.619Z:
Received chunk:  for...

2025-04-02T02:20:46.707Z:
Received chunk:  their...

2025-04-02T02:20:46.797Z:
Received chunk:  simple...

2025-04-02T02:20:46.884Z:
Received chunk:  yet...

2025-04-02T02:20:46.971Z:
Received chunk:  rich...

2025-04-02T02:20:47.057Z:
Received chunk:  flavors...

2025-04-02T02:20:47.145Z:
Received chunk: .

...

2025-04-02T02:20:47.231Z:
Received chunk: 2...

2025-04-02T02:20:47.318Z:
Received chunk: ....

2025-04-02T02:20:47.406Z:
Received chunk:  **...

2025-04-02T02:20:47.499Z:
Received chunk: O...

2025-04-02T02:20:47.582Z:
Received chunk: at...

2025-04-02T02:20:47.673Z:
Received chunk: meal...

2025-04-02T02:20:47.765Z:
Received chunk:  R...

2025-04-02T02:20:47.851Z:
Received chunk: ais...

2025-04-02T02:20:47.938Z:
Received chunk: in...

2025-04-02T02:20:48.025Z:
Received chunk:  Cookies...

2025-04-02T02:20:48.112Z:
Received chunk: **...

2025-04-02T02:20:48.201Z:
Received chunk:   
...

2025-04-02T02:20:48.288Z:
Received chunk:   ...

2025-04-02T02:20:48.374Z:
Received chunk:  -...

2025-04-02T02:20:48.461Z:
Received chunk:  Known...

2025-04-02T02:20:48.550Z:
Received chunk:  for...

2025-04-02T02:20:48.640Z:
Received chunk:  their...

2025-04-02T02:20:48.732Z:
Received chunk:  chew...

2025-04-02T02:20:48.821Z:
Received chunk: y...

2025-04-02T02:20:48.908Z:
Received chunk:  texture...

2025-04-02T02:20:48.996Z:
Received chunk:  and...

2025-04-02T02:20:49.083Z:
Received chunk:  aromatic...

2025-04-02T02:20:49.170Z:
Received chunk:  combination...

2025-04-02T02:20:49.258Z:
Received chunk:  of...

2025-04-02T02:20:49.345Z:
Received chunk:  oats...

2025-04-02T02:20:49.432Z:
Received chunk:  and...

2025-04-02T02:20:49.519Z:
Received chunk:  rais...

2025-04-02T02:20:49.610Z:
Received chunk: ins...

2025-04-02T02:20:49.703Z:
Received chunk: ,...

2025-04-02T02:20:49.795Z:
Received chunk:  these...

2025-04-02T02:20:49.884Z:
Received chunk:  are...

2025-04-02T02:20:49.971Z:
Received chunk:  a...

2025-04-02T02:20:50.058Z:
Received chunk:  crowd...

2025-04-02T02:20:50.146Z:
Received chunk:  favorite...

2025-04-02T02:20:50.235Z:
Received chunk: .

...

2025-04-02T02:20:50.321Z:
Received chunk: 3...

2025-04-02T02:20:50.408Z:
Received chunk: ....

2025-04-02T02:20:50.495Z:
Received chunk:  **...

2025-04-02T02:20:50.585Z:
Received chunk: Sn...

2025-04-02T02:20:50.677Z:
Received chunk: icker...

2025-04-02T02:20:50.768Z:
Received chunk: d...

2025-04-02T02:20:50.855Z:
Received chunk: oodle...

2025-04-02T02:20:50.942Z:
Received chunk:  Cookies...

2025-04-02T02:20:51.028Z:
Received chunk: **...

2025-04-02T02:20:51.115Z:
Received chunk:   
...

2025-04-02T02:20:51.203Z:
Received chunk:   ...

2025-04-02T02:20:51.290Z:
Received chunk:  -...

2025-04-02T02:20:51.377Z:
Received chunk:  Featuring...

2025-04-02T02:20:51.463Z:
Received chunk:  that...

2025-04-02T02:20:51.551Z:
Received chunk:  iconic...

2025-04-02T02:20:51.643Z:
Received chunk:  crack...

2025-04-02T02:20:51.734Z:
Received chunk: ly...

2025-04-02T02:20:51.823Z:
Received chunk:  exterior...

2025-04-02T02:20:51.910Z:
Received chunk:  with...

2025-04-02T02:20:51.997Z:
Received chunk:  a...

2025-04-02T02:20:52.085Z:
Received chunk:  tender...

2025-04-02T02:20:52.171Z:
Received chunk:  interior...

2025-04-02T02:20:52.260Z:
Received chunk: ,...

2025-04-02T02:20:52.347Z:
Received chunk:  these...

2025-04-02T02:20:52.433Z:
Received chunk:  cookies...

2025-04-02T02:20:52.529Z:
Received chunk:  are...

2025-04-02T02:20:52.612Z:
Received chunk:  beloved...

2025-04-02T02:20:52.704Z:
Received chunk:  by...

2025-04-02T02:20:52.796Z:
Received chunk:  many...

2025-04-02T02:20:52.883Z:
Received chunk: .

...

2025-04-02T02:20:52.971Z:
Received chunk: 4...

2025-04-02T02:20:53.060Z:
Received chunk: ....

2025-04-02T02:20:53.147Z:
Received chunk:  **...

2025-04-02T02:20:53.234Z:
Received chunk: Sugar...

2025-04-02T02:20:53.322Z:
Received chunk:  Cookies...

2025-04-02T02:20:53.409Z:
Received chunk: **...

2025-04-02T02:20:53.496Z:
Received chunk:   
...

2025-04-02T02:20:53.585Z:
Received chunk:   ...

2025-04-02T02:20:53.676Z:
Received chunk:  -...

2025-04-02T02:20:53.770Z:
Received chunk:  Perfect...

2025-04-02T02:20:53.859Z:
Received chunk: ly...

2025-04-02T02:20:53.946Z:
Received chunk:  suited...

2025-04-02T02:20:54.033Z:
Received chunk:  for...

2025-04-02T02:20:54.120Z:
Received chunk:  festive...

2025-04-02T02:20:54.207Z:
Received chunk:  occasions...

2025-04-02T02:20:54.295Z:
Received chunk: ,...

2025-04-02T02:20:54.381Z:
Received chunk:  these...

2025-04-02T02:20:54.469Z:
Received chunk:  cookies...

2025-04-02T02:20:54.557Z:
Received chunk:  are...

2025-04-02T02:20:54.648Z:
Received chunk:  tre...

2025-04-02T02:20:54.740Z:
Received chunk: asured...

2025-04-02T02:20:54.830Z:
Received chunk:  during...

2025-04-02T02:20:54.918Z:
Received chunk:  the...

2025-04-02T02:20:55.005Z:
Received chunk:  holiday...

2025-04-02T02:20:55.092Z:
Received chunk:  season...

2025-04-02T02:20:55.179Z:
Received chunk: .

...

2025-04-02T02:20:55.268Z:
Received chunk: 5...

2025-04-02T02:20:55.356Z:
Received chunk: ....

2025-04-02T02:20:55.443Z:
Received chunk:  **...

2025-04-02T02:20:55.532Z:
Received chunk: Double...

2025-04-02T02:20:55.622Z:
Received chunk:  Chocolate...

2025-04-02T02:20:55.713Z:
Received chunk:  Chip...

2025-04-02T02:20:55.805Z:
Received chunk:  Cookies...

2025-04-02T02:20:55.892Z:
Received chunk: **...

2025-04-02T02:20:55.979Z:
Received chunk:   
...

2025-04-02T02:20:56.066Z:
Received chunk:   ...

2025-04-02T02:20:56.154Z:
Received chunk:  -...

2025-04-02T02:20:56.241Z:
Received chunk:  Rich...

2025-04-02T02:20:56.329Z:
Received chunk:  and...

2025-04-02T02:20:56.416Z:
Received chunk:  flavorful...

2025-04-02T02:20:56.503Z:
Received chunk:  with...

2025-04-02T02:20:56.591Z:
Received chunk:  a...

2025-04-02T02:20:56.682Z:
Received chunk:  hint...

2025-04-02T02:20:56.775Z:
Received chunk:  of...

2025-04-02T02:20:56.862Z:
Received chunk:  dark...

2025-04-02T02:20:56.949Z:
Received chunk:  chocolate...

2025-04-02T02:20:57.037Z:
Received chunk: ,...

2025-04-02T02:20:57.125Z:
Received chunk:  these...

2025-04-02T02:20:57.212Z:
Received chunk:  cookies...

2025-04-02T02:20:57.299Z:
Received chunk:  offer...

2025-04-02T02:20:57.387Z:
Received chunk:  a...

2025-04-02T02:20:57.474Z:
Received chunk:  satisfying...

2025-04-02T02:20:57.561Z:
Received chunk:  texture...

2025-04-02T02:20:57.651Z:
Received chunk: .

...

2025-04-02T02:20:57.746Z:
Received chunk: These...

2025-04-02T02:20:57.841Z:
Received chunk:  selections...

2025-04-02T02:20:57.929Z:
Received chunk:  ensure...

2025-04-02T02:20:58.017Z:
Received chunk:  a...

2025-04-02T02:20:58.106Z:
Received chunk:  variety...

2025-04-02T02:20:58.193Z:
Received chunk:  of...

2025-04-02T02:20:58.280Z:
Received chunk:  tastes...

2025-04-02T02:20:58.368Z:
Received chunk:  while...

2025-04-02T02:20:58.455Z:
Received chunk:  maintaining...

2025-04-02T02:20:58.542Z:
Received chunk:  their...

2025-04-02T02:20:58.631Z:
Received chunk:  universal...

2025-04-02T02:20:58.724Z:
Received chunk:  appeal...

2025-04-02T02:20:58.817Z:
Received chunk: ....

2025-04-02T02:20:58.904Z:
Received chunk: 

> ...

2025-04-02T02:23:29.957Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:23:29.957Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:23:29.957Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:23:29.958Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:23:30.992Z:
Unloading model...

2025-04-02T02:23:30.993Z:
Model unloaded successfully

2025-04-02T02:23:30.993Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:23:30.993Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:23:30.993Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:23:30.994Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:23:46.487Z:
Generate response called with message: give me the shortest list of cookies you can...

2025-04-02T02:23:46.488Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me the shortest list of cookies you can
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:23:46.513Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:23:46.537Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:23:46.559Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:23:46.568Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:23:46.591Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:23:46.643Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:23:46.644Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:23:46.661Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06


2025-04-02T02:23:46.661Z:
Process stderr: print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:23:46.886Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:23:46.909Z:
Process stderr: .

2025-04-02T02:23:46.928Z:
Process stderr: .

2025-04-02T02:23:46.948Z:
Process stderr: .

2025-04-02T02:23:46.967Z:
Process stderr: .

2025-04-02T02:23:46.986Z:
Process stderr: .

2025-04-02T02:23:47.005Z:
Process stderr: .

2025-04-02T02:23:47.026Z:
Process stderr: .

2025-04-02T02:23:47.058Z:
Process stderr: .

2025-04-02T02:23:47.065Z:
Process stderr: .

2025-04-02T02:23:47.097Z:
Process stderr: .

2025-04-02T02:23:47.114Z:
Process stderr: .

2025-04-02T02:23:47.137Z:
Process stderr: .

2025-04-02T02:23:47.153Z:
Process stderr: .

2025-04-02T02:23:47.177Z:
Process stderr: .

2025-04-02T02:23:47.192Z:
Process stderr: .

2025-04-02T02:23:47.208Z:
Process stderr: .

2025-04-02T02:23:47.232Z:
Process stderr: .

2025-04-02T02:23:47.248Z:
Process stderr: .

2025-04-02T02:23:47.264Z:
Process stderr: .

2025-04-02T02:23:47.287Z:
Process stderr: .

2025-04-02T02:23:47.308Z:
Process stderr: .

2025-04-02T02:23:47.327Z:
Process stderr: .

2025-04-02T02:23:47.342Z:
Process stderr: .

2025-04-02T02:23:47.363Z:
Process stderr: .

2025-04-02T02:23:47.384Z:
Process stderr: .

2025-04-02T02:23:47.416Z:
Process stderr: .

2025-04-02T02:23:47.443Z:
Process stderr: .

2025-04-02T02:23:47.459Z:
Process stderr: .

2025-04-02T02:23:47.482Z:
Process stderr: .

2025-04-02T02:23:47.498Z:
Process stderr: .

2025-04-02T02:23:47.514Z:
Process stderr: .

2025-04-02T02:23:47.538Z:
Process stderr: .

2025-04-02T02:23:47.553Z:
Process stderr: .

2025-04-02T02:23:47.570Z:
Process stderr: .

2025-04-02T02:23:47.593Z:
Process stderr: .

2025-04-02T02:23:47.609Z:
Process stderr: .

2025-04-02T02:23:47.633Z:
Process stderr: .

2025-04-02T02:23:47.648Z:
Process stderr: .

2025-04-02T02:23:47.667Z:
Process stderr: .

2025-04-02T02:23:47.688Z:
Process stderr: .

2025-04-02T02:23:47.719Z:
Process stderr: .

2025-04-02T02:23:47.726Z:
Process stderr: .

2025-04-02T02:23:47.758Z:
Process stderr: .

2025-04-02T02:23:47.766Z:
Process stderr: .

2025-04-02T02:23:47.798Z:
Process stderr: .

2025-04-02T02:23:47.815Z:
Process stderr: .

2025-04-02T02:23:47.839Z:
Process stderr: .

2025-04-02T02:23:47.854Z:
Process stderr: .

2025-04-02T02:23:47.871Z:
Process stderr: .

2025-04-02T02:23:47.896Z:
Process stderr: .

2025-04-02T02:23:47.914Z:
Process stderr: .

2025-04-02T02:23:47.940Z:
Process stderr: .

2025-04-02T02:23:47.957Z:
Process stderr: .

2025-04-02T02:23:47.976Z:
Process stderr: .

2025-04-02T02:23:48.001Z:
Process stderr: .

2025-04-02T02:23:48.018Z:
Process stderr: .

2025-04-02T02:23:48.040Z:
Process stderr: .

2025-04-02T02:23:48.062Z:
Process stderr: .

2025-04-02T02:23:48.084Z:
Process stderr: .

2025-04-02T02:23:48.106Z:
Process stderr: .

2025-04-02T02:23:48.129Z:
Process stderr: .

2025-04-02T02:23:48.170Z:
Process stderr: .

2025-04-02T02:23:48.178Z:
Process stderr: .

2025-04-02T02:23:48.218Z:
Process stderr: .

2025-04-02T02:23:48.227Z:
Process stderr: .

2025-04-02T02:23:48.268Z:
Process stderr: ...

2025-04-02T02:23:48.268Z:
Process stderr: ........

2025-04-02T02:23:48.268Z:
Process stderr: .......


2025-04-02T02:23:48.271Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:23:48.271Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:23:48.291Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:23:48.297Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:23:48.588Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T02:23:48.589Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:23:48.589Z:
Process stderr: sampler seed: 2708077570
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:23:48.869Z:
Received chunk: <think>...

2025-04-02T02:23:48.963Z:
Received chunk: 
...

2025-04-02T02:23:49.057Z:
Received chunk: Okay...

2025-04-02T02:23:49.148Z:
Received chunk: ,...

2025-04-02T02:23:49.241Z:
Received chunk:  so...

2025-04-02T02:23:49.335Z:
Received chunk:  I...

2025-04-02T02:23:49.442Z:
Received chunk: 'm...

2025-04-02T02:23:49.554Z:
Received chunk:  trying...

2025-04-02T02:23:49.650Z:
Received chunk:  to...

2025-04-02T02:23:49.751Z:
Received chunk:  figure...

2025-04-02T02:23:49.851Z:
Received chunk:  out...

2025-04-02T02:23:49.949Z:
Received chunk:  what...

2025-04-02T02:23:50.047Z:
Received chunk:  the...

2025-04-02T02:23:50.145Z:
Received chunk:  user...

2025-04-02T02:23:50.244Z:
Received chunk:  is...

2025-04-02T02:23:50.344Z:
Received chunk:  asking...

2025-04-02T02:23:50.446Z:
Received chunk:  for...

2025-04-02T02:23:50.539Z:
Received chunk:  here...

2025-04-02T02:23:50.625Z:
Received chunk: ....

2025-04-02T02:23:50.711Z:
Received chunk:  They...

2025-04-02T02:23:50.796Z:
Received chunk:  wrote...

2025-04-02T02:23:50.882Z:
Received chunk:  "...

2025-04-02T02:23:50.967Z:
Received chunk: give...

2025-04-02T02:23:51.053Z:
Received chunk:  me...

2025-04-02T02:23:51.139Z:
Received chunk:  the...

2025-04-02T02:23:51.224Z:
Received chunk:  shortest...

2025-04-02T02:23:51.309Z:
Received chunk:  list...

2025-04-02T02:23:51.398Z:
Received chunk:  of...

2025-04-02T02:23:51.487Z:
Received chunk:  cookies...

2025-04-02T02:23:51.575Z:
Received chunk:  you...

2025-04-02T02:23:51.661Z:
Received chunk:  can...

2025-04-02T02:23:51.747Z:
Received chunk: ."...

2025-04-02T02:23:51.832Z:
Received chunk:  Hmm...

2025-04-02T02:23:51.918Z:
Received chunk: ,...

2025-04-02T02:23:52.004Z:
Received chunk:  that...

2025-04-02T02:23:52.089Z:
Received chunk: 's...

2025-04-02T02:23:52.175Z:
Received chunk:  a...

2025-04-02T02:23:52.260Z:
Received chunk:  bit...

2025-04-02T02:23:52.353Z:
Received chunk:  vague...

2025-04-02T02:23:52.436Z:
Received chunk: ....

2025-04-02T02:23:52.526Z:
Received chunk:  Maybe...

2025-04-02T02:23:52.614Z:
Received chunk:  they...

2025-04-02T02:23:52.702Z:
Received chunk:  want...

2025-04-02T02:23:52.788Z:
Received chunk:  a...

2025-04-02T02:23:52.873Z:
Received chunk:  list...

2025-04-02T02:23:52.958Z:
Received chunk:  of...

2025-04-02T02:23:53.043Z:
Received chunk:  cookie...

2025-04-02T02:23:53.129Z:
Received chunk:  types...

2025-04-02T02:23:53.214Z:
Received chunk:  but...

2025-04-02T02:23:53.299Z:
Received chunk:  it...

2025-04-02T02:23:53.385Z:
Received chunk:  needs...

2025-04-02T02:23:53.474Z:
Received chunk:  to...

2025-04-02T02:23:53.565Z:
Received chunk:  be...

2025-04-02T02:23:53.653Z:
Received chunk:  as...

2025-04-02T02:23:53.746Z:
Received chunk:  short...

2025-04-02T02:23:53.832Z:
Received chunk:  as...

2025-04-02T02:23:53.918Z:
Received chunk:  possible...

2025-04-02T02:23:54.004Z:
Received chunk: ?...

2025-04-02T02:23:54.090Z:
Received chunk:  

...

2025-04-02T02:23:54.175Z:
Received chunk: First...

2025-04-02T02:23:54.261Z:
Received chunk: ,...

2025-04-02T02:23:54.356Z:
Received chunk:  I...

2025-04-02T02:23:54.453Z:
Received chunk:  should...

2025-04-02T02:23:54.547Z:
Received chunk:  think...

2025-04-02T02:23:54.636Z:
Received chunk:  about...

2025-04-02T02:23:54.724Z:
Received chunk:  common...

2025-04-02T02:23:54.810Z:
Received chunk:  cookies...

2025-04-02T02:23:54.897Z:
Received chunk:  people...

2025-04-02T02:23:54.983Z:
Received chunk:  know...

2025-04-02T02:23:55.068Z:
Received chunk: ....

2025-04-02T02:23:55.154Z:
Received chunk:  There...

2025-04-02T02:23:55.239Z:
Received chunk: 's...

2025-04-02T02:23:55.324Z:
Received chunk:  chocolate...

2025-04-02T02:23:55.413Z:
Received chunk:  chip...

2025-04-02T02:23:55.504Z:
Received chunk: ,...

2025-04-02T02:23:55.595Z:
Received chunk:  oat...

2025-04-02T02:23:55.681Z:
Received chunk: meal...

2025-04-02T02:23:55.768Z:
Received chunk:  rais...

2025-04-02T02:23:55.853Z:
Received chunk: in...

2025-04-02T02:23:55.939Z:
Received chunk: ,...

2025-04-02T02:23:56.025Z:
Received chunk:  maybe...

2025-04-02T02:23:56.111Z:
Received chunk:  some...

2025-04-02T02:23:56.197Z:
Received chunk:  classic...

2025-04-02T02:23:56.283Z:
Received chunk:  ones...

2025-04-02T02:23:56.369Z:
Received chunk:  like...

2025-04-02T02:23:56.456Z:
Received chunk:  sn...

2025-04-02T02:23:56.550Z:
Received chunk: icker...

2025-04-02T02:23:56.640Z:
Received chunk: d...

2025-04-02T02:23:56.727Z:
Received chunk: oodles...

2025-04-02T02:23:56.815Z:
Received chunk:  or...

2025-04-02T02:23:56.902Z:
Received chunk:  mac...

2025-04-02T02:23:56.989Z:
Received chunk: adam...

2025-04-02T02:23:57.076Z:
Received chunk: ia...

2025-04-02T02:23:57.163Z:
Received chunk:  nut...

2025-04-02T02:23:57.249Z:
Received chunk:  cookies...

2025-04-02T02:23:57.336Z:
Received chunk: ....

2025-04-02T02:23:57.423Z:
Received chunk:  But...

2025-04-02T02:23:57.513Z:
Received chunk:  wait...

2025-04-02T02:23:57.608Z:
Received chunk: ,...

2025-04-02T02:23:57.695Z:
Received chunk:  the...

2025-04-02T02:23:57.782Z:
Received chunk:  user...

2025-04-02T02:23:57.868Z:
Received chunk:  said...

2025-04-02T02:23:57.954Z:
Received chunk:  "...

2025-04-02T02:23:58.038Z:
Received chunk: the...

2025-04-02T02:23:58.124Z:
Received chunk:  shortest...

2025-04-02T02:23:58.210Z:
Received chunk:  list...

2025-04-02T02:23:58.296Z:
Received chunk: ,"...

2025-04-02T02:23:58.381Z:
Received chunk:  so...

2025-04-02T02:23:58.468Z:
Received chunk:  perhaps...

2025-04-02T02:23:58.560Z:
Received chunk:  they...

2025-04-02T02:23:58.651Z:
Received chunk: 're...

2025-04-02T02:23:58.739Z:
Received chunk:  looking...

2025-04-02T02:23:58.825Z:
Received chunk:  for...

2025-04-02T02:23:58.910Z:
Received chunk:  something...

2025-04-02T02:23:58.995Z:
Received chunk:  minimal...

2025-04-02T02:23:59.082Z:
Received chunk:  but...

2025-04-02T02:23:59.168Z:
Received chunk:  still...

2025-04-02T02:23:59.254Z:
Received chunk:  covers...

2025-04-02T02:23:59.341Z:
Received chunk:  a...

2025-04-02T02:23:59.430Z:
Received chunk:  variety...

2025-04-02T02:23:59.519Z:
Received chunk: .

...

2025-04-02T02:23:59.612Z:
Received chunk: I...

2025-04-02T02:23:59.699Z:
Received chunk:  wonder...

2025-04-02T02:23:59.784Z:
Received chunk:  if...

2025-04-02T02:23:59.871Z:
Received chunk:  they...

2025-04-02T02:23:59.957Z:
Received chunk:  want...

2025-04-02T02:24:00.045Z:
Received chunk:  a...

2025-04-02T02:24:00.131Z:
Received chunk:  list...

2025-04-02T02:24:00.218Z:
Received chunk:  of...

2025-04-02T02:24:00.304Z:
Received chunk:  cookie...

2025-04-02T02:24:00.390Z:
Received chunk:  types...

2025-04-02T02:24:00.478Z:
Received chunk:  that...

2025-04-02T02:24:00.571Z:
Received chunk:  are...

2025-04-02T02:24:00.664Z:
Received chunk:  easy...

2025-04-02T02:24:00.751Z:
Received chunk:  to...

2025-04-02T02:24:00.836Z:
Received chunk:  make...

2025-04-02T02:24:00.923Z:
Received chunk:  or...

2025-04-02T02:24:01.009Z:
Received chunk:  have...

2025-04-02T02:24:01.098Z:
Received chunk:  different...

2025-04-02T02:24:01.187Z:
Received chunk:  flavors...

2025-04-02T02:24:01.281Z:
Received chunk: ....

2025-04-02T02:24:01.373Z:
Received chunk:  Maybe...

2025-04-02T02:24:01.468Z:
Received chunk:  including...

2025-04-02T02:24:01.562Z:
Received chunk:  a...

2025-04-02T02:24:01.654Z:
Received chunk:  mix...

2025-04-02T02:24:01.741Z:
Received chunk:  of...

2025-04-02T02:24:01.827Z:
Received chunk:  sweet...

2025-04-02T02:24:01.916Z:
Received chunk:  and...

2025-04-02T02:24:02.005Z:
Received chunk:  maybe...

2025-04-02T02:24:02.093Z:
Received chunk:  some...

2025-04-02T02:24:02.181Z:
Received chunk:  with...

2025-04-02T02:24:02.270Z:
Received chunk:  nuts...

2025-04-02T02:24:02.358Z:
Received chunk: ?...

2025-04-02T02:24:02.449Z:
Received chunk:  Like...

2025-04-02T02:24:02.540Z:
Received chunk:  chocolate...

2025-04-02T02:24:02.634Z:
Received chunk:  chip...

2025-04-02T02:24:02.725Z:
Received chunk:  is...

2025-04-02T02:24:02.814Z:
Received chunk:  pretty...

2025-04-02T02:24:02.904Z:
Received chunk:  standard...

2025-04-02T02:24:02.994Z:
Received chunk: ,...

2025-04-02T02:24:03.082Z:
Received chunk:  then...

2025-04-02T02:24:03.170Z:
Received chunk:  add...

2025-04-02T02:24:03.258Z:
Received chunk:  something...

2025-04-02T02:24:03.345Z:
Received chunk:  like...

2025-04-02T02:24:03.432Z:
Received chunk:  oat...

2025-04-02T02:24:03.520Z:
Received chunk: meal...

2025-04-02T02:24:03.610Z:
Received chunk:  rais...

2025-04-02T02:24:03.701Z:
Received chunk: in...

2025-04-02T02:24:03.787Z:
Received chunk:  for...

2025-04-02T02:24:03.877Z:
Received chunk:  variation...

2025-04-02T02:24:03.968Z:
Received chunk: ....

2025-04-02T02:24:04.059Z:
Received chunk:  Mac...

2025-04-02T02:24:04.148Z:
Received chunk: adam...

2025-04-02T02:24:04.234Z:
Received chunk: ia...

2025-04-02T02:24:04.321Z:
Received chunk:  nut...

2025-04-02T02:24:04.408Z:
Received chunk:  cookies...

2025-04-02T02:24:04.497Z:
Received chunk:  sound...

2025-04-02T02:24:04.588Z:
Received chunk:  unique...

2025-04-02T02:24:04.678Z:
Received chunk:  and...

2025-04-02T02:24:04.767Z:
Received chunk:  not...

2025-04-02T02:24:04.854Z:
Received chunk:  too...

2025-04-02T02:24:04.940Z:
Received chunk:  common...

2025-04-02T02:24:05.026Z:
Received chunk:  in...

2025-04-02T02:24:05.112Z:
Received chunk:  everyone...

2025-04-02T02:24:05.199Z:
Received chunk: 's...

2025-04-02T02:24:05.285Z:
Received chunk:  kitchen...

2025-04-02T02:24:05.371Z:
Received chunk: .

...

2025-04-02T02:24:05.458Z:
Received chunk: So...

2025-04-02T02:24:05.546Z:
Received chunk:  putting...

2025-04-02T02:24:05.637Z:
Received chunk:  it...

2025-04-02T02:24:05.726Z:
Received chunk:  together...

2025-04-02T02:24:05.813Z:
Received chunk: :...

2025-04-02T02:24:05.900Z:
Received chunk:  chocolate...

2025-04-02T02:24:05.987Z:
Received chunk:  chip...

2025-04-02T02:24:06.072Z:
Received chunk: ,...

2025-04-02T02:24:06.158Z:
Received chunk:  oat...

2025-04-02T02:24:06.245Z:
Received chunk: meal...

2025-04-02T02:24:06.331Z:
Received chunk:  rais...

2025-04-02T02:24:06.418Z:
Received chunk: in...

2025-04-02T02:24:06.507Z:
Received chunk: ,...

2025-04-02T02:24:06.598Z:
Received chunk:  and...

2025-04-02T02:24:06.690Z:
Received chunk:  mac...

2025-04-02T02:24:06.777Z:
Received chunk: adam...

2025-04-02T02:24:06.865Z:
Received chunk: ia...

2025-04-02T02:24:06.951Z:
Received chunk:  nut...

2025-04-02T02:24:07.037Z:
Received chunk:  cookies...

2025-04-02T02:24:07.124Z:
Received chunk: ....

2025-04-02T02:24:07.209Z:
Received chunk:  That...

2025-04-02T02:24:07.295Z:
Received chunk:  makes...

2025-04-02T02:24:07.382Z:
Received chunk:  three...

2025-04-02T02:24:07.469Z:
Received chunk:  different...

2025-04-02T02:24:07.563Z:
Received chunk:  types...

2025-04-02T02:24:07.647Z:
Received chunk: ,...

2025-04-02T02:24:07.734Z:
Received chunk:  which...

2025-04-02T02:24:07.821Z:
Received chunk:  I...

2025-04-02T02:24:07.907Z:
Received chunk:  think...

2025-04-02T02:24:07.992Z:
Received chunk:  is...

2025-04-02T02:24:08.078Z:
Received chunk:  a...

2025-04-02T02:24:08.163Z:
Received chunk:  concise...

2025-04-02T02:24:08.249Z:
Received chunk:  list...

2025-04-02T02:24:08.335Z:
Received chunk:  that...

2025-04-02T02:24:08.422Z:
Received chunk:  offers...

2025-04-02T02:24:08.510Z:
Received chunk:  variety...

2025-04-02T02:24:08.602Z:
Received chunk:  without...

2025-04-02T02:24:08.699Z:
Received chunk:  being...

2025-04-02T02:24:08.788Z:
Received chunk:  too...

2025-04-02T02:24:08.878Z:
Received chunk:  lengthy...

2025-04-02T02:24:08.967Z:
Received chunk: .
...

2025-04-02T02:24:09.057Z:
Received chunk: </think>...

2025-04-02T02:24:09.145Z:
Received chunk: 

...

2025-04-02T02:24:09.235Z:
Received chunk: Here...

2025-04-02T02:24:09.323Z:
Received chunk: 's...

2025-04-02T02:24:09.413Z:
Received chunk:  a...

2025-04-02T02:24:09.503Z:
Received chunk:  short...

2025-04-02T02:24:09.594Z:
Received chunk:  list...

2025-04-02T02:24:09.691Z:
Received chunk:  of...

2025-04-02T02:24:09.780Z:
Received chunk:  cookie...

2025-04-02T02:24:09.871Z:
Received chunk:  varieties...

2025-04-02T02:24:09.962Z:
Received chunk: :

...

2025-04-02T02:24:10.052Z:
Received chunk: 1...

2025-04-02T02:24:10.141Z:
Received chunk: ....

2025-04-02T02:24:10.230Z:
Received chunk:  Chocolate...

2025-04-02T02:24:10.319Z:
Received chunk:  Chip...

2025-04-02T02:24:10.414Z:
Received chunk: 
...

2025-04-02T02:24:10.507Z:
Received chunk: 2...

2025-04-02T02:24:10.598Z:
Received chunk: ....

2025-04-02T02:24:10.693Z:
Received chunk:  O...

2025-04-02T02:24:10.782Z:
Received chunk: at...

2025-04-02T02:24:10.873Z:
Received chunk: meal...

2025-04-02T02:24:10.966Z:
Received chunk:  R...

2025-04-02T02:24:11.063Z:
Received chunk: ais...

2025-04-02T02:24:11.149Z:
Received chunk: in...

2025-04-02T02:24:11.235Z:
Received chunk: 
...

2025-04-02T02:24:11.322Z:
Received chunk: 3...

2025-04-02T02:24:11.408Z:
Received chunk: ....

2025-04-02T02:24:11.497Z:
Received chunk:  Mac...

2025-04-02T02:24:11.585Z:
Received chunk: adam...

2025-04-02T02:24:11.677Z:
Received chunk: ia...

2025-04-02T02:24:11.765Z:
Received chunk:  Nut...

2025-04-02T02:24:11.852Z:
Received chunk:  Cookies...

2025-04-02T02:24:11.938Z:
Received chunk: 

...

2025-04-02T02:24:12.023Z:
Received chunk: This...

2025-04-02T02:24:12.109Z:
Received chunk:  selection...

2025-04-02T02:24:12.198Z:
Received chunk:  provides...

2025-04-02T02:24:12.285Z:
Received chunk:  a...

2025-04-02T02:24:12.372Z:
Received chunk:  variety...

2025-04-02T02:24:12.462Z:
Received chunk:  with...

2025-04-02T02:24:12.552Z:
Received chunk:  minimal...

2025-04-02T02:24:12.644Z:
Received chunk:  ingredients...

2025-04-02T02:24:12.734Z:
Received chunk:  and...

2025-04-02T02:24:12.821Z:
Received chunk:  flavors...

2025-04-02T02:24:12.909Z:
Received chunk: ....

2025-04-02T02:24:12.995Z:
Received chunk: 

> ...

2025-04-02T02:26:13.412Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:26:13.412Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:26:13.413Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:26:13.413Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:26:14.732Z:
Unloading model...

2025-04-02T02:26:14.733Z:
Model unloaded successfully

2025-04-02T02:26:14.733Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:26:14.733Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:26:14.733Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:26:14.733Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:26:20.898Z:
Generate response called with message: give me a short cooking recipe...

2025-04-02T02:26:20.898Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a short cooking recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:26:20.923Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:26:20.947Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:26:20.969Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:26:20.979Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:26:21.001Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:26:21.056Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:26:21.057Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:26:21.073Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0


2025-04-02T02:26:21.074Z:
Process stderr: print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:26:21.305Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:26:21.328Z:
Process stderr: .

2025-04-02T02:26:21.347Z:
Process stderr: .

2025-04-02T02:26:21.367Z:
Process stderr: .

2025-04-02T02:26:21.386Z:
Process stderr: .

2025-04-02T02:26:21.406Z:
Process stderr: .

2025-04-02T02:26:21.425Z:
Process stderr: .

2025-04-02T02:26:21.445Z:
Process stderr: .

2025-04-02T02:26:21.482Z:
Process stderr: .

2025-04-02T02:26:21.485Z:
Process stderr: .

2025-04-02T02:26:21.517Z:
Process stderr: .

2025-04-02T02:26:21.534Z:
Process stderr: .

2025-04-02T02:26:21.558Z:
Process stderr: .

2025-04-02T02:26:21.574Z:
Process stderr: .

2025-04-02T02:26:21.598Z:
Process stderr: .

2025-04-02T02:26:21.614Z:
Process stderr: .

2025-04-02T02:26:21.631Z:
Process stderr: .

2025-04-02T02:26:21.654Z:
Process stderr: .

2025-04-02T02:26:21.670Z:
Process stderr: .

2025-04-02T02:26:21.686Z:
Process stderr: .

2025-04-02T02:26:21.709Z:
Process stderr: .

2025-04-02T02:26:21.726Z:
Process stderr: .

2025-04-02T02:26:21.749Z:
Process stderr: .

2025-04-02T02:26:21.765Z:
Process stderr: .

2025-04-02T02:26:21.785Z:
Process stderr: .

2025-04-02T02:26:21.805Z:
Process stderr: .

2025-04-02T02:26:21.836Z:
Process stderr: .

2025-04-02T02:26:21.861Z:
Process stderr: .

2025-04-02T02:26:21.878Z:
Process stderr: .

2025-04-02T02:26:21.901Z:
Process stderr: .

2025-04-02T02:26:21.917Z:
Process stderr: .

2025-04-02T02:26:21.933Z:
Process stderr: .

2025-04-02T02:26:21.957Z:
Process stderr: .

2025-04-02T02:26:21.973Z:
Process stderr: .

2025-04-02T02:26:21.989Z:
Process stderr: .

2025-04-02T02:26:22.012Z:
Process stderr: .

2025-04-02T02:26:22.029Z:
Process stderr: .

2025-04-02T02:26:22.056Z:
Process stderr: .

2025-04-02T02:26:22.071Z:
Process stderr: .

2025-04-02T02:26:22.092Z:
Process stderr: .

2025-04-02T02:26:22.113Z:
Process stderr: .

2025-04-02T02:26:22.146Z:
Process stderr: .

2025-04-02T02:26:22.153Z:
Process stderr: .

2025-04-02T02:26:22.186Z:
Process stderr: .

2025-04-02T02:26:22.194Z:
Process stderr: .

2025-04-02T02:26:22.226Z:
Process stderr: .

2025-04-02T02:26:22.243Z:
Process stderr: .

2025-04-02T02:26:22.267Z:
Process stderr: .

2025-04-02T02:26:22.283Z:
Process stderr: .

2025-04-02T02:26:22.300Z:
Process stderr: .

2025-04-02T02:26:22.324Z:
Process stderr: .

2025-04-02T02:26:22.341Z:
Process stderr: .

2025-04-02T02:26:22.364Z:
Process stderr: .

2025-04-02T02:26:22.380Z:
Process stderr: .

2025-04-02T02:26:22.398Z:
Process stderr: .

2025-04-02T02:26:22.423Z:
Process stderr: .

2025-04-02T02:26:22.440Z:
Process stderr: .

2025-04-02T02:26:22.462Z:
Process stderr: .

2025-04-02T02:26:22.486Z:
Process stderr: .

2025-04-02T02:26:22.508Z:
Process stderr: .

2025-04-02T02:26:22.530Z:
Process stderr: .

2025-04-02T02:26:22.556Z:
Process stderr: .

2025-04-02T02:26:22.594Z:
Process stderr: .

2025-04-02T02:26:22.602Z:
Process stderr: .

2025-04-02T02:26:22.639Z:
Process stderr: .

2025-04-02T02:26:22.646Z:
Process stderr: .

2025-04-02T02:26:22.683Z:
Process stderr: ......

2025-04-02T02:26:22.684Z:
Process stderr: .........

2025-04-02T02:26:22.684Z:
Process stderr: ...


2025-04-02T02:26:22.686Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:26:22.687Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:26:22.706Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:26:22.712Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:26:22.985Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-02T02:26:22.985Z:
Process stderr: main: interactive mode on.


2025-04-02T02:26:22.986Z:
Process stderr: sampler seed: 2626379104
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:26:23.209Z:
Received chunk: <think>...

2025-04-02T02:26:23.304Z:
Received chunk: 
...

2025-04-02T02:26:23.399Z:
Received chunk: Alright...

2025-04-02T02:26:23.505Z:
Received chunk: ,...

2025-04-02T02:26:23.621Z:
Received chunk:  so...

2025-04-02T02:26:23.736Z:
Received chunk:  the...

2025-04-02T02:26:23.844Z:
Received chunk:  user...

2025-04-02T02:26:23.953Z:
Received chunk:  just...

2025-04-02T02:26:24.060Z:
Received chunk:  asked...

2025-04-02T02:26:24.166Z:
Received chunk:  for...

2025-04-02T02:26:24.279Z:
Received chunk:  a...

2025-04-02T02:26:24.391Z:
Received chunk:  short...

2025-04-02T02:26:24.508Z:
Received chunk:  cooking...

2025-04-02T02:26:24.620Z:
Received chunk:  recipe...

2025-04-02T02:26:24.726Z:
Received chunk: ....

2025-04-02T02:26:24.828Z:
Received chunk:  Hmm...

2025-04-02T02:26:24.928Z:
Received chunk: ,...

2025-04-02T02:26:25.028Z:
Received chunk:  okay...

2025-04-02T02:26:25.123Z:
Received chunk: ....

2025-04-02T02:26:25.210Z:
Received chunk:  Let...

2025-04-02T02:26:25.299Z:
Received chunk: 's...

2025-04-02T02:26:25.387Z:
Received chunk:  see...

2025-04-02T02:26:25.475Z:
Received chunk:  what...

2025-04-02T02:26:25.567Z:
Received chunk:  they...

2025-04-02T02:26:25.659Z:
Received chunk: 're...

2025-04-02T02:26:25.748Z:
Received chunk:  looking...

2025-04-02T02:26:25.835Z:
Received chunk:  for...

2025-04-02T02:26:25.921Z:
Received chunk: .

...

2025-04-02T02:26:26.007Z:
Received chunk: First...

2025-04-02T02:26:26.093Z:
Received chunk:  off...

2025-04-02T02:26:26.181Z:
Received chunk: ,...

2025-04-02T02:26:26.269Z:
Received chunk:  I...

2025-04-02T02:26:26.356Z:
Received chunk:  need...

2025-04-02T02:26:26.444Z:
Received chunk:  to...

2025-04-02T02:26:26.532Z:
Received chunk:  figure...

2025-04-02T02:26:26.628Z:
Received chunk:  out...

2025-04-02T02:26:26.728Z:
Received chunk:  their...

2025-04-02T02:26:26.821Z:
Received chunk:  intent...

2025-04-02T02:26:26.914Z:
Received chunk: ....

2025-04-02T02:26:27.003Z:
Received chunk:  Maybe...

2025-04-02T02:26:27.091Z:
Received chunk:  they...

2025-04-02T02:26:27.178Z:
Received chunk: 're...

2025-04-02T02:26:27.265Z:
Received chunk:  in...

2025-04-02T02:26:27.353Z:
Received chunk:  a...

2025-04-02T02:26:27.439Z:
Received chunk:  hurry...

2025-04-02T02:26:27.534Z:
Received chunk:  and...

2025-04-02T02:26:27.618Z:
Received chunk:  want...

2025-04-02T02:26:27.709Z:
Received chunk:  something...

2025-04-02T02:26:27.798Z:
Received chunk:  quick...

2025-04-02T02:26:27.885Z:
Received chunk: ....

2025-04-02T02:26:27.973Z:
Received chunk:  Or...

2025-04-02T02:26:28.065Z:
Received chunk:  perhaps...

2025-04-02T02:26:28.165Z:
Received chunk:  they...

2025-04-02T02:26:28.269Z:
Received chunk:  have...

2025-04-02T02:26:28.372Z:
Received chunk:  specific...

2025-04-02T02:26:28.472Z:
Received chunk:  dietary...

2025-04-02T02:26:28.579Z:
Received chunk:  preferences...

2025-04-02T02:26:28.685Z:
Received chunk:  or...

2025-04-02T02:26:28.795Z:
Received chunk:  restrictions...

2025-04-02T02:26:28.895Z:
Received chunk:  that...

2025-04-02T02:26:28.999Z:
Received chunk:  weren...

2025-04-02T02:26:29.100Z:
Received chunk: 't...

2025-04-02T02:26:29.200Z:
Received chunk:  mentioned...

2025-04-02T02:26:29.305Z:
Received chunk: ....

2025-04-02T02:26:29.413Z:
Received chunk:  Since...

2025-04-02T02:26:29.525Z:
Received chunk:  it...

2025-04-02T02:26:29.631Z:
Received chunk: 's...

2025-04-02T02:26:29.726Z:
Received chunk:  not...

2025-04-02T02:26:29.817Z:
Received chunk:  specified...

2025-04-02T02:26:29.906Z:
Received chunk: ,...

2025-04-02T02:26:29.996Z:
Received chunk:  I...

2025-04-02T02:26:30.087Z:
Received chunk: 'll...

2025-04-02T02:26:30.180Z:
Received chunk:  aim...

2025-04-02T02:26:30.271Z:
Received chunk:  for...

2025-04-02T02:26:30.362Z:
Received chunk:  a...

2025-04-02T02:26:30.450Z:
Received chunk:  general...

2025-04-02T02:26:30.538Z:
Received chunk:  but...

2025-04-02T02:26:30.628Z:
Received chunk:  tasty...

2025-04-02T02:26:30.718Z:
Received chunk:  recipe...

2025-04-02T02:26:30.809Z:
Received chunk:  that...

2025-04-02T02:26:30.897Z:
Received chunk: 's...

2025-04-02T02:26:30.985Z:
Received chunk:  easy...

2025-04-02T02:26:31.072Z:
Received chunk:  to...

2025-04-02T02:26:31.159Z:
Received chunk:  make...

2025-04-02T02:26:31.246Z:
Received chunk: .

...

2025-04-02T02:26:31.334Z:
Received chunk: I...

2025-04-02T02:26:31.422Z:
Received chunk:  should...

2025-04-02T02:26:31.509Z:
Received chunk:  consider...

2025-04-02T02:26:31.599Z:
Received chunk:  the...

2025-04-02T02:26:31.692Z:
Received chunk:  cooking...

2025-04-02T02:26:31.785Z:
Received chunk:  time...

2025-04-02T02:26:31.872Z:
Received chunk:  and...

2025-04-02T02:26:31.961Z:
Received chunk:  ingredients...

2025-04-02T02:26:32.047Z:
Received chunk:  that...

2025-04-02T02:26:32.134Z:
Received chunk:  are...

2025-04-02T02:26:32.221Z:
Received chunk:  commonly...

2025-04-02T02:26:32.308Z:
Received chunk:  available...

2025-04-02T02:26:32.395Z:
Received chunk: ....

2025-04-02T02:26:32.484Z:
Received chunk:  Avoid...

2025-04-02T02:26:32.575Z:
Received chunk: ing...

2025-04-02T02:26:32.661Z:
Received chunk:  too...

2025-04-02T02:26:32.758Z:
Received chunk:  many...

2025-04-02T02:26:32.852Z:
Received chunk:  exotic...

2025-04-02T02:26:32.942Z:
Received chunk:  items...

2025-04-02T02:26:33.032Z:
Received chunk:  is...

2025-04-02T02:26:33.119Z:
Received chunk:  probably...

2025-04-02T02:26:33.205Z:
Received chunk:  best...

2025-04-02T02:26:33.292Z:
Received chunk:  here...

2025-04-02T02:26:33.384Z:
Received chunk: ....

2025-04-02T02:26:33.468Z:
Received chunk:  Chicken...

2025-04-02T02:26:33.575Z:
Received chunk:  stir...

2025-04-02T02:26:33.664Z:
Received chunk: -f...

2025-04-02T02:26:33.773Z:
Received chunk: ry...

2025-04-02T02:26:33.868Z:
Received chunk:  comes...

2025-04-02T02:26:33.956Z:
Received chunk:  to...

2025-04-02T02:26:34.045Z:
Received chunk:  mind...

2025-04-02T02:26:34.134Z:
Received chunk:  because...

2025-04-02T02:26:34.221Z:
Received chunk:  it...

2025-04-02T02:26:34.307Z:
Received chunk: 's...

2025-04-02T02:26:34.393Z:
Received chunk:  versatile...

2025-04-02T02:26:34.481Z:
Received chunk:  and...

2025-04-02T02:26:34.576Z:
Received chunk:  can...

2025-04-02T02:26:34.657Z:
Received chunk:  be...

2025-04-02T02:26:34.751Z:
Received chunk:  made...

2025-04-02T02:26:34.845Z:
Received chunk:  in...

2025-04-02T02:26:34.933Z:
Received chunk:  different...

2025-04-02T02:26:35.019Z:
Received chunk:  ways...

2025-04-02T02:26:35.104Z:
Received chunk:  depending...

2025-04-02T02:26:35.191Z:
Received chunk:  on...

2025-04-02T02:26:35.278Z:
Received chunk:  what...

2025-04-02T02:26:35.364Z:
Received chunk: 's...

2025-04-02T02:26:35.449Z:
Received chunk:  available...

2025-04-02T02:26:35.537Z:
Received chunk: .

...

2025-04-02T02:26:35.626Z:
Received chunk: Let...

2025-04-02T02:26:35.717Z:
Received chunk:  me...

2025-04-02T02:26:35.810Z:
Received chunk:  think...

2025-04-02T02:26:35.895Z:
Received chunk:  about...

2025-04-02T02:26:35.983Z:
Received chunk:  the...

2025-04-02T02:26:36.069Z:
Received chunk:  vegetables...

2025-04-02T02:26:36.156Z:
Received chunk: ....

2025-04-02T02:26:36.242Z:
Received chunk:  Bro...

2025-04-02T02:26:36.328Z:
Received chunk: ccoli...

2025-04-02T02:26:36.414Z:
Received chunk: ,...

2025-04-02T02:26:36.500Z:
Received chunk:  bell...

2025-04-02T02:26:36.591Z:
Received chunk:  peppers...

2025-04-02T02:26:36.678Z:
Received chunk: ,...

2025-04-02T02:26:36.774Z:
Received chunk:  carrots...

2025-04-02T02:26:36.865Z:
Received chunk: ,...

2025-04-02T02:26:36.953Z:
Received chunk:  and...

2025-04-02T02:26:37.043Z:
Received chunk:  snap...

2025-04-02T02:26:37.131Z:
Received chunk:  peas...

2025-04-02T02:26:37.219Z:
Received chunk:  are...

2025-04-02T02:26:37.307Z:
Received chunk:  all...

2025-04-02T02:26:37.395Z:
Received chunk:  easy...

2025-04-02T02:26:37.483Z:
Received chunk:  to...

2025-04-02T02:26:37.576Z:
Received chunk:  find...

2025-04-02T02:26:37.670Z:
Received chunk:  and...

2025-04-02T02:26:37.767Z:
Received chunk:  add...

2025-04-02T02:26:37.861Z:
Received chunk:  a...

2025-04-02T02:26:37.951Z:
Received chunk:  nice...

2025-04-02T02:26:38.042Z:
Received chunk:  variety...

2025-04-02T02:26:38.134Z:
Received chunk:  of...

2025-04-02T02:26:38.228Z:
Received chunk:  flavors...

2025-04-02T02:26:38.324Z:
Received chunk: ....

2025-04-02T02:26:38.419Z:
Received chunk:  How...

2025-04-02T02:26:38.515Z:
Received chunk:  about...

2025-04-02T02:26:38.612Z:
Received chunk:  the...

2025-04-02T02:26:38.703Z:
Received chunk:  spices...

2025-04-02T02:26:38.796Z:
Received chunk: ?...

2025-04-02T02:26:38.886Z:
Received chunk:  Using...

2025-04-02T02:26:38.972Z:
Received chunk:  soy...

2025-04-02T02:26:39.059Z:
Received chunk:  sauce...

2025-04-02T02:26:39.146Z:
Received chunk:  adds...

2025-04-02T02:26:39.234Z:
Received chunk:  um...

2025-04-02T02:26:39.320Z:
Received chunk: ami...

2025-04-02T02:26:39.407Z:
Received chunk: ,...

2025-04-02T02:26:39.497Z:
Received chunk:  garlic...

2025-04-02T02:26:39.593Z:
Received chunk:  for...

2025-04-02T02:26:39.676Z:
Received chunk:  flavor...

2025-04-02T02:26:39.767Z:
Received chunk: ,...

2025-04-02T02:26:39.856Z:
Received chunk:  ginger...

2025-04-02T02:26:39.943Z:
Received chunk:  to...

2025-04-02T02:26:40.031Z:
Received chunk:  bright...

2025-04-02T02:26:40.120Z:
Received chunk: en...

2025-04-02T02:26:40.209Z:
Received chunk:  things...

2025-04-02T02:26:40.297Z:
Received chunk:  up...

2025-04-02T02:26:40.385Z:
Received chunk: ,...

2025-04-02T02:26:40.472Z:
Received chunk:  and...

2025-04-02T02:26:40.562Z:
Received chunk:  some...

2025-04-02T02:26:40.652Z:
Received chunk:  sesame...

2025-04-02T02:26:40.742Z:
Received chunk:  oil...

2025-04-02T02:26:40.833Z:
Received chunk:  for...

2025-04-02T02:26:40.920Z:
Received chunk:  that...

2025-04-02T02:26:41.007Z:
Received chunk:  nice...

2025-04-02T02:26:41.092Z:
Received chunk:  aroma...

2025-04-02T02:26:41.181Z:
Received chunk:  and...

2025-04-02T02:26:41.273Z:
Received chunk:  shine...

2025-04-02T02:26:41.365Z:
Received chunk:  on...

2025-04-02T02:26:41.455Z:
Received chunk:  the...

2025-04-02T02:26:41.543Z:
Received chunk:  rice...

2025-04-02T02:26:41.632Z:
Received chunk: .

...

2025-04-02T02:26:41.722Z:
Received chunk: The...

2025-04-02T02:26:41.815Z:
Received chunk:  proteins...

2025-04-02T02:26:41.910Z:
Received chunk: :...

2025-04-02T02:26:42.000Z:
Received chunk:  chicken...

2025-04-02T02:26:42.088Z:
Received chunk:  breast...

2025-04-02T02:26:42.173Z:
Received chunk:  is...

2025-04-02T02:26:42.260Z:
Received chunk:  lean...

2025-04-02T02:26:42.347Z:
Received chunk:  and...

2025-04-02T02:26:42.436Z:
Received chunk:  versatile...

2025-04-02T02:26:42.525Z:
Received chunk: ....

2025-04-02T02:26:42.623Z:
Received chunk:  It...

2025-04-02T02:26:42.729Z:
Received chunk: 's...

2025-04-02T02:26:42.824Z:
Received chunk:  also...

2025-04-02T02:26:42.918Z:
Received chunk:  widely...

2025-04-02T02:26:43.010Z:
Received chunk:  available...

2025-04-02T02:26:43.099Z:
Received chunk:  in...

2025-04-02T02:26:43.191Z:
Received chunk:  most...

2025-04-02T02:26:43.282Z:
Received chunk:  households...

2025-04-02T02:26:43.368Z:
Received chunk: ....

2025-04-02T02:26:43.458Z:
Received chunk:  Adding...

2025-04-02T02:26:43.548Z:
Received chunk:  tofu...

2025-04-02T02:26:43.641Z:
Received chunk:  could...

2025-04-02T02:26:43.740Z:
Received chunk:  be...

2025-04-02T02:26:43.826Z:
Received chunk:  an...

2025-04-02T02:26:43.913Z:
Received chunk:  option...

2025-04-02T02:26:44.004Z:
Received chunk:  if...

2025-04-02T02:26:44.101Z:
Received chunk:  someone...

2025-04-02T02:26:44.184Z:
Received chunk:  is...

2025-04-02T02:26:44.272Z:
Received chunk:  vegetarian...

2025-04-02T02:26:44.361Z:
Received chunk: ,...

2025-04-02T02:26:44.450Z:
Received chunk:  but...

2025-04-02T02:26:44.540Z:
Received chunk:  since...

2025-04-02T02:26:44.633Z:
Received chunk:  it...

2025-04-02T02:26:44.730Z:
Received chunk:  wasn...

2025-04-02T02:26:44.826Z:
Received chunk: 't...

2025-04-02T02:26:44.916Z:
Received chunk:  specified...

2025-04-02T02:26:45.005Z:
Received chunk: ,...

2025-04-02T02:26:45.092Z:
Received chunk:  I...

2025-04-02T02:26:45.179Z:
Received chunk: 'll...

2025-04-02T02:26:45.265Z:
Received chunk:  stick...

2025-04-02T02:26:45.354Z:
Received chunk:  with...

2025-04-02T02:26:45.443Z:
Received chunk:  chicken...

2025-04-02T02:26:45.530Z:
Received chunk:  only...

2025-04-02T02:26:45.616Z:
Received chunk: .

...

2025-04-02T02:26:45.704Z:
Received chunk: For...

2025-04-02T02:26:45.794Z:
Received chunk:  the...

2025-04-02T02:26:45.887Z:
Received chunk:  dish...

2025-04-02T02:26:45.977Z:
Received chunk:  itself...

2025-04-02T02:26:46.066Z:
Received chunk: ,...

2025-04-02T02:26:46.157Z:
Received chunk:  boiling...

2025-04-02T02:26:46.249Z:
Received chunk:  rice...

2025-04-02T02:26:46.337Z:
Received chunk:  is...

2025-04-02T02:26:46.425Z:
Received chunk:  simple...

2025-04-02T02:26:46.513Z:
Received chunk:  and...

2025-04-02T02:26:46.604Z:
Received chunk:  goes...

2025-04-02T02:26:46.697Z:
Received chunk:  well...

2025-04-02T02:26:46.795Z:
Received chunk:  with...

2025-04-02T02:26:46.891Z:
Received chunk:  stir...

2025-04-02T02:26:46.982Z:
Received chunk: -f...

2025-04-02T02:26:47.075Z:
Received chunk: ries...

2025-04-02T02:26:47.175Z:
Received chunk:  because...

2025-04-02T02:26:47.276Z:
Received chunk:  it...

2025-04-02T02:26:47.368Z:
Received chunk:  keeps...

2025-04-02T02:26:47.460Z:
Received chunk:  its...

2025-04-02T02:26:47.546Z:
Received chunk:  shape...

2025-04-02T02:26:47.642Z:
Received chunk: ....

2025-04-02T02:26:47.722Z:
Received chunk:  I...

2025-04-02T02:26:47.819Z:
Received chunk:  should...

2025-04-02T02:26:47.911Z:
Received chunk:  mention...

2025-04-02T02:26:47.999Z:
Received chunk:  cooking...

2025-04-02T02:26:48.086Z:
Received chunk:  the...

2025-04-02T02:26:48.175Z:
Received chunk:  rice...

2025-04-02T02:26:48.262Z:
Received chunk:  first...

2025-04-02T02:26:48.350Z:
Received chunk:  so...

2025-04-02T02:26:48.438Z:
Received chunk:  it...

2025-04-02T02:26:48.526Z:
Received chunk: 's...

2025-04-02T02:26:48.615Z:
Received chunk:  ready...

2025-04-02T02:26:48.705Z:
Received chunk:  when...

2025-04-02T02:26:48.802Z:
Received chunk:  needed...

2025-04-02T02:26:48.901Z:
Received chunk: .

...

2025-04-02T02:26:48.991Z:
Received chunk: Putting...

2025-04-02T02:26:49.083Z:
Received chunk:  it...

2025-04-02T02:26:49.172Z:
Received chunk:  all...

2025-04-02T02:26:49.265Z:
Received chunk:  together...

2025-04-02T02:26:49.360Z:
Received chunk: :...

2025-04-02T02:26:49.450Z:
Received chunk:  sa...

2025-04-02T02:26:49.537Z:
Received chunk: ut...

2025-04-02T02:26:49.624Z:
Received chunk: é...

2025-04-02T02:26:49.713Z:
Received chunk:  some...

2025-04-02T02:26:49.804Z:
Received chunk:  garlic...

2025-04-02T02:26:49.897Z:
Received chunk:  and...

2025-04-02T02:26:49.984Z:
Received chunk:  ginger...

2025-04-02T02:26:50.071Z:
Received chunk: ,...

2025-04-02T02:26:50.168Z:
Received chunk:  add...

2025-04-02T02:26:50.254Z:
Received chunk:  chicken...

2025-04-02T02:26:50.343Z:
Received chunk: ,...

2025-04-02T02:26:50.436Z:
Received chunk:  then...

2025-04-02T02:26:50.525Z:
Received chunk:  veggies...

2025-04-02T02:26:50.611Z:
Received chunk: ,...

2025-04-02T02:26:50.699Z:
Received chunk:  pour...

2025-04-02T02:26:50.796Z:
Received chunk:  in...

2025-04-02T02:26:50.883Z:
Received chunk:  soy...

2025-04-02T02:26:50.970Z:
Received chunk:  sauce...

2025-04-02T02:26:51.057Z:
Received chunk: ,...

2025-04-02T02:26:51.142Z:
Received chunk:  maybe...

2025-04-02T02:26:51.229Z:
Received chunk:  a...

2025-04-02T02:26:51.316Z:
Received chunk:  bit...

2025-04-02T02:26:51.401Z:
Received chunk:  of...

2025-04-02T02:26:51.488Z:
Received chunk:  sugar...

2025-04-02T02:26:51.575Z:
Received chunk:  for...

2025-04-02T02:26:51.667Z:
Received chunk:  sweetness...

2025-04-02T02:26:51.752Z:
Received chunk: ,...

2025-04-02T02:26:51.846Z:
Received chunk:  toss...

2025-04-02T02:26:51.936Z:
Received chunk:  everything...

2025-04-02T02:26:52.023Z:
Received chunk:  until...

2025-04-02T02:26:52.110Z:
Received chunk:  it...

2025-04-02T02:26:52.196Z:
Received chunk: 's...

2025-04-02T02:26:52.283Z:
Received chunk:  coated...

2025-04-02T02:26:52.369Z:
Received chunk:  nicely...

2025-04-02T02:26:52.455Z:
Received chunk: ,...

2025-04-02T02:26:52.542Z:
Received chunk:  cook...

2025-04-02T02:26:52.629Z:
Received chunk:  briefly...

2025-04-02T02:26:52.717Z:
Received chunk: ,...

2025-04-02T02:26:52.806Z:
Received chunk:  serve...

2025-04-02T02:26:52.902Z:
Received chunk:  with...

2025-04-02T02:26:52.990Z:
Received chunk:  rice...

2025-04-02T02:26:53.078Z:
Received chunk: .

...

2025-04-02T02:26:53.164Z:
Received chunk: I...

2025-04-02T02:26:53.250Z:
Received chunk:  should...

2025-04-02T02:26:53.336Z:
Received chunk:  keep...

2025-04-02T02:26:53.422Z:
Received chunk:  the...

2025-04-02T02:26:53.508Z:
Received chunk:  instructions...

2025-04-02T02:26:53.595Z:
Received chunk:  clear...

2025-04-02T02:26:53.687Z:
Received chunk:  and...

2025-04-02T02:26:53.773Z:
Received chunk:  concise...

2025-04-02T02:26:53.866Z:
Received chunk:  since...

2025-04-02T02:26:53.957Z:
Received chunk:  the...

2025-04-02T02:26:54.044Z:
Received chunk:  user...

2025-04-02T02:26:54.132Z:
Received chunk:  asked...

2025-04-02T02:26:54.220Z:
Received chunk:  for...

2025-04-02T02:26:54.307Z:
Received chunk:  a...

2025-04-02T02:26:54.393Z:
Received chunk:  short...

2025-04-02T02:26:54.479Z:
Received chunk:  recipe...

2025-04-02T02:26:54.566Z:
Received chunk: ....

2025-04-02T02:26:54.656Z:
Received chunk:  Maybe...

2025-04-02T02:26:54.747Z:
Received chunk:  mention...

2025-04-02T02:26:54.837Z:
Received chunk:  optional...

2025-04-02T02:26:54.933Z:
Received chunk:  ingredients...

2025-04-02T02:26:55.020Z:
Received chunk:  like...

2025-04-02T02:26:55.106Z:
Received chunk:  adding...

2025-04-02T02:26:55.193Z:
Received chunk:  more...

2025-04-02T02:26:55.278Z:
Received chunk:  veggies...

2025-04-02T02:26:55.363Z:
Received chunk:  or...

2025-04-02T02:26:55.449Z:
Received chunk:  tofu...

2025-04-02T02:26:55.536Z:
Received chunk:  if...

2025-04-02T02:26:55.624Z:
Received chunk:  they...

2025-04-02T02:26:55.713Z:
Received chunk:  want...

2025-04-02T02:26:55.801Z:
Received chunk:  to...

2025-04-02T02:26:55.891Z:
Received chunk:  customize...

2025-04-02T02:26:55.983Z:
Received chunk:  it...

2025-04-02T02:26:56.069Z:
Received chunk: .

...

2025-04-02T02:26:56.156Z:
Received chunk: Also...

2025-04-02T02:26:56.243Z:
Received chunk: ,...

2025-04-02T02:26:56.330Z:
Received chunk:  considering...

2025-04-02T02:26:56.416Z:
Received chunk:  cultural...

2025-04-02T02:26:56.503Z:
Received chunk:  preferences...

2025-04-02T02:26:56.589Z:
Received chunk: ,...

2025-04-02T02:26:56.676Z:
Received chunk:  this...

2025-04-02T02:26:56.765Z:
Received chunk:  chicken...

2025-04-02T02:26:56.857Z:
Received chunk:  stir...

2025-04-02T02:26:56.948Z:
Received chunk: -f...

2025-04-02T02:26:57.036Z:
Received chunk: ry...

2025-04-02T02:26:57.122Z:
Received chunk:  is...

2025-04-02T02:26:57.212Z:
Received chunk:  pretty...

2025-04-02T02:26:57.301Z:
Received chunk:  universal...

2025-04-02T02:26:57.391Z:
Received chunk:  so...

2025-04-02T02:26:57.480Z:
Received chunk:  chances...

2025-04-02T02:26:57.569Z:
Received chunk:  are...

2025-04-02T02:26:57.659Z:
Received chunk:  most...

2025-04-02T02:26:57.750Z:
Received chunk:  people...

2025-04-02T02:26:57.845Z:
Received chunk:  can...

2025-04-02T02:26:57.942Z:
Received chunk:  enjoy...

2025-04-02T02:26:58.032Z:
Received chunk:  it...

2025-04-02T02:26:58.123Z:
Received chunk:  without...

2025-04-02T02:26:58.213Z:
Received chunk:  any...

2025-04-02T02:26:58.302Z:
Received chunk:  issues...

2025-04-02T02:26:58.390Z:
Received chunk: .

...

2025-04-02T02:26:58.484Z:
Received chunk: Alright...

2025-04-02T02:26:58.573Z:
Received chunk: ,...

2025-04-02T02:26:58.669Z:
Received chunk:  I...

2025-04-02T02:26:58.762Z:
Received chunk:  think...

2025-04-02T02:26:58.859Z:
Received chunk:  that...

2025-04-02T02:26:58.954Z:
Received chunk: 's...

2025-04-02T02:26:59.049Z:
Received chunk:  a...

2025-04-02T02:26:59.141Z:
Received chunk:  solid...

2025-04-02T02:26:59.230Z:
Received chunk:  plan...

2025-04-02T02:26:59.320Z:
Received chunk: ....

2025-04-02T02:26:59.409Z:
Received chunk:  It...

2025-04-02T02:26:59.499Z:
Received chunk: 's...

2025-04-02T02:26:59.593Z:
Received chunk:  quick...

2025-04-02T02:26:59.690Z:
Received chunk: ,...

2025-04-02T02:26:59.780Z:
Received chunk:  uses...

2025-04-02T02:26:59.873Z:
Received chunk:  common...

2025-04-02T02:26:59.966Z:
Received chunk:  ingredients...

2025-04-02T02:27:00.055Z:
Received chunk: ,...

2025-04-02T02:27:00.143Z:
Received chunk:  and...

2025-04-02T02:27:00.230Z:
Received chunk:  offers...

2025-04-02T02:27:00.318Z:
Received chunk:  some...

2025-04-02T02:27:00.405Z:
Received chunk:  flexibility...

2025-04-02T02:27:00.494Z:
Received chunk:  for...

2025-04-02T02:27:00.581Z:
Received chunk:  the...

2025-04-02T02:27:00.668Z:
Received chunk:  user...

2025-04-02T02:27:00.756Z:
Received chunk: .
...

2025-04-02T02:27:00.850Z:
Received chunk: </think>...

2025-04-02T02:27:00.943Z:
Received chunk: 

...

2025-04-02T02:27:01.032Z:
Received chunk: Sure...

2025-04-02T02:27:01.121Z:
Received chunk: !...

2025-04-02T02:27:01.209Z:
Received chunk:  Here...

2025-04-02T02:27:01.298Z:
Received chunk: 's...

2025-04-02T02:27:01.386Z:
Received chunk:  a...

2025-04-02T02:27:01.475Z:
Received chunk:  short...

2025-04-02T02:27:01.562Z:
Received chunk:  and...

2025-04-02T02:27:01.648Z:
Received chunk:  simple...

2025-04-02T02:27:01.736Z:
Received chunk:  recipe...

2025-04-02T02:27:01.823Z:
Received chunk:  for...

2025-04-02T02:27:01.915Z:
Received chunk:  **...

2025-04-02T02:27:02.005Z:
Received chunk: Chicken...

2025-04-02T02:27:02.093Z:
Received chunk:  Stir...

2025-04-02T02:27:02.181Z:
Received chunk: -F...

2025-04-02T02:27:02.269Z:
Received chunk: ry...

2025-04-02T02:27:02.355Z:
Received chunk:  with...

2025-04-02T02:27:02.442Z:
Received chunk:  Bro...

2025-04-02T02:27:02.529Z:
Received chunk: ccoli...

2025-04-02T02:27:02.616Z:
Received chunk: **...

2025-04-02T02:27:02.704Z:
Received chunk: :

...

2025-04-02T02:27:02.793Z:
Received chunk: ###...

2025-04-02T02:27:02.887Z:
Received chunk:  Ingredients...

2025-04-02T02:27:02.979Z:
Received chunk: :
...

2025-04-02T02:27:03.066Z:
Received chunk: -...

2025-04-02T02:27:03.155Z:
Received chunk:  ...

2025-04-02T02:27:03.242Z:
Received chunk: 2...

2025-04-02T02:27:03.329Z:
Received chunk:  cups...

2025-04-02T02:27:03.415Z:
Received chunk:  chicken...

2025-04-02T02:27:03.503Z:
Received chunk:  breast...

2025-04-02T02:27:03.590Z:
Received chunk:  (...

2025-04-02T02:27:03.678Z:
Received chunk: or...

2025-04-02T02:27:03.768Z:
Received chunk:  tofu...

2025-04-02T02:27:03.862Z:
Received chunk:  if...

2025-04-02T02:27:03.954Z:
Received chunk:  vegetarian...

2025-04-02T02:27:04.040Z:
Received chunk: )
...

2025-04-02T02:27:04.127Z:
Received chunk: -...

2025-04-02T02:27:04.215Z:
Received chunk:  ...

2025-04-02T02:27:04.302Z:
Received chunk: 1...

2025-04-02T02:27:04.388Z:
Received chunk:  red...

2025-04-02T02:27:04.476Z:
Received chunk:  bell...

2025-04-02T02:27:04.562Z:
Received chunk:  pepper...

2025-04-02T02:27:04.649Z:
Received chunk: ,...

2025-04-02T02:27:04.744Z:
Received chunk:  sliced...

2025-04-02T02:27:04.826Z:
Received chunk: 
...

2025-04-02T02:27:04.920Z:
Received chunk: -...

2025-04-02T02:27:05.010Z:
Received chunk:  ...

2025-04-02T02:27:05.098Z:
Received chunk: 1...

2025-04-02T02:27:05.187Z:
Received chunk:  green...

2025-04-02T02:27:05.274Z:
Received chunk:  bell...

2025-04-02T02:27:05.361Z:
Received chunk:  pepper...

2025-04-02T02:27:05.448Z:
Received chunk: ,...

2025-04-02T02:27:05.535Z:
Received chunk:  sliced...

2025-04-02T02:27:05.623Z:
Received chunk: 
...

2025-04-02T02:27:05.711Z:
Received chunk: -...

2025-04-02T02:27:05.800Z:
Received chunk:  ...

2025-04-02T02:27:05.893Z:
Received chunk: 2...

2025-04-02T02:27:05.986Z:
Received chunk:  carrots...

2025-04-02T02:27:06.072Z:
Received chunk: ,...

2025-04-02T02:27:06.160Z:
Received chunk:  sliced...

2025-04-02T02:27:06.248Z:
Received chunk: 
...

2025-04-02T02:27:06.334Z:
Received chunk: -...

2025-04-02T02:27:06.421Z:
Received chunk:  ...

2025-04-02T02:27:06.508Z:
Received chunk: 3...

2025-04-02T02:27:06.594Z:
Received chunk:  cups...

2025-04-02T02:27:06.681Z:
Received chunk:  broccoli...

2025-04-02T02:27:06.769Z:
Received chunk:  flo...

2025-04-02T02:27:06.859Z:
Received chunk: rets...

2025-04-02T02:27:06.951Z:
Received chunk: 
...

2025-04-02T02:27:07.040Z:
Received chunk: -...

2025-04-02T02:27:07.127Z:
Received chunk:  ...

2025-04-02T02:27:07.216Z:
Received chunk: 2...

2025-04-02T02:27:07.302Z:
Received chunk:  tablespoons...

2025-04-02T02:27:07.389Z:
Received chunk:  soy...

2025-04-02T02:27:07.477Z:
Received chunk:  sauce...

2025-04-02T02:27:07.564Z:
Received chunk: 
...

2025-04-02T02:27:07.651Z:
Received chunk: -...

2025-04-02T02:27:07.740Z:
Received chunk:  ...

2025-04-02T02:27:07.829Z:
Received chunk: 1...

2025-04-02T02:27:07.924Z:
Received chunk:  teaspoon...

2025-04-02T02:27:08.016Z:
Received chunk:  sesame...

2025-04-02T02:27:08.104Z:
Received chunk:  oil...

2025-04-02T02:27:08.192Z:
Received chunk: 
...

2025-04-02T02:27:08.280Z:
Received chunk: -...

2025-04-02T02:27:08.367Z:
Received chunk:  ...

2025-04-02T02:27:08.454Z:
Received chunk: 4...

2025-04-02T02:27:08.541Z:
Received chunk:  garlic...

2025-04-02T02:27:08.628Z:
Received chunk:  cloves...

2025-04-02T02:27:08.715Z:
Received chunk: ,...

2025-04-02T02:27:08.807Z:
Received chunk:  minced...

2025-04-02T02:27:08.905Z:
Received chunk: 
...

2025-04-02T02:27:08.992Z:
Received chunk: -...

2025-04-02T02:27:09.081Z:
Received chunk:  ...

2025-04-02T02:27:09.169Z:
Received chunk: 1...

2025-04-02T02:27:09.257Z:
Received chunk:  tablespoon...

2025-04-02T02:27:09.344Z:
Received chunk:  ginger...

2025-04-02T02:27:09.430Z:
Received chunk: ,...

2025-04-02T02:27:09.518Z:
Received chunk:  minced...

2025-04-02T02:27:09.605Z:
Received chunk:  (...

2025-04-02T02:27:09.691Z:
Received chunk: or...

2025-04-02T02:27:09.780Z:
Received chunk:  ...

2025-04-02T02:27:09.868Z:
Received chunk: 1...

2025-04-02T02:27:09.961Z:
Received chunk:  tsp...

2025-04-02T02:27:10.051Z:
Received chunk:  ground...

2025-04-02T02:27:10.139Z:
Received chunk:  ginger...

2025-04-02T02:27:10.226Z:
Received chunk: )
...

2025-04-02T02:27:10.314Z:
Received chunk: -...

2025-04-02T02:27:10.402Z:
Received chunk:  Salt...

2025-04-02T02:27:10.489Z:
Received chunk:  and...

2025-04-02T02:27:10.576Z:
Received chunk:  pepper...

2025-04-02T02:27:10.662Z:
Received chunk:  to...

2025-04-02T02:27:10.749Z:
Received chunk:  taste...

2025-04-02T02:27:10.838Z:
Received chunk: 

...

2025-04-02T02:27:10.929Z:
Received chunk: ###...

2025-04-02T02:27:11.023Z:
Received chunk:  Instructions...

2025-04-02T02:27:11.111Z:
Received chunk: :
...

2025-04-02T02:27:11.199Z:
Received chunk: 1...

2025-04-02T02:27:11.286Z:
Received chunk: ....

2025-04-02T02:27:11.373Z:
Received chunk:  **...

2025-04-02T02:27:11.460Z:
Received chunk: Cook...

2025-04-02T02:27:11.547Z:
Received chunk:  the...

2025-04-02T02:27:11.634Z:
Received chunk:  rice...

2025-04-02T02:27:11.721Z:
Received chunk: **:...

2025-04-02T02:27:11.810Z:
Received chunk:  Bo...

2025-04-02T02:27:11.902Z:
Received chunk: il...

2025-04-02T02:27:11.993Z:
Received chunk:  water...

2025-04-02T02:27:12.080Z:
Received chunk:  in...

2025-04-02T02:27:12.167Z:
Received chunk:  a...

2025-04-02T02:27:12.254Z:
Received chunk:  pot...

2025-04-02T02:27:12.341Z:
Received chunk:  and...

2025-04-02T02:27:12.428Z:
Received chunk:  add...

2025-04-02T02:27:12.515Z:
Received chunk:  un...

2025-04-02T02:27:12.602Z:
Received chunk: cooked...

2025-04-02T02:27:12.689Z:
Received chunk:  white...

2025-04-02T02:27:12.785Z:
Received chunk:  rice...

2025-04-02T02:27:12.866Z:
Received chunk: ....

2025-04-02T02:27:12.958Z:
Received chunk:  Cook...

2025-04-02T02:27:13.049Z:
Received chunk:  according...

2025-04-02T02:27:13.137Z:
Received chunk:  to...

2025-04-02T02:27:13.224Z:
Received chunk:  package...

2025-04-02T02:27:13.311Z:
Received chunk:  instructions...

2025-04-02T02:27:13.399Z:
Received chunk:  until...

2025-04-02T02:27:13.487Z:
Received chunk:  tender...

2025-04-02T02:27:13.573Z:
Received chunk: .
...

2025-04-02T02:27:13.660Z:
Received chunk: 2...

2025-04-02T02:27:13.748Z:
Received chunk: ....

2025-04-02T02:27:13.839Z:
Received chunk:  **...

2025-04-02T02:27:13.931Z:
Received chunk: S...

2025-04-02T02:27:14.022Z:
Received chunk: auté...

2025-04-02T02:27:14.110Z:
Received chunk:  the...

2025-04-02T02:27:14.197Z:
Received chunk:  spices...

2025-04-02T02:27:14.284Z:
Received chunk: **:...

2025-04-02T02:27:14.372Z:
Received chunk:  In...

2025-04-02T02:27:14.459Z:
Received chunk:  a...

2025-04-02T02:27:14.546Z:
Received chunk:  large...

2025-04-02T02:27:14.634Z:
Received chunk:  skillet...

2025-04-02T02:27:14.721Z:
Received chunk:  or...

2025-04-02T02:27:14.810Z:
Received chunk:  w...

2025-04-02T02:27:14.900Z:
Received chunk: ok...

2025-04-02T02:27:15.007Z:
Received chunk: ,...

2025-04-02T02:27:15.096Z:
Received chunk:  heat...

2025-04-02T02:27:15.183Z:
Received chunk:  sesame...

2025-04-02T02:27:15.271Z:
Received chunk:  oil...

2025-04-02T02:27:15.358Z:
Received chunk:  over...

2025-04-02T02:27:15.446Z:
Received chunk:  medium...

2025-04-02T02:27:15.534Z:
Received chunk:  heat...

2025-04-02T02:27:15.622Z:
Received chunk: ....

2025-04-02T02:27:15.709Z:
Received chunk:  Add...

2025-04-02T02:27:15.802Z:
Received chunk:  minced...

2025-04-02T02:27:15.885Z:
Received chunk:  garlic...

2025-04-02T02:27:15.978Z:
Received chunk:  and...

2025-04-02T02:27:16.070Z:
Received chunk:  ginger...

2025-04-02T02:27:16.158Z:
Received chunk: ,...

2025-04-02T02:27:16.248Z:
Received chunk:  sa...

2025-04-02T02:27:16.335Z:
Received chunk: ut...

2025-04-02T02:27:16.423Z:
Received chunk: é...

2025-04-02T02:27:16.510Z:
Received chunk:  for...

2025-04-02T02:27:16.597Z:
Received chunk:  ...

2025-04-02T02:27:16.685Z:
Received chunk: 1...

2025-04-02T02:27:16.773Z:
Received chunk: -...

2025-04-02T02:27:16.862Z:
Received chunk: 2...

2025-04-02T02:27:16.955Z:
Received chunk:  minutes...

2025-04-02T02:27:17.049Z:
Received chunk:  until...

2025-04-02T02:27:17.135Z:
Received chunk:  frag...

2025-04-02T02:27:17.224Z:
Received chunk: rant...

2025-04-02T02:27:17.314Z:
Received chunk: .
...

2025-04-02T02:27:17.401Z:
Received chunk: 3...

2025-04-02T02:27:17.493Z:
Received chunk: ....

2025-04-02T02:27:17.580Z:
Received chunk:  **...

2025-04-02T02:27:17.668Z:
Received chunk: Cook...

2025-04-02T02:27:17.757Z:
Received chunk:  the...

2025-04-02T02:27:17.845Z:
Received chunk:  chicken...

2025-04-02T02:27:17.939Z:
Received chunk: **:...

2025-04-02T02:27:18.037Z:
Received chunk:  Add...

2025-04-02T02:27:18.125Z:
Received chunk:  chicken...

2025-04-02T02:27:18.213Z:
Received chunk:  breast...

2025-04-02T02:27:18.300Z:
Received chunk:  (...

2025-04-02T02:27:18.387Z:
Received chunk: or...

2025-04-02T02:27:18.477Z:
Received chunk:  tofu...

2025-04-02T02:27:18.564Z:
Received chunk: )...

2025-04-02T02:27:18.650Z:
Received chunk:  to...

2025-04-02T02:27:18.737Z:
Received chunk:  the...

2025-04-02T02:27:18.834Z:
Received chunk:  pan...

2025-04-02T02:27:18.917Z:
Received chunk: ....

2025-04-02T02:27:19.012Z:
Received chunk:  Cook...

2025-04-02T02:27:19.104Z:
Received chunk:  until...

2025-04-02T02:27:19.191Z:
Received chunk:  fully...

2025-04-02T02:27:19.279Z:
Received chunk:  cooked...

2025-04-02T02:27:19.368Z:
Received chunk: ,...

2025-04-02T02:27:19.456Z:
Received chunk:  about...

2025-04-02T02:27:19.545Z:
Received chunk:  ...

2025-04-02T02:27:19.635Z:
Received chunk: 5...

2025-04-02T02:27:19.725Z:
Received chunk: -...

2025-04-02T02:27:19.816Z:
Received chunk: 7...

2025-04-02T02:27:19.909Z:
Received chunk:  minutes...

2025-04-02T02:27:20.008Z:
Received chunk:  depending...

2025-04-02T02:27:20.101Z:
Received chunk:  on...

2025-04-02T02:27:20.191Z:
Received chunk:  thickness...

2025-04-02T02:27:20.280Z:
Received chunk: .
...

2025-04-02T02:27:20.369Z:
Received chunk: 4...

2025-04-02T02:27:20.456Z:
Received chunk: ....

2025-04-02T02:27:20.544Z:
Received chunk:  **...

2025-04-02T02:27:20.631Z:
Received chunk: Add...

2025-04-02T02:27:20.719Z:
Received chunk:  vegetables...

2025-04-02T02:27:20.805Z:
Received chunk: **:...

2025-04-02T02:27:20.895Z:
Received chunk:  Add...

2025-04-02T02:27:20.987Z:
Received chunk:  broccoli...

2025-04-02T02:27:21.081Z:
Received chunk: ,...

2025-04-02T02:27:21.168Z:
Received chunk:  carrots...

2025-04-02T02:27:21.256Z:
Received chunk: ,...

2025-04-02T02:27:21.344Z:
Received chunk:  bell...

2025-04-02T02:27:21.432Z:
Received chunk:  peppers...

2025-04-02T02:27:21.521Z:
Received chunk: ,...

2025-04-02T02:27:21.610Z:
Received chunk:  and...

2025-04-02T02:27:21.697Z:
Received chunk:  snap...

2025-04-02T02:27:21.784Z:
Received chunk:  peas...

2025-04-02T02:27:21.873Z:
Received chunk: ....

2025-04-02T02:27:21.963Z:
Received chunk:  T...

2025-04-02T02:27:22.057Z:
Received chunk: oss...

2025-04-02T02:27:22.145Z:
Received chunk:  everything...

2025-04-02T02:27:22.232Z:
Received chunk:  together...

2025-04-02T02:27:22.319Z:
Received chunk:  until...

2025-04-02T02:27:22.407Z:
Received chunk:  tender...

2025-04-02T02:27:22.494Z:
Received chunk: -c...

2025-04-02T02:27:22.582Z:
Received chunk: ris...

2025-04-02T02:27:22.671Z:
Received chunk: p...

2025-04-02T02:27:22.760Z:
Received chunk: .
...

2025-04-02T02:27:22.851Z:
Received chunk: 5...

2025-04-02T02:27:22.938Z:
Received chunk: ....

2025-04-02T02:27:23.031Z:
Received chunk:  **...

2025-04-02T02:27:23.121Z:
Received chunk: Season...

2025-04-02T02:27:23.209Z:
Received chunk:  with...

2025-04-02T02:27:23.299Z:
Received chunk:  sauce...

2025-04-02T02:27:23.388Z:
Received chunk: **:...

2025-04-02T02:27:23.477Z:
Received chunk:  Pour...

2025-04-02T02:27:23.565Z:
Received chunk:  soy...

2025-04-02T02:27:23.653Z:
Received chunk:  sauce...

2025-04-02T02:27:23.740Z:
Received chunk:  over...

2025-04-02T02:27:23.827Z:
Received chunk:  the...

2025-04-02T02:27:23.918Z:
Received chunk:  chicken...

2025-04-02T02:27:24.012Z:
Received chunk:  and...

2025-04-02T02:27:24.104Z:
Received chunk:  vegetables...

2025-04-02T02:27:24.191Z:
Received chunk: ....

2025-04-02T02:27:24.280Z:
Received chunk:  Stir...

2025-04-02T02:27:24.367Z:
Received chunk:  well...

2025-04-02T02:27:24.455Z:
Received chunk:  to...

2025-04-02T02:27:24.543Z:
Received chunk:  coat...

2025-04-02T02:27:24.633Z:
Received chunk:  evenly...

2025-04-02T02:27:24.719Z:
Received chunk: ....

2025-04-02T02:27:24.808Z:
Received chunk:  Adjust...

2025-04-02T02:27:24.898Z:
Received chunk:  salt...

2025-04-02T02:27:24.989Z:
Received chunk:  and...

2025-04-02T02:27:25.082Z:
Received chunk:  pepper...

2025-04-02T02:27:25.171Z:
Received chunk:  if...

2025-04-02T02:27:25.261Z:
Received chunk:  needed...

2025-04-02T02:27:25.349Z:
Received chunk: .
...

2025-04-02T02:27:25.437Z:
Received chunk: 6...

2025-04-02T02:27:25.525Z:
Received chunk: ....

2025-04-02T02:27:25.614Z:
Received chunk:  **...

2025-04-02T02:27:25.702Z:
Received chunk: Combine...

2025-04-02T02:27:25.790Z:
Received chunk:  and...

2025-04-02T02:27:25.880Z:
Received chunk:  serve...

2025-04-02T02:27:25.969Z:
Received chunk: **:...

2025-04-02T02:27:26.063Z:
Received chunk:  Mix...

2025-04-02T02:27:26.154Z:
Received chunk:  cooked...

2025-04-02T02:27:26.241Z:
Received chunk:  rice...

2025-04-02T02:27:26.329Z:
Received chunk:  with...

2025-04-02T02:27:26.418Z:
Received chunk:  the...

2025-04-02T02:27:26.506Z:
Received chunk:  stir...

2025-04-02T02:27:26.596Z:
Received chunk: -f...

2025-04-02T02:27:26.685Z:
Received chunk: ry...

2025-04-02T02:27:26.773Z:
Received chunk:  ingredients...

2025-04-02T02:27:26.866Z:
Received chunk: ....

2025-04-02T02:27:26.951Z:
Received chunk:  Serve...

2025-04-02T02:27:27.046Z:
Received chunk:  hot...

2025-04-02T02:27:27.138Z:
Received chunk: .

...

2025-04-02T02:27:27.225Z:
Received chunk: Enjoy...

2025-04-02T02:27:27.313Z:
Received chunk:  your...

2025-04-02T02:27:27.401Z:
Received chunk:  quick...

2025-04-02T02:27:27.489Z:
Received chunk:  and...

2025-04-02T02:27:27.577Z:
Received chunk:  delicious...

2025-04-02T02:27:27.665Z:
Received chunk:  Chicken...

2025-04-02T02:27:27.752Z:
Received chunk:  Stir...

2025-04-02T02:27:27.841Z:
Received chunk: -F...

2025-04-02T02:27:27.931Z:
Received chunk: ry...

2025-04-02T02:27:28.025Z:
Received chunk:  with...

2025-04-02T02:27:28.118Z:
Received chunk:  Bro...

2025-04-02T02:27:28.205Z:
Received chunk: ccoli...

2025-04-02T02:27:28.301Z:
Received chunk: !...

2025-04-02T02:27:28.388Z:
Received chunk: 

> ...

2025-04-02T02:29:56.116Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:29:56.116Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:29:56.117Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:29:56.117Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:29:56.846Z:
Unloading model...

2025-04-02T02:29:56.846Z:
Model unloaded successfully

2025-04-02T02:29:56.846Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:29:56.847Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:29:56.847Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:29:56.847Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:30:03.321Z:
Generate response called with message: give me a recipe...

2025-04-02T02:30:03.321Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:30:03.349Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:30:03.373Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:30:03.396Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:30:03.405Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:30:03.427Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:30:03.482Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:30:03.482Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:30:03.499Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0


2025-04-02T02:30:03.499Z:
Process stderr: print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:30:03.729Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:30:03.752Z:
Process stderr: .

2025-04-02T02:30:03.771Z:
Process stderr: .

2025-04-02T02:30:03.791Z:
Process stderr: .

2025-04-02T02:30:03.810Z:
Process stderr: .

2025-04-02T02:30:03.830Z:
Process stderr: .

2025-04-02T02:30:03.849Z:
Process stderr: .

2025-04-02T02:30:03.870Z:
Process stderr: .

2025-04-02T02:30:03.902Z:
Process stderr: .

2025-04-02T02:30:03.909Z:
Process stderr: .

2025-04-02T02:30:03.941Z:
Process stderr: .

2025-04-02T02:30:03.957Z:
Process stderr: .

2025-04-02T02:30:03.980Z:
Process stderr: .

2025-04-02T02:30:03.996Z:
Process stderr: .

2025-04-02T02:30:04.019Z:
Process stderr: .

2025-04-02T02:30:04.035Z:
Process stderr: .

2025-04-02T02:30:04.051Z:
Process stderr: .

2025-04-02T02:30:04.075Z:
Process stderr: .

2025-04-02T02:30:04.090Z:
Process stderr: .

2025-04-02T02:30:04.106Z:
Process stderr: .

2025-04-02T02:30:04.130Z:
Process stderr: .

2025-04-02T02:30:04.146Z:
Process stderr: .

2025-04-02T02:30:04.169Z:
Process stderr: .

2025-04-02T02:30:04.184Z:
Process stderr: .

2025-04-02T02:30:04.204Z:
Process stderr: .

2025-04-02T02:30:04.228Z:
Process stderr: .

2025-04-02T02:30:04.256Z:
Process stderr: .

2025-04-02T02:30:04.279Z:
Process stderr: .

2025-04-02T02:30:04.296Z:
Process stderr: .

2025-04-02T02:30:04.321Z:
Process stderr: .

2025-04-02T02:30:04.337Z:
Process stderr: .

2025-04-02T02:30:04.355Z:
Process stderr: .

2025-04-02T02:30:04.380Z:
Process stderr: .

2025-04-02T02:30:04.396Z:
Process stderr: .

2025-04-02T02:30:04.412Z:
Process stderr: .

2025-04-02T02:30:04.435Z:
Process stderr: .

2025-04-02T02:30:04.461Z:
Process stderr: .

2025-04-02T02:30:04.484Z:
Process stderr: .

2025-04-02T02:30:04.499Z:
Process stderr: .

2025-04-02T02:30:04.519Z:
Process stderr: .

2025-04-02T02:30:04.539Z:
Process stderr: .

2025-04-02T02:30:04.570Z:
Process stderr: .

2025-04-02T02:30:04.577Z:
Process stderr: .

2025-04-02T02:30:04.610Z:
Process stderr: .

2025-04-02T02:30:04.617Z:
Process stderr: .

2025-04-02T02:30:04.649Z:
Process stderr: .

2025-04-02T02:30:04.666Z:
Process stderr: .

2025-04-02T02:30:04.690Z:
Process stderr: .

2025-04-02T02:30:04.706Z:
Process stderr: .

2025-04-02T02:30:04.722Z:
Process stderr: .

2025-04-02T02:30:04.746Z:
Process stderr: .

2025-04-02T02:30:04.763Z:
Process stderr: .

2025-04-02T02:30:04.786Z:
Process stderr: .

2025-04-02T02:30:04.803Z:
Process stderr: .

2025-04-02T02:30:04.819Z:
Process stderr: .

2025-04-02T02:30:04.842Z:
Process stderr: .

2025-04-02T02:30:04.857Z:
Process stderr: .

2025-04-02T02:30:04.876Z:
Process stderr: .

2025-04-02T02:30:04.896Z:
Process stderr: .

2025-04-02T02:30:04.916Z:
Process stderr: .

2025-04-02T02:30:04.938Z:
Process stderr: .

2025-04-02T02:30:04.959Z:
Process stderr: .

2025-04-02T02:30:05.001Z:
Process stderr: .

2025-04-02T02:30:05.008Z:
Process stderr: .

2025-04-02T02:30:05.045Z:
Process stderr: .

2025-04-02T02:30:05.053Z:
Process stderr: .

2025-04-02T02:30:05.089Z:
Process stderr: ....

2025-04-02T02:30:05.089Z:
Process stderr: .........

2025-04-02T02:30:05.089Z:
Process stderr: .....


2025-04-02T02:30:05.092Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:30:05.092Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:30:05.110Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:30:05.115Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:30:05.381Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T02:30:05.382Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:30:05.383Z:
Process stderr: sampler seed: 1757886541
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:33:58.573Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:33:58.573Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:33:58.573Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:33:58.574Z:
Model mmproj-model-f16 file verified

2025-04-02T02:33:59.102Z:
Unloading model...

2025-04-02T02:33:59.102Z:
Model unloaded successfully

2025-04-02T02:33:59.103Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:33:59.103Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:33:59.103Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:33:59.103Z:
Model mmproj-model-f16 file verified

2025-04-02T02:34:03.999Z:
Generate response called with message: tell me a quick story...

2025-04-02T02:34:04.000Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: tell me a quick story
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:34:04.025Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:34:04.026Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)


2025-04-02T02:34:04.026Z:
Process stderr: print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T02:34:04.027Z:
Process exited with code 1

2025-04-02T02:34:04.028Z:
Process failed, falling back to simulation

2025-04-02T02:34:04.028Z:
Generating simulated response

2025-04-02T02:34:04.130Z:
Sent simulated chunk: I'm

2025-04-02T02:34:04.222Z:
Sent simulated chunk: a

2025-04-02T02:34:04.316Z:
Sent simulated chunk: locally

2025-04-02T02:34:04.395Z:
Sent simulated chunk: hosted

2025-04-02T02:34:04.457Z:
Sent simulated chunk: AI

2025-04-02T02:34:04.552Z:
Sent simulated chunk: assistant

2025-04-02T02:34:04.615Z:
Sent simulated chunk: running

2025-04-02T02:34:04.692Z:
Sent simulated chunk: directly

2025-04-02T02:34:04.740Z:
Sent simulated chunk: on

2025-04-02T02:34:04.802Z:
Sent simulated chunk: your

2025-04-02T02:34:04.894Z:
Sent simulated chunk: device.

2025-04-02T02:34:04.986Z:
Sent simulated chunk: I

2025-04-02T02:34:05.095Z:
Sent simulated chunk: process

2025-04-02T02:34:05.141Z:
Sent simulated chunk: information

2025-04-02T02:34:05.219Z:
Sent simulated chunk: using

2025-04-02T02:34:05.314Z:
Sent simulated chunk: the

2025-04-02T02:34:05.391Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:34:05.457Z:
Sent simulated chunk: model

2025-04-02T02:34:05.532Z:
Sent simulated chunk: loaded

2025-04-02T02:34:05.572Z:
Sent simulated chunk: in

2025-04-02T02:34:05.626Z:
Sent simulated chunk: the

2025-04-02T02:34:05.673Z:
Sent simulated chunk: LM

2025-04-02T02:34:05.752Z:
Sent simulated chunk: Terminal

2025-04-02T02:34:05.815Z:
Sent simulated chunk: application.

2025-04-02T02:34:05.876Z:
Sent simulated chunk: I've

2025-04-02T02:34:05.955Z:
Sent simulated chunk: received

2025-04-02T02:34:06.016Z:
Sent simulated chunk: your

2025-04-02T02:34:06.077Z:
Sent simulated chunk: message:

2025-04-02T02:34:06.156Z:
Sent simulated chunk: "tell

2025-04-02T02:34:06.250Z:
Sent simulated chunk: me

2025-04-02T02:34:06.342Z:
Sent simulated chunk: a

2025-04-02T02:34:06.449Z:
Sent simulated chunk: quick

2025-04-02T02:34:06.529Z:
Sent simulated chunk: story"

2025-04-02T02:34:06.637Z:
Sent simulated chunk: and

2025-04-02T02:34:06.730Z:
Sent simulated chunk: am

2025-04-02T02:34:06.823Z:
Sent simulated chunk: responding

2025-04-02T02:34:06.915Z:
Sent simulated chunk: without

2025-04-02T02:34:06.993Z:
Sent simulated chunk: requiring

2025-04-02T02:34:07.086Z:
Sent simulated chunk: internet

2025-04-02T02:34:07.195Z:
Sent simulated chunk: access.

(This

2025-04-02T02:34:07.272Z:
Sent simulated chunk: response

2025-04-02T02:34:07.350Z:
Sent simulated chunk: was

2025-04-02T02:34:07.396Z:
Sent simulated chunk: generated

2025-04-02T02:34:07.478Z:
Sent simulated chunk: using

2025-04-02T02:34:07.551Z:
Sent simulated chunk: a

2025-04-02T02:34:07.583Z:
Sent simulated chunk: simulated

2025-04-02T02:34:07.675Z:
Sent simulated chunk: version

2025-04-02T02:34:07.769Z:
Sent simulated chunk: of

2025-04-02T02:34:07.800Z:
Sent simulated chunk: the

2025-04-02T02:34:07.894Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:34:07.956Z:
Sent simulated chunk: model)

2025-04-02T02:34:07.956Z:
Simulated response generation completed

2025-04-02T02:34:14.541Z:
Generate response called with message: quick cooking recipe...

2025-04-02T02:34:14.541Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: quick cooking recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:34:14.565Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:34:14.566Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-02T02:34:14.566Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T02:34:14.567Z:
Process exited with code 1

2025-04-02T02:34:14.567Z:
Process failed, falling back to simulation

2025-04-02T02:34:14.567Z:
Generating simulated response

2025-04-02T02:34:14.628Z:
Sent simulated chunk: I'm

2025-04-02T02:34:14.706Z:
Sent simulated chunk: a

2025-04-02T02:34:14.753Z:
Sent simulated chunk: locally

2025-04-02T02:34:14.814Z:
Sent simulated chunk: hosted

2025-04-02T02:34:14.862Z:
Sent simulated chunk: AI

2025-04-02T02:34:14.909Z:
Sent simulated chunk: assistant

2025-04-02T02:34:14.955Z:
Sent simulated chunk: running

2025-04-02T02:34:15.032Z:
Sent simulated chunk: directly

2025-04-02T02:34:15.094Z:
Sent simulated chunk: on

2025-04-02T02:34:15.141Z:
Sent simulated chunk: your

2025-04-02T02:34:15.219Z:
Sent simulated chunk: device.

2025-04-02T02:34:15.294Z:
Sent simulated chunk: I

2025-04-02T02:34:15.325Z:
Sent simulated chunk: process

2025-04-02T02:34:15.418Z:
Sent simulated chunk: information

2025-04-02T02:34:15.512Z:
Sent simulated chunk: using

2025-04-02T02:34:15.606Z:
Sent simulated chunk: the

2025-04-02T02:34:15.700Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:34:15.776Z:
Sent simulated chunk: model

2025-04-02T02:34:15.822Z:
Sent simulated chunk: loaded

2025-04-02T02:34:15.917Z:
Sent simulated chunk: in

2025-04-02T02:34:15.995Z:
Sent simulated chunk: the

2025-04-02T02:34:16.027Z:
Sent simulated chunk: LM

2025-04-02T02:34:16.103Z:
Sent simulated chunk: Terminal

2025-04-02T02:34:16.197Z:
Sent simulated chunk: application.

2025-04-02T02:34:16.292Z:
Sent simulated chunk: I've

2025-04-02T02:34:16.352Z:
Sent simulated chunk: received

2025-04-02T02:34:16.463Z:
Sent simulated chunk: your

2025-04-02T02:34:16.577Z:
Sent simulated chunk: message:

2025-04-02T02:34:16.649Z:
Sent simulated chunk: "quick

2025-04-02T02:34:16.726Z:
Sent simulated chunk: cooking

2025-04-02T02:34:16.835Z:
Sent simulated chunk: recipe"

2025-04-02T02:34:16.881Z:
Sent simulated chunk: and

2025-04-02T02:34:16.989Z:
Sent simulated chunk: am

2025-04-02T02:34:17.066Z:
Sent simulated chunk: responding

2025-04-02T02:34:17.159Z:
Sent simulated chunk: without

2025-04-02T02:34:17.253Z:
Sent simulated chunk: requiring

2025-04-02T02:34:17.317Z:
Sent simulated chunk: internet

2025-04-02T02:34:17.394Z:
Sent simulated chunk: access.

(This

2025-04-02T02:34:17.441Z:
Sent simulated chunk: response

2025-04-02T02:34:17.489Z:
Sent simulated chunk: was

2025-04-02T02:34:17.587Z:
Sent simulated chunk: generated

2025-04-02T02:34:17.645Z:
Sent simulated chunk: using

2025-04-02T02:34:17.691Z:
Sent simulated chunk: a

2025-04-02T02:34:17.770Z:
Sent simulated chunk: simulated

2025-04-02T02:34:17.817Z:
Sent simulated chunk: version

2025-04-02T02:34:17.865Z:
Sent simulated chunk: of

2025-04-02T02:34:17.941Z:
Sent simulated chunk: the

2025-04-02T02:34:18.034Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:34:18.080Z:
Sent simulated chunk: model)

2025-04-02T02:34:18.080Z:
Simulated response generation completed

2025-04-02T02:46:25.280Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:46:25.280Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:46:25.280Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:46:25.281Z:
Model mmproj-model-f16 file verified

2025-04-02T02:46:25.663Z:
Unloading model...

2025-04-02T02:46:25.663Z:
Model unloaded successfully

2025-04-02T02:46:25.663Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:46:25.664Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:46:25.664Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T02:46:25.664Z:
Model mmproj-model-f16 file verified

2025-04-02T02:46:31.936Z:
Generate response called with message: short response please...

2025-04-02T02:46:31.936Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: short response please
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:46:31.962Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:46:31.963Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors


2025-04-02T02:46:31.963Z:
Process stderr: llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T02:46:31.964Z:
Process exited with code 1

2025-04-02T02:46:31.964Z:
Process failed, falling back to simulation

2025-04-02T02:46:31.965Z:
Generating simulated response

2025-04-02T02:46:32.023Z:
Sent simulated chunk: I'm

2025-04-02T02:46:32.102Z:
Sent simulated chunk: a

2025-04-02T02:46:32.149Z:
Sent simulated chunk: locally

2025-04-02T02:46:32.211Z:
Sent simulated chunk: hosted

2025-04-02T02:46:32.288Z:
Sent simulated chunk: AI

2025-04-02T02:46:32.366Z:
Sent simulated chunk: assistant

2025-04-02T02:46:32.415Z:
Sent simulated chunk: running

2025-04-02T02:46:32.523Z:
Sent simulated chunk: directly

2025-04-02T02:46:32.631Z:
Sent simulated chunk: on

2025-04-02T02:46:32.708Z:
Sent simulated chunk: your

2025-04-02T02:46:32.774Z:
Sent simulated chunk: device.

2025-04-02T02:46:32.836Z:
Sent simulated chunk: I

2025-04-02T02:46:32.925Z:
Sent simulated chunk: process

2025-04-02T02:46:33.019Z:
Sent simulated chunk: information

2025-04-02T02:46:33.127Z:
Sent simulated chunk: using

2025-04-02T02:46:33.188Z:
Sent simulated chunk: the

2025-04-02T02:46:33.265Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:46:33.342Z:
Sent simulated chunk: model

2025-04-02T02:46:33.388Z:
Sent simulated chunk: loaded

2025-04-02T02:46:33.450Z:
Sent simulated chunk: in

2025-04-02T02:46:33.513Z:
Sent simulated chunk: the

2025-04-02T02:46:33.623Z:
Sent simulated chunk: LM

2025-04-02T02:46:33.716Z:
Sent simulated chunk: Terminal

2025-04-02T02:46:33.810Z:
Sent simulated chunk: application.

2025-04-02T02:46:33.892Z:
Sent simulated chunk: I've

2025-04-02T02:46:33.995Z:
Sent simulated chunk: received

2025-04-02T02:46:34.057Z:
Sent simulated chunk: your

2025-04-02T02:46:34.103Z:
Sent simulated chunk: message:

2025-04-02T02:46:34.181Z:
Sent simulated chunk: "short

2025-04-02T02:46:34.227Z:
Sent simulated chunk: response

2025-04-02T02:46:34.306Z:
Sent simulated chunk: please"

2025-04-02T02:46:34.353Z:
Sent simulated chunk: and

2025-04-02T02:46:34.461Z:
Sent simulated chunk: am

2025-04-02T02:46:34.523Z:
Sent simulated chunk: responding

2025-04-02T02:46:34.630Z:
Sent simulated chunk: without

2025-04-02T02:46:34.709Z:
Sent simulated chunk: requiring

2025-04-02T02:46:34.803Z:
Sent simulated chunk: internet

2025-04-02T02:46:34.898Z:
Sent simulated chunk: access.

(This

2025-04-02T02:46:34.960Z:
Sent simulated chunk: response

2025-04-02T02:46:34.991Z:
Sent simulated chunk: was

2025-04-02T02:46:35.068Z:
Sent simulated chunk: generated

2025-04-02T02:46:35.129Z:
Sent simulated chunk: using

2025-04-02T02:46:35.209Z:
Sent simulated chunk: a

2025-04-02T02:46:35.273Z:
Sent simulated chunk: simulated

2025-04-02T02:46:35.348Z:
Sent simulated chunk: version

2025-04-02T02:46:35.426Z:
Sent simulated chunk: of

2025-04-02T02:46:35.473Z:
Sent simulated chunk: the

2025-04-02T02:46:35.551Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T02:46:35.658Z:
Sent simulated chunk: model)

2025-04-02T02:46:35.659Z:
Simulated response generation completed

2025-04-02T02:49:21.632Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:49:21.633Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:49:21.633Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:49:21.633Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:49:22.713Z:
Unloading model...

2025-04-02T02:49:22.713Z:
Model unloaded successfully

2025-04-02T02:49:22.713Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:49:22.714Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:49:22.714Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:49:22.714Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:49:27.054Z:
Generate response called with message: MAKE ME A QUICK RECIPE...

2025-04-02T02:49:27.055Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: MAKE ME A QUICK RECIPE
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:49:27.080Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:49:27.104Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:49:27.126Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:49:27.135Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:49:27.158Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:49:27.210Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:49:27.211Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:49:27.228Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0


2025-04-02T02:49:27.228Z:
Process stderr: print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:49:27.465Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:49:27.487Z:
Process stderr: .

2025-04-02T02:49:27.507Z:
Process stderr: .

2025-04-02T02:49:27.527Z:
Process stderr: .

2025-04-02T02:49:27.546Z:
Process stderr: .

2025-04-02T02:49:27.569Z:
Process stderr: .

2025-04-02T02:49:27.585Z:
Process stderr: .

2025-04-02T02:49:27.606Z:
Process stderr: .

2025-04-02T02:49:27.638Z:
Process stderr: .

2025-04-02T02:49:27.645Z:
Process stderr: .

2025-04-02T02:49:27.677Z:
Process stderr: .

2025-04-02T02:49:27.693Z:
Process stderr: .

2025-04-02T02:49:27.715Z:
Process stderr: .

2025-04-02T02:49:27.731Z:
Process stderr: .

2025-04-02T02:49:27.753Z:
Process stderr: .

2025-04-02T02:49:27.769Z:
Process stderr: .

2025-04-02T02:49:27.785Z:
Process stderr: .

2025-04-02T02:49:27.808Z:
Process stderr: .

2025-04-02T02:49:27.823Z:
Process stderr: .

2025-04-02T02:49:27.839Z:
Process stderr: .

2025-04-02T02:49:27.861Z:
Process stderr: .

2025-04-02T02:49:27.877Z:
Process stderr: .

2025-04-02T02:49:27.900Z:
Process stderr: .

2025-04-02T02:49:27.915Z:
Process stderr: .

2025-04-02T02:49:27.935Z:
Process stderr: .

2025-04-02T02:49:27.960Z:
Process stderr: .

2025-04-02T02:49:27.991Z:
Process stderr: .

2025-04-02T02:49:28.014Z:
Process stderr: .

2025-04-02T02:49:28.030Z:
Process stderr: .

2025-04-02T02:49:28.053Z:
Process stderr: .

2025-04-02T02:49:28.068Z:
Process stderr: .

2025-04-02T02:49:28.085Z:
Process stderr: .

2025-04-02T02:49:28.107Z:
Process stderr: .

2025-04-02T02:49:28.123Z:
Process stderr: .

2025-04-02T02:49:28.139Z:
Process stderr: .

2025-04-02T02:49:28.161Z:
Process stderr: .

2025-04-02T02:49:28.177Z:
Process stderr: .

2025-04-02T02:49:28.200Z:
Process stderr: .

2025-04-02T02:49:28.215Z:
Process stderr: .

2025-04-02T02:49:28.234Z:
Process stderr: .

2025-04-02T02:49:28.255Z:
Process stderr: .

2025-04-02T02:49:28.287Z:
Process stderr: .

2025-04-02T02:49:28.294Z:
Process stderr: .

2025-04-02T02:49:28.327Z:
Process stderr: .

2025-04-02T02:49:28.335Z:
Process stderr: .

2025-04-02T02:49:28.367Z:
Process stderr: .

2025-04-02T02:49:28.384Z:
Process stderr: .

2025-04-02T02:49:28.407Z:
Process stderr: .

2025-04-02T02:49:28.423Z:
Process stderr: .

2025-04-02T02:49:28.440Z:
Process stderr: .

2025-04-02T02:49:28.465Z:
Process stderr: .

2025-04-02T02:49:28.483Z:
Process stderr: .

2025-04-02T02:49:28.508Z:
Process stderr: .

2025-04-02T02:49:28.521Z:
Process stderr: .

2025-04-02T02:49:28.539Z:
Process stderr: .

2025-04-02T02:49:28.565Z:
Process stderr: .

2025-04-02T02:49:28.583Z:
Process stderr: .

2025-04-02T02:49:28.605Z:
Process stderr: .

2025-04-02T02:49:28.628Z:
Process stderr: .

2025-04-02T02:49:28.650Z:
Process stderr: .

2025-04-02T02:49:28.672Z:
Process stderr: .

2025-04-02T02:49:28.713Z:
Process stderr: .

2025-04-02T02:49:28.754Z:
Process stderr: .

2025-04-02T02:49:28.763Z:
Process stderr: .

2025-04-02T02:49:28.808Z:
Process stderr: .

2025-04-02T02:49:28.815Z:
Process stderr: .

2025-04-02T02:49:28.855Z:
Process stderr: ....

2025-04-02T02:49:28.855Z:
Process stderr: .........

2025-04-02T02:49:28.856Z:
Process stderr: .....


2025-04-02T02:49:28.858Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:49:28.858Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:49:28.877Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:49:28.883Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:49:29.142Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:49:29.143Z:
Process stderr: sampler seed: 4272036317
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:49:29.371Z:
Received chunk: <think>...

2025-04-02T02:49:29.458Z:
Received chunk: 
...

2025-04-02T02:49:29.556Z:
Received chunk: Alright...

2025-04-02T02:49:29.634Z:
Received chunk: ,...

2025-04-02T02:49:29.720Z:
Received chunk:  the...

2025-04-02T02:49:29.806Z:
Received chunk:  user...

2025-04-02T02:49:29.890Z:
Received chunk:  just...

2025-04-02T02:49:29.972Z:
Received chunk:  asked...

2025-04-02T02:49:30.054Z:
Received chunk:  for...

2025-04-02T02:49:30.137Z:
Received chunk:  a...

2025-04-02T02:49:30.219Z:
Received chunk:  quick...

2025-04-02T02:49:30.302Z:
Received chunk:  recipe...

2025-04-02T02:49:30.385Z:
Received chunk: ....

2025-04-02T02:49:30.467Z:
Received chunk:  I...

2025-04-02T02:49:30.550Z:
Received chunk:  need...

2025-04-02T02:49:30.639Z:
Received chunk:  to...

2025-04-02T02:49:30.720Z:
Received chunk:  figure...

2025-04-02T02:49:30.803Z:
Received chunk:  out...

2025-04-02T02:49:30.885Z:
Received chunk:  what...

2025-04-02T02:49:30.968Z:
Received chunk:  they...

2025-04-02T02:49:31.051Z:
Received chunk: 're...

2025-04-02T02:49:31.136Z:
Received chunk:  looking...

2025-04-02T02:49:31.218Z:
Received chunk:  for...

2025-04-02T02:49:31.300Z:
Received chunk: .

...

2025-04-02T02:49:31.381Z:
Received chunk: They...

2025-04-02T02:49:31.465Z:
Received chunk:  didn...

2025-04-02T02:49:31.549Z:
Received chunk: 't...

2025-04-02T02:49:31.640Z:
Received chunk:  specify...

2025-04-02T02:49:31.721Z:
Received chunk:  the...

2025-04-02T02:49:31.804Z:
Received chunk:  cuisine...

2025-04-02T02:49:31.884Z:
Received chunk:  or...

2025-04-02T02:49:31.966Z:
Received chunk:  type...

2025-04-02T02:49:32.047Z:
Received chunk:  of...

2025-04-02T02:49:32.131Z:
Received chunk:  meal...

2025-04-02T02:49:32.216Z:
Received chunk: —...

2025-04-02T02:49:32.299Z:
Received chunk: maybe...

2025-04-02T02:49:32.384Z:
Received chunk:  something...

2025-04-02T02:49:32.470Z:
Received chunk:  fast...

2025-04-02T02:49:32.554Z:
Received chunk:  and...

2025-04-02T02:49:32.643Z:
Received chunk:  delicious...

2025-04-02T02:49:32.731Z:
Received chunk: ?

...

2025-04-02T02:49:32.821Z:
Received chunk: I...

2025-04-02T02:49:32.903Z:
Received chunk:  should...

2025-04-02T02:49:32.988Z:
Received chunk:  consider...

2025-04-02T02:49:33.071Z:
Received chunk:  popular...

2025-04-02T02:49:33.155Z:
Received chunk:  quick...

2025-04-02T02:49:33.239Z:
Received chunk:  options...

2025-04-02T02:49:33.322Z:
Received chunk:  like...

2025-04-02T02:49:33.405Z:
Received chunk:  sandwiches...

2025-04-02T02:49:33.489Z:
Received chunk:  or...

2025-04-02T02:49:33.587Z:
Received chunk:  wraps...

2025-04-02T02:49:33.679Z:
Received chunk:  because...

2025-04-02T02:49:33.767Z:
Received chunk:  those...

2025-04-02T02:49:33.854Z:
Received chunk:  are...

2025-04-02T02:49:33.940Z:
Received chunk:  easy...

2025-04-02T02:49:34.025Z:
Received chunk:  to...

2025-04-02T02:49:34.122Z:
Received chunk:  make...

2025-04-02T02:49:34.206Z:
Received chunk:  and...

2025-04-02T02:49:34.291Z:
Received chunk:  can...

2025-04-02T02:49:34.380Z:
Received chunk:  be...

2025-04-02T02:49:34.471Z:
Received chunk:  done...

2025-04-02T02:49:34.563Z:
Received chunk:  quickly...

2025-04-02T02:49:34.652Z:
Received chunk: .

...

2025-04-02T02:49:34.741Z:
Received chunk: Let...

2025-04-02T02:49:34.828Z:
Received chunk: 's...

2025-04-02T02:49:34.914Z:
Received chunk:  go...

2025-04-02T02:49:34.998Z:
Received chunk:  with...

2025-04-02T02:49:35.083Z:
Received chunk:  a...

2025-04-02T02:49:35.171Z:
Received chunk:  Chicken...

2025-04-02T02:49:35.255Z:
Received chunk:  and...

2025-04-02T02:49:35.339Z:
Received chunk:  Av...

2025-04-02T02:49:35.425Z:
Received chunk: ocado...

2025-04-02T02:49:35.509Z:
Received chunk:  Wrap...

2025-04-02T02:49:35.595Z:
Received chunk:  since...

2025-04-02T02:49:35.683Z:
Received chunk:  it...

2025-04-02T02:49:35.770Z:
Received chunk: 's...

2025-04-02T02:49:35.855Z:
Received chunk:  trendy...

2025-04-02T02:49:35.940Z:
Received chunk: ,...

2025-04-02T02:49:36.026Z:
Received chunk:  healthy...

2025-04-02T02:49:36.111Z:
Received chunk: ,...

2025-04-02T02:49:36.196Z:
Received chunk:  and...

2025-04-02T02:49:36.279Z:
Received chunk:  customizable...

2025-04-02T02:49:36.363Z:
Received chunk: .

...

2025-04-02T02:50:13.013Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-02T02:50:13.014Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:50:13.014Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\gemma-3-4b-it-Q8_0.gguf

2025-04-02T02:50:13.014Z:
Model gemma-3-4b-it-Q8_0 file verified

2025-04-02T02:50:15.228Z:
Unloading model...

2025-04-02T02:50:15.228Z:
Model unloaded successfully

2025-04-02T02:50:15.228Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:50:15.229Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:50:15.229Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:50:15.229Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:50:20.745Z:
Generate response called with message: give me a quick recipe...

2025-04-02T02:50:20.745Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a quick recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:50:20.769Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:50:20.792Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:50:20.814Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:50:20.824Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:50:20.846Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:50:20.914Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:50:20.914Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:50:20.931Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944


2025-04-02T02:50:20.931Z:
Process stderr: print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:50:21.167Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:50:21.190Z:
Process stderr: .

2025-04-02T02:50:21.208Z:
Process stderr: .

2025-04-02T02:50:21.227Z:
Process stderr: .

2025-04-02T02:50:21.246Z:
Process stderr: .

2025-04-02T02:50:21.265Z:
Process stderr: .

2025-04-02T02:50:21.284Z:
Process stderr: .

2025-04-02T02:50:21.304Z:
Process stderr: .

2025-04-02T02:50:21.336Z:
Process stderr: .

2025-04-02T02:50:21.343Z:
Process stderr: .

2025-04-02T02:50:21.375Z:
Process stderr: .

2025-04-02T02:50:21.392Z:
Process stderr: .

2025-04-02T02:50:21.414Z:
Process stderr: .

2025-04-02T02:50:21.430Z:
Process stderr: .

2025-04-02T02:50:21.453Z:
Process stderr: .

2025-04-02T02:50:21.468Z:
Process stderr: .

2025-04-02T02:50:21.484Z:
Process stderr: .

2025-04-02T02:50:21.507Z:
Process stderr: .

2025-04-02T02:50:21.523Z:
Process stderr: .

2025-04-02T02:50:21.540Z:
Process stderr: .

2025-04-02T02:50:21.568Z:
Process stderr: .

2025-04-02T02:50:21.580Z:
Process stderr: .

2025-04-02T02:50:21.605Z:
Process stderr: .

2025-04-02T02:50:21.621Z:
Process stderr: .

2025-04-02T02:50:21.641Z:
Process stderr: .

2025-04-02T02:50:21.661Z:
Process stderr: .

2025-04-02T02:50:21.693Z:
Process stderr: .

2025-04-02T02:50:21.716Z:
Process stderr: .

2025-04-02T02:50:21.732Z:
Process stderr: .

2025-04-02T02:50:21.756Z:
Process stderr: .

2025-04-02T02:50:21.771Z:
Process stderr: .

2025-04-02T02:50:21.787Z:
Process stderr: .

2025-04-02T02:50:21.810Z:
Process stderr: .

2025-04-02T02:50:21.830Z:
Process stderr: .

2025-04-02T02:50:21.846Z:
Process stderr: .

2025-04-02T02:50:21.868Z:
Process stderr: .

2025-04-02T02:50:21.884Z:
Process stderr: .

2025-04-02T02:50:21.907Z:
Process stderr: .

2025-04-02T02:50:21.922Z:
Process stderr: .

2025-04-02T02:50:21.941Z:
Process stderr: .

2025-04-02T02:50:21.963Z:
Process stderr: .

2025-04-02T02:50:21.996Z:
Process stderr: .

2025-04-02T02:50:22.002Z:
Process stderr: .

2025-04-02T02:50:22.036Z:
Process stderr: .

2025-04-02T02:50:22.043Z:
Process stderr: .

2025-04-02T02:50:22.076Z:
Process stderr: .

2025-04-02T02:50:22.092Z:
Process stderr: .

2025-04-02T02:50:22.117Z:
Process stderr: .

2025-04-02T02:50:22.133Z:
Process stderr: .

2025-04-02T02:50:22.149Z:
Process stderr: .

2025-04-02T02:50:22.174Z:
Process stderr: .

2025-04-02T02:50:22.190Z:
Process stderr: .

2025-04-02T02:50:22.214Z:
Process stderr: .

2025-04-02T02:50:22.229Z:
Process stderr: .

2025-04-02T02:50:22.246Z:
Process stderr: .

2025-04-02T02:50:22.269Z:
Process stderr: .

2025-04-02T02:50:22.284Z:
Process stderr: .

2025-04-02T02:50:22.303Z:
Process stderr: .

2025-04-02T02:50:22.322Z:
Process stderr: .

2025-04-02T02:50:22.341Z:
Process stderr: .

2025-04-02T02:50:22.363Z:
Process stderr: .

2025-04-02T02:50:22.384Z:
Process stderr: .

2025-04-02T02:50:22.424Z:
Process stderr: .

2025-04-02T02:50:22.431Z:
Process stderr: .

2025-04-02T02:50:22.467Z:
Process stderr: .

2025-04-02T02:50:22.475Z:
Process stderr: .

2025-04-02T02:50:22.512Z:
Process stderr: ....

2025-04-02T02:50:22.512Z:
Process stderr: ........

2025-04-02T02:50:22.512Z:
Process stderr: .....

2025-04-02T02:50:22.512Z:
Process stderr: .


2025-04-02T02:50:22.514Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:50:22.514Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:50:22.533Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:50:22.538Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:50:22.812Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>



2025-04-02T02:50:22.813Z:
Process stderr: system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:50:22.813Z:
Process stderr: sampler seed: 2123516682
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:50:23.070Z:
Received chunk: <think>...

2025-04-02T02:50:23.159Z:
Received chunk: 
...

2025-04-02T02:50:23.247Z:
Received chunk: Alright...

2025-04-02T02:50:23.340Z:
Received chunk: ,...

2025-04-02T02:50:23.430Z:
Received chunk:  the...

2025-04-02T02:50:23.517Z:
Received chunk:  user...

2025-04-02T02:50:23.602Z:
Received chunk:  just...

2025-04-02T02:50:23.691Z:
Received chunk:  asked...

2025-04-02T02:50:23.779Z:
Received chunk:  for...

2025-04-02T02:50:23.864Z:
Received chunk:  a...

2025-04-02T02:50:23.948Z:
Received chunk:  quick...

2025-04-02T02:50:24.032Z:
Received chunk:  recipe...

2025-04-02T02:50:24.117Z:
Received chunk: ....

2025-04-02T02:50:24.201Z:
Received chunk:  I...

2025-04-02T02:50:24.286Z:
Received chunk:  need...

2025-04-02T02:50:24.370Z:
Received chunk:  to...

2025-04-02T02:50:24.453Z:
Received chunk:  figure...

2025-04-02T02:50:24.538Z:
Received chunk:  out...

2025-04-02T02:50:24.626Z:
Received chunk:  what...

2025-04-02T02:50:24.719Z:
Received chunk:  they...

2025-04-02T02:50:24.808Z:
Received chunk:  might...

2025-04-02T02:50:24.893Z:
Received chunk:  be...

2025-04-02T02:50:24.978Z:
Received chunk:  looking...

2025-04-02T02:50:25.064Z:
Received chunk:  for...

2025-04-02T02:50:25.151Z:
Received chunk: ....

2025-04-02T02:50:25.236Z:
Received chunk:  Since...

2025-04-02T02:50:25.320Z:
Received chunk:  it...

2025-04-02T02:50:25.404Z:
Received chunk: 's...

2025-04-02T02:50:25.488Z:
Received chunk:  so...

2025-04-02T02:50:25.573Z:
Received chunk:  broad...

2025-04-02T02:50:25.661Z:
Received chunk: ,...

2025-04-02T02:50:25.752Z:
Received chunk:  maybe...

2025-04-02T02:50:25.839Z:
Received chunk:  they...

2025-04-02T02:50:25.924Z:
Received chunk: 're...

2025-04-02T02:50:26.007Z:
Received chunk:  in...

2025-04-02T02:50:26.091Z:
Received chunk:  a...

2025-04-02T02:50:26.174Z:
Received chunk:  hurry...

2025-04-02T02:50:26.259Z:
Received chunk:  and...

2025-04-02T02:50:26.343Z:
Received chunk:  want...

2025-04-02T02:50:26.427Z:
Received chunk:  something...

2025-04-02T02:50:26.510Z:
Received chunk:  easy...

2025-04-02T02:50:26.603Z:
Received chunk:  but...

2025-04-02T02:50:26.682Z:
Received chunk:  still...

2025-04-02T02:50:26.772Z:
Received chunk:  tasty...

2025-04-02T02:50:26.861Z:
Received chunk: .

...

2025-04-02T02:50:26.946Z:
Received chunk: I...

2025-04-02T02:50:27.031Z:
Received chunk:  should...

2025-04-02T02:50:27.119Z:
Received chunk:  consider...

2025-04-02T02:50:27.206Z:
Received chunk:  common...

2025-04-02T02:50:27.293Z:
Received chunk:  quick...

2025-04-02T02:50:27.380Z:
Received chunk:  options...

2025-04-02T02:50:27.465Z:
Received chunk:  that...

2025-04-02T02:50:27.549Z:
Received chunk:  are...

2025-04-02T02:50:27.635Z:
Received chunk:  popular...

2025-04-02T02:50:27.723Z:
Received chunk:  and...

2025-04-02T02:50:27.813Z:
Received chunk:  balanced...

2025-04-02T02:50:27.899Z:
Received chunk:  between...

2025-04-02T02:50:27.984Z:
Received chunk:  time...

2025-04-02T02:50:28.068Z:
Received chunk:  and...

2025-04-02T02:50:28.153Z:
Received chunk:  flavor...

2025-04-02T02:50:28.238Z:
Received chunk: ....

2025-04-02T02:50:28.322Z:
Received chunk:  Pasta...

2025-04-02T02:50:28.406Z:
Received chunk:  with...

2025-04-02T02:50:28.490Z:
Received chunk:  marin...

2025-04-02T02:50:28.576Z:
Received chunk: ara...

2025-04-02T02:50:28.671Z:
Received chunk:  sauce...

2025-04-02T02:50:28.760Z:
Received chunk:  comes...

2025-04-02T02:50:28.850Z:
Received chunk:  to...

2025-04-02T02:50:28.934Z:
Received chunk:  mind...

2025-04-02T02:50:29.019Z:
Received chunk:  because...

2025-04-02T02:50:29.105Z:
Received chunk:  it...

2025-04-02T02:50:29.189Z:
Received chunk: 's...

2025-04-02T02:50:29.273Z:
Received chunk:  straightforward...

2025-04-02T02:50:29.357Z:
Received chunk:  and...

2025-04-02T02:50:29.439Z:
Received chunk:  can...

2025-04-02T02:50:29.523Z:
Received chunk:  be...

2025-04-02T02:50:29.608Z:
Received chunk:  prepared...

2025-04-02T02:50:29.695Z:
Received chunk:  quickly...

2025-04-02T02:50:29.785Z:
Received chunk: ....

2025-04-02T02:50:29.876Z:
Received chunk:  Plus...

2025-04-02T02:50:29.960Z:
Received chunk: ,...

2025-04-02T02:50:30.046Z:
Received chunk:  adding...

2025-04-02T02:50:30.130Z:
Received chunk:  some...

2025-04-02T02:50:30.215Z:
Received chunk:  veggies...

2025-04-02T02:50:30.299Z:
Received chunk:  like...

2025-04-02T02:50:30.383Z:
Received chunk:  z...

2025-04-02T02:50:30.467Z:
Received chunk: ucchini...

2025-04-02T02:50:30.552Z:
Received chunk:  and...

2025-04-02T02:50:30.640Z:
Received chunk:  mushrooms...

2025-04-02T02:50:30.724Z:
Received chunk:  adds...

2025-04-02T02:50:30.817Z:
Received chunk:  variety...

2025-04-02T02:50:30.905Z:
Received chunk:  without...

2025-04-02T02:50:30.989Z:
Received chunk:  taking...

2025-04-02T02:50:31.076Z:
Received chunk:  much...

2025-04-02T02:50:31.163Z:
Received chunk:  longer...

2025-04-02T02:50:31.249Z:
Received chunk: .

...

2025-04-02T02:50:31.334Z:
Received chunk: I...

2025-04-02T02:50:31.418Z:
Received chunk: 'll...

2025-04-02T02:50:31.503Z:
Received chunk:  make...

2025-04-02T02:50:31.586Z:
Received chunk:  a...

2025-04-02T02:50:31.673Z:
Received chunk:  simple...

2025-04-02T02:50:31.759Z:
Received chunk:  pasta...

2025-04-02T02:50:31.850Z:
Received chunk:  dish...

2025-04-02T02:50:31.936Z:
Received chunk:  with...

2025-04-02T02:50:32.022Z:
Received chunk:  ground...

2025-04-02T02:50:32.107Z:
Received chunk:  turkey...

2025-04-02T02:50:32.191Z:
Received chunk:  for...

2025-04-02T02:50:32.276Z:
Received chunk:  a...

2025-04-02T02:50:32.360Z:
Received chunk:  bit...

2025-04-02T02:50:32.444Z:
Received chunk:  of...

2025-04-02T02:50:32.529Z:
Received chunk:  meat...

2025-04-02T02:50:32.613Z:
Received chunk: iness...

2025-04-02T02:50:32.699Z:
Received chunk: ....

2025-04-02T02:50:32.786Z:
Received chunk:  Using...

2025-04-02T02:50:32.876Z:
Received chunk:  jar...

2025-04-02T02:50:32.961Z:
Received chunk: red...

2025-04-02T02:50:33.046Z:
Received chunk:  sauce...

2025-04-02T02:50:33.132Z:
Received chunk:  saves...

2025-04-02T02:50:33.218Z:
Received chunk:  time...

2025-04-02T02:50:33.302Z:
Received chunk:  too...

2025-04-02T02:50:33.385Z:
Received chunk: ....

2025-04-02T02:50:33.470Z:
Received chunk:  Season...

2025-04-02T02:50:33.554Z:
Received chunk: ing...

2025-04-02T02:50:33.638Z:
Received chunk:  it...

2025-04-02T02:50:33.724Z:
Received chunk:  with...

2025-04-02T02:50:33.812Z:
Received chunk:  garlic...

2025-04-02T02:50:33.900Z:
Received chunk:  powder...

2025-04-02T02:50:33.984Z:
Received chunk: ,...

2025-04-02T02:50:34.068Z:
Received chunk:  onion...

2025-04-02T02:50:34.152Z:
Received chunk:  powder...

2025-04-02T02:50:34.236Z:
Received chunk: ,...

2025-04-02T02:50:34.320Z:
Received chunk:  and...

2025-04-02T02:50:34.405Z:
Received chunk:  pap...

2025-04-02T02:50:34.489Z:
Received chunk: rika...

2025-04-02T02:50:34.578Z:
Received chunk:  will...

2025-04-02T02:50:34.666Z:
Received chunk:  give...

2025-04-02T02:50:34.751Z:
Received chunk:  it...

2025-04-02T02:50:34.842Z:
Received chunk:  some...

2025-04-02T02:50:34.929Z:
Received chunk:  flavor...

2025-04-02T02:50:35.012Z:
Received chunk:  without...

2025-04-02T02:50:35.097Z:
Received chunk:  needing...

2025-04-02T02:50:35.182Z:
Received chunk:  to...

2025-04-02T02:50:35.266Z:
Received chunk:  sa...

2025-04-02T02:50:35.350Z:
Received chunk: ut...

2025-04-02T02:50:35.434Z:
Received chunk: é...

2025-04-02T02:50:35.519Z:
Received chunk:  too...

2025-04-02T02:50:35.603Z:
Received chunk:  much...

2025-04-02T02:50:35.689Z:
Received chunk: .

...

2025-04-02T02:50:35.778Z:
Received chunk: Including...

2025-04-02T02:50:35.866Z:
Received chunk:  ste...

2025-04-02T02:50:35.953Z:
Received chunk: amed...

2025-04-02T02:50:36.038Z:
Received chunk:  broccoli...

2025-04-02T02:50:36.122Z:
Received chunk:  is...

2025-04-02T02:50:36.206Z:
Received chunk:  a...

2025-04-02T02:50:36.290Z:
Received chunk:  good...

2025-04-02T02:50:36.375Z:
Received chunk:  idea...

2025-04-02T02:50:36.458Z:
Received chunk:  because...

2025-04-02T02:50:36.542Z:
Received chunk:  it...

2025-04-02T02:50:36.627Z:
Received chunk: 's...

2025-04-02T02:50:36.713Z:
Received chunk:  quick...

2025-04-02T02:50:36.804Z:
Received chunk:  and...

2025-04-02T02:50:36.895Z:
Received chunk:  adds...

2025-04-02T02:50:36.983Z:
Received chunk:  nutrients...

2025-04-02T02:50:37.068Z:
Received chunk: ....

2025-04-02T02:50:37.153Z:
Received chunk:  I...

2025-04-02T02:50:37.238Z:
Received chunk:  should...

2025-04-02T02:50:37.322Z:
Received chunk:  also...

2025-04-02T02:50:37.406Z:
Received chunk:  mention...

2025-04-02T02:50:37.491Z:
Received chunk:  optional...

2025-04-02T02:50:37.575Z:
Received chunk:  additions...

2025-04-02T02:50:37.663Z:
Received chunk:  like...

2025-04-02T02:50:37.745Z:
Received chunk:  Parm...

2025-04-02T02:50:37.837Z:
Received chunk: esan...

2025-04-02T02:50:37.926Z:
Received chunk:  or...

2025-04-02T02:50:38.012Z:
Received chunk:  spinach...

2025-04-02T02:50:38.097Z:
Received chunk:  for...

2025-04-02T02:50:38.182Z:
Received chunk:  those...

2025-04-02T02:50:38.266Z:
Received chunk:  who...

2025-04-02T02:50:38.352Z:
Received chunk:  want...

2025-04-02T02:50:38.437Z:
Received chunk:  more...

2025-04-02T02:50:38.523Z:
Received chunk:  flavor...

2025-04-02T02:50:38.608Z:
Received chunk:  or...

2025-04-02T02:50:38.694Z:
Received chunk:  texture...

2025-04-02T02:50:38.780Z:
Received chunk: .

...

2025-04-02T02:50:38.871Z:
Received chunk: Finally...

2025-04-02T02:50:38.959Z:
Received chunk: ,...

2025-04-02T02:50:39.045Z:
Received chunk:  I...

2025-04-02T02:50:39.130Z:
Received chunk: 'll...

2025-04-02T02:50:39.214Z:
Received chunk:  outline...

2025-04-02T02:50:39.299Z:
Received chunk:  the...

2025-04-02T02:50:39.384Z:
Received chunk:  steps...

2025-04-02T02:50:39.470Z:
Received chunk:  clearly...

2025-04-02T02:50:39.555Z:
Received chunk:  so...

2025-04-02T02:50:39.643Z:
Received chunk:  they...

2025-04-02T02:50:39.729Z:
Received chunk:  can...

2025-04-02T02:50:39.819Z:
Received chunk:  follow...

2025-04-02T02:50:39.910Z:
Received chunk:  along...

2025-04-02T02:50:39.995Z:
Received chunk:  easily...

2025-04-02T02:50:40.083Z:
Received chunk: ....

2025-04-02T02:50:40.167Z:
Received chunk:  Making...

2025-04-02T02:50:40.253Z:
Received chunk:  sure...

2025-04-02T02:50:40.338Z:
Received chunk:  each...

2025-04-02T02:50:40.424Z:
Received chunk:  part...

2025-04-02T02:50:40.509Z:
Received chunk:  is...

2025-04-02T02:50:40.595Z:
Received chunk:  concise...

2025-04-02T02:50:40.687Z:
Received chunk:  but...

2025-04-02T02:50:40.767Z:
Received chunk:  provides...

2025-04-02T02:50:40.856Z:
Received chunk:  enough...

2025-04-02T02:50:40.947Z:
Received chunk:  detail...

2025-04-02T02:50:41.034Z:
Received chunk:  to...

2025-04-02T02:50:41.120Z:
Received chunk:  execute...

2025-04-02T02:50:41.205Z:
Received chunk:  the...

2025-04-02T02:50:41.290Z:
Received chunk:  recipe...

2025-04-02T02:50:41.375Z:
Received chunk:  without...

2025-04-02T02:50:41.459Z:
Received chunk:  confusion...

2025-04-02T02:50:41.543Z:
Received chunk: .
...

2025-04-02T02:50:41.627Z:
Received chunk: </think>...

2025-04-02T02:50:41.713Z:
Received chunk: 

...

2025-04-02T02:50:41.800Z:
Received chunk: Sure...

2025-04-02T02:50:41.889Z:
Received chunk: !...

2025-04-02T02:50:41.978Z:
Received chunk:  Here...

2025-04-02T02:50:42.069Z:
Received chunk: ’s...

2025-04-02T02:50:42.155Z:
Received chunk:  a...

2025-04-02T02:50:42.243Z:
Received chunk:  quick...

2025-04-02T02:50:42.331Z:
Received chunk:  and...

2025-04-02T02:50:42.418Z:
Received chunk:  easy...

2025-04-02T02:50:42.505Z:
Received chunk:  pasta...

2025-04-02T02:50:42.592Z:
Received chunk:  dish...

2025-04-02T02:50:42.680Z:
Received chunk:  recipe...

2025-04-02T02:50:42.767Z:
Received chunk: :

...

2025-04-02T02:50:42.857Z:
Received chunk: ---

...

2025-04-02T02:50:42.947Z:
Received chunk: ###...

2025-04-02T02:50:43.034Z:
Received chunk:  **...

2025-04-02T02:50:43.121Z:
Received chunk: Quick...

2025-04-02T02:50:43.205Z:
Received chunk:  Pasta...

2025-04-02T02:50:43.290Z:
Received chunk:  with...

2025-04-02T02:50:43.374Z:
Received chunk:  Marin...

2025-04-02T02:50:43.457Z:
Received chunk: ara...

2025-04-02T02:50:43.542Z:
Received chunk:  Sauce...

2025-04-02T02:50:43.627Z:
Received chunk:  and...

2025-04-02T02:50:43.712Z:
Received chunk:  Veget...

2025-04-02T02:50:43.802Z:
Received chunk: ables...

2025-04-02T02:50:43.895Z:
Received chunk: **

...

2025-04-02T02:50:43.985Z:
Received chunk: ####...

2025-04-02T02:50:44.073Z:
Received chunk:  **...

2025-04-02T02:50:44.162Z:
Received chunk: Ingredients...

2025-04-02T02:50:44.249Z:
Received chunk: **...

2025-04-02T02:50:44.337Z:
Received chunk:   
...

2025-04-02T02:50:44.427Z:
Received chunk: -...

2025-04-02T02:50:44.515Z:
Received chunk:  ...

2025-04-02T02:50:44.602Z:
Received chunk: 1...

2025-04-02T02:50:44.690Z:
Received chunk:  lb...

2025-04-02T02:50:44.779Z:
Received chunk:  ground...

2025-04-02T02:50:44.875Z:
Received chunk:  turkey...

2025-04-02T02:50:44.972Z:
Received chunk:  (...

2025-04-02T02:50:45.062Z:
Received chunk: or...

2025-04-02T02:50:45.150Z:
Received chunk:  use...

2025-04-02T02:50:45.237Z:
Received chunk:  lean...

2025-04-02T02:50:45.324Z:
Received chunk:  ground...

2025-04-02T02:50:45.414Z:
Received chunk:  beef...

2025-04-02T02:50:45.505Z:
Received chunk: )
...

2025-04-02T02:50:45.597Z:
Received chunk: -...

2025-04-02T02:50:45.684Z:
Received chunk:  ...

2025-04-02T02:50:45.774Z:
Received chunk: 2...

2025-04-02T02:50:45.869Z:
Received chunk:  cans...

2025-04-02T02:50:45.962Z:
Received chunk:  jar...

2025-04-02T02:50:46.054Z:
Received chunk: red...

2025-04-02T02:50:46.144Z:
Received chunk:  marin...

2025-04-02T02:50:46.236Z:
Received chunk: ara...

2025-04-02T02:50:46.325Z:
Received chunk:  sauce...

2025-04-02T02:50:46.415Z:
Received chunk: 
...

2025-04-02T02:50:46.504Z:
Received chunk: -...

2025-04-02T02:50:46.589Z:
Received chunk:  ...

2025-04-02T02:50:46.675Z:
Received chunk: 4...

2025-04-02T02:50:46.762Z:
Received chunk:  large...

2025-04-02T02:50:46.849Z:
Received chunk:  z...

2025-04-02T02:50:46.940Z:
Received chunk: ucchini...

2025-04-02T02:50:47.025Z:
Received chunk: ,...

2025-04-02T02:50:47.111Z:
Received chunk:  sliced...

2025-04-02T02:50:47.196Z:
Received chunk: 
...

2025-04-02T02:50:47.282Z:
Received chunk: -...

2025-04-02T02:50:47.367Z:
Received chunk:  ...

2025-04-02T02:50:47.453Z:
Received chunk: 6...

2025-04-02T02:50:47.537Z:
Received chunk:  medium...

2025-04-02T02:50:47.622Z:
Received chunk:  mushrooms...

2025-04-02T02:50:47.707Z:
Received chunk: ,...

2025-04-02T02:50:47.793Z:
Received chunk:  sliced...

2025-04-02T02:50:47.887Z:
Received chunk: 
...

2025-04-02T02:50:47.979Z:
Received chunk: -...

2025-04-02T02:50:48.068Z:
Received chunk:  ...

2025-04-02T02:50:48.155Z:
Received chunk: 1...

2025-04-02T02:50:48.243Z:
Received chunk:  cup...

2025-04-02T02:50:48.330Z:
Received chunk:  frozen...

2025-04-02T02:50:48.420Z:
Received chunk:  peas...

2025-04-02T02:50:48.508Z:
Received chunk:  or...

2025-04-02T02:50:48.595Z:
Received chunk:  green...

2025-04-02T02:50:48.682Z:
Received chunk:  beans...

2025-04-02T02:50:48.772Z:
Received chunk:  (...

2025-04-02T02:50:48.864Z:
Received chunk: optional...

2025-04-02T02:50:48.956Z:
Received chunk: )
...

2025-04-02T02:50:49.044Z:
Received chunk: -...

2025-04-02T02:50:49.131Z:
Received chunk:  Salt...

2025-04-02T02:50:49.218Z:
Received chunk:  and...

2025-04-02T02:50:49.307Z:
Received chunk:  pepper...

2025-04-02T02:50:49.399Z:
Received chunk:  to...

2025-04-02T02:50:49.487Z:
Received chunk:  taste...

2025-04-02T02:50:49.575Z:
Received chunk: 
...

2025-04-02T02:50:49.662Z:
Received chunk: -...

2025-04-02T02:50:49.752Z:
Received chunk:  Fresh...

2025-04-02T02:50:49.849Z:
Received chunk:  garlic...

2025-04-02T02:50:49.944Z:
Received chunk: ,...

2025-04-02T02:50:50.034Z:
Received chunk:  minced...

2025-04-02T02:50:50.123Z:
Received chunk:  (...

2025-04-02T02:50:50.211Z:
Received chunk: for...

2025-04-02T02:50:50.300Z:
Received chunk:  seasoning...

2025-04-02T02:50:50.392Z:
Received chunk: )
...

2025-04-02T02:50:50.486Z:
Received chunk: -...

2025-04-02T02:50:50.573Z:
Received chunk:  ...

2025-04-02T02:50:50.658Z:
Received chunk: 1...

2025-04-02T02:50:50.743Z:
Received chunk:  tsp...

2025-04-02T02:50:50.831Z:
Received chunk:  dried...

2025-04-02T02:50:50.921Z:
Received chunk:  Italian...

2025-04-02T02:50:51.008Z:
Received chunk:  season...

2025-04-02T02:50:51.093Z:
Received chunk: ings...

2025-04-02T02:50:51.179Z:
Received chunk:  (...

2025-04-02T02:50:51.264Z:
Received chunk: e...

2025-04-02T02:50:51.351Z:
Received chunk: .g...

2025-04-02T02:50:51.436Z:
Received chunk: .,...

2025-04-02T02:50:51.521Z:
Received chunk:  garlic...

2025-04-02T02:50:51.606Z:
Received chunk:  powder...

2025-04-02T02:50:51.690Z:
Received chunk: ,...

2025-04-02T02:50:51.778Z:
Received chunk:  onion...

2025-04-02T02:50:51.867Z:
Received chunk:  powder...

2025-04-02T02:50:51.956Z:
Received chunk: ,...

2025-04-02T02:50:52.042Z:
Received chunk:  pap...

2025-04-02T02:50:52.128Z:
Received chunk: rika...

2025-04-02T02:50:52.213Z:
Received chunk: )

...

2025-04-02T02:50:52.300Z:
Received chunk: ---

...

2025-04-02T02:50:52.386Z:
Received chunk: ####...

2025-04-02T02:50:52.472Z:
Received chunk:  **...

2025-04-02T02:50:52.557Z:
Received chunk: Instructions...

2025-04-02T02:50:52.642Z:
Received chunk: **...

2025-04-02T02:50:52.735Z:
Received chunk:   
...

2025-04-02T02:50:52.816Z:
Received chunk: 1...

2025-04-02T02:50:52.907Z:
Received chunk: ....

2025-04-02T02:50:52.996Z:
Received chunk:  Cook...

2025-04-02T02:50:53.082Z:
Received chunk:  the...

2025-04-02T02:50:53.167Z:
Received chunk:  ground...

2025-04-02T02:50:53.253Z:
Received chunk:  turkey...

2025-04-02T02:50:53.340Z:
Received chunk:  in...

2025-04-02T02:50:53.426Z:
Received chunk:  a...

2025-04-02T02:50:53.512Z:
Received chunk:  skillet...

2025-04-02T02:50:53.597Z:
Received chunk:  with...

2025-04-02T02:50:53.681Z:
Received chunk:  a...

2025-04-02T02:50:53.769Z:
Received chunk:  splash...

2025-04-02T02:50:53.855Z:
Received chunk:  of...

2025-04-02T02:50:53.944Z:
Received chunk:  water...

2025-04-02T02:50:54.033Z:
Received chunk:  until...

2025-04-02T02:50:54.119Z:
Received chunk:  brown...

2025-04-02T02:50:54.204Z:
Received chunk: ed...

2025-04-02T02:50:54.290Z:
Received chunk: .
...

2025-04-02T02:50:54.375Z:
Received chunk: 2...

2025-04-02T02:50:54.461Z:
Received chunk: ....

2025-04-02T02:50:54.546Z:
Received chunk:  Add...

2025-04-02T02:50:54.633Z:
Received chunk:  the...

2025-04-02T02:50:54.719Z:
Received chunk:  z...

2025-04-02T02:50:54.806Z:
Received chunk: ucchini...

2025-04-02T02:50:54.898Z:
Received chunk:  and...

2025-04-02T02:50:54.987Z:
Received chunk:  mushrooms...

2025-04-02T02:50:55.073Z:
Received chunk:  to...

2025-04-02T02:50:55.159Z:
Received chunk:  the...

2025-04-02T02:50:55.246Z:
Received chunk:  skillet...

2025-04-02T02:50:55.330Z:
Received chunk:  and...

2025-04-02T02:50:55.416Z:
Received chunk:  sa...

2025-04-02T02:50:55.501Z:
Received chunk: ut...

2025-04-02T02:50:55.586Z:
Received chunk: é...

2025-04-02T02:50:55.672Z:
Received chunk:  for...

2025-04-02T02:50:55.758Z:
Received chunk:  ...

2025-04-02T02:50:55.845Z:
Received chunk: 5...

2025-04-02T02:50:55.936Z:
Received chunk:  minutes...

2025-04-02T02:50:56.025Z:
Received chunk: .
...

2025-04-02T02:50:56.111Z:
Received chunk: 3...

2025-04-02T02:50:56.196Z:
Received chunk: ....

2025-04-02T02:50:56.281Z:
Received chunk:  Stir...

2025-04-02T02:50:56.366Z:
Received chunk:  in...

2025-04-02T02:50:56.452Z:
Received chunk:  the...

2025-04-02T02:50:56.537Z:
Received chunk:  marin...

2025-04-02T02:50:56.623Z:
Received chunk: ara...

2025-04-02T02:50:56.709Z:
Received chunk:  sauce...

2025-04-02T02:50:56.796Z:
Received chunk:  and...

2025-04-02T02:50:56.889Z:
Received chunk:  bring...

2025-04-02T02:50:56.976Z:
Received chunk:  to...

2025-04-02T02:50:57.063Z:
Received chunk:  a...

2025-04-02T02:50:57.149Z:
Received chunk:  boil...

2025-04-02T02:50:57.236Z:
Received chunk: .
...

2025-04-02T02:50:57.321Z:
Received chunk: 4...

2025-04-02T02:50:57.406Z:
Received chunk: ....

2025-04-02T02:50:57.501Z:
Received chunk:  Season...

2025-04-02T02:50:57.587Z:
Received chunk:  with...

2025-04-02T02:50:57.673Z:
Received chunk:  salt...

2025-04-02T02:50:57.761Z:
Received chunk: ,...

2025-04-02T02:50:57.845Z:
Received chunk:  pepper...

2025-04-02T02:50:57.938Z:
Received chunk: ,...

2025-04-02T02:50:58.028Z:
Received chunk:  and...

2025-04-02T02:50:58.114Z:
Received chunk:  Italian...

2025-04-02T02:50:58.199Z:
Received chunk:  season...

2025-04-02T02:50:58.285Z:
Received chunk: ings...

2025-04-02T02:50:58.370Z:
Received chunk: .
...

2025-04-02T02:50:58.456Z:
Received chunk: 5...

2025-04-02T02:50:58.543Z:
Received chunk: ....

2025-04-02T02:50:58.629Z:
Received chunk:  Steam...

2025-04-02T02:50:58.713Z:
Received chunk:  broccoli...

2025-04-02T02:50:58.801Z:
Received chunk:  flo...

2025-04-02T02:50:58.894Z:
Received chunk: rets...

2025-04-02T02:50:58.984Z:
Received chunk:  for...

2025-04-02T02:50:59.072Z:
Received chunk:  ...

2025-04-02T02:50:59.157Z:
Received chunk: 5...

2025-04-02T02:50:59.241Z:
Received chunk: -...

2025-04-02T02:50:59.327Z:
Received chunk: 7...

2025-04-02T02:50:59.414Z:
Received chunk:  minutes...

2025-04-02T02:50:59.500Z:
Received chunk:  until...

2025-04-02T02:50:59.585Z:
Received chunk:  just...

2025-04-02T02:50:59.671Z:
Received chunk:  tender...

2025-04-02T02:50:59.757Z:
Received chunk: .
...

2025-04-02T02:50:59.846Z:
Received chunk: 6...

2025-04-02T02:50:59.938Z:
Received chunk: ....

2025-04-02T02:51:00.030Z:
Received chunk:  Combine...

2025-04-02T02:51:00.117Z:
Received chunk:  all...

2025-04-02T02:51:00.202Z:
Received chunk:  ingredients...

2025-04-02T02:51:00.286Z:
Received chunk:  in...

2025-04-02T02:51:00.370Z:
Received chunk:  a...

2025-04-02T02:51:00.454Z:
Received chunk:  large...

2025-04-02T02:51:00.540Z:
Received chunk:  pot...

2025-04-02T02:51:00.625Z:
Received chunk:  or...

2025-04-02T02:51:00.711Z:
Received chunk:  c...

2025-04-02T02:51:00.797Z:
Received chunk: asser...

2025-04-02T02:51:00.886Z:
Received chunk: ole...

2025-04-02T02:51:00.978Z:
Received chunk:  dish...

2025-04-02T02:51:01.066Z:
Received chunk: .
...

2025-04-02T02:51:01.151Z:
Received chunk: 7...

2025-04-02T02:51:01.236Z:
Received chunk: ....

2025-04-02T02:51:01.320Z:
Received chunk:  Serve...

2025-04-02T02:51:01.406Z:
Received chunk:  hot...

2025-04-02T02:51:01.491Z:
Received chunk: .

...

2025-04-02T02:51:01.578Z:
Received chunk: ---

...

2025-04-02T02:51:01.662Z:
Received chunk: This...

2025-04-02T02:51:01.747Z:
Received chunk:  recipe...

2025-04-02T02:51:01.835Z:
Received chunk:  is...

2025-04-02T02:51:01.927Z:
Received chunk:  quick...

2025-04-02T02:51:02.017Z:
Received chunk: ,...

2025-04-02T02:51:02.103Z:
Received chunk:  flavors...

2025-04-02T02:51:02.188Z:
Received chunk: ome...

2025-04-02T02:51:02.275Z:
Received chunk: ,...

2025-04-02T02:51:02.360Z:
Received chunk:  and...

2025-04-02T02:51:02.444Z:
Received chunk:  can...

2025-04-02T02:51:02.529Z:
Received chunk:  be...

2025-04-02T02:51:02.616Z:
Received chunk:  made...

2025-04-02T02:51:02.703Z:
Received chunk:  in...

2025-04-02T02:51:02.790Z:
Received chunk:  about...

2025-04-02T02:51:02.877Z:
Received chunk:  ...

2025-04-02T02:51:02.968Z:
Received chunk: 3...

2025-04-02T02:51:03.059Z:
Received chunk: 0...

2025-04-02T02:51:03.145Z:
Received chunk:  minutes...

2025-04-02T02:51:03.231Z:
Received chunk: ....

2025-04-02T02:51:03.318Z:
Received chunk:  Enjoy...

2025-04-02T02:51:03.405Z:
Received chunk: !...

2025-04-02T02:51:03.491Z:
Received chunk: 

> ...

2025-04-02T02:51:08.961Z:
Generating HTML from 2 messages

2025-04-02T02:51:08.962Z:
Spawning process for HTML generation: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:51:08.984Z:
HTML Generation stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:51:09.008Z:
HTML Generation stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:51:09.029Z:
HTML Generation stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:51:09.039Z:
HTML Generation stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:51:09.061Z:
HTML Generation stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:51:09.113Z:
HTML Generation stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:51:09.114Z:
HTML Generation stderr: load: special tokens cache size = 22


2025-04-02T02:51:09.131Z:
HTML Generation stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00


2025-04-02T02:51:09.131Z:
HTML Generation stderr: print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:51:09.362Z:
HTML Generation stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:51:09.383Z:
HTML Generation stderr: .

2025-04-02T02:51:09.400Z:
HTML Generation stderr: .

2025-04-02T02:51:09.435Z:
HTML Generation stderr: .

2025-04-02T02:51:09.455Z:
HTML Generation stderr: .

2025-04-02T02:51:09.475Z:
HTML Generation stderr: .

2025-04-02T02:51:09.494Z:
HTML Generation stderr: .

2025-04-02T02:51:09.514Z:
HTML Generation stderr: .

2025-04-02T02:51:09.551Z:
HTML Generation stderr: .

2025-04-02T02:51:09.558Z:
HTML Generation stderr: .

2025-04-02T02:51:09.591Z:
HTML Generation stderr: .

2025-04-02T02:51:09.608Z:
HTML Generation stderr: .

2025-04-02T02:51:09.633Z:
HTML Generation stderr: .

2025-04-02T02:51:09.649Z:
HTML Generation stderr: .

2025-04-02T02:51:09.673Z:
HTML Generation stderr: .

2025-04-02T02:51:09.688Z:
HTML Generation stderr: .

2025-04-02T02:51:09.705Z:
HTML Generation stderr: .

2025-04-02T02:51:09.727Z:
HTML Generation stderr: .

2025-04-02T02:51:09.743Z:
HTML Generation stderr: .

2025-04-02T02:51:09.759Z:
HTML Generation stderr: .

2025-04-02T02:51:09.782Z:
HTML Generation stderr: .

2025-04-02T02:51:09.798Z:
HTML Generation stderr: .

2025-04-02T02:51:09.821Z:
HTML Generation stderr: .

2025-04-02T02:51:09.839Z:
HTML Generation stderr: .

2025-04-02T02:51:09.864Z:
HTML Generation stderr: .

2025-04-02T02:51:09.880Z:
HTML Generation stderr: .

2025-04-02T02:51:09.911Z:
HTML Generation stderr: .

2025-04-02T02:51:09.934Z:
HTML Generation stderr: .

2025-04-02T02:51:09.951Z:
HTML Generation stderr: .

2025-04-02T02:51:09.975Z:
HTML Generation stderr: .

2025-04-02T02:51:09.990Z:
HTML Generation stderr: .

2025-04-02T02:51:10.007Z:
HTML Generation stderr: .

2025-04-02T02:51:10.031Z:
HTML Generation stderr: .

2025-04-02T02:51:10.046Z:
HTML Generation stderr: .

2025-04-02T02:51:10.063Z:
HTML Generation stderr: .

2025-04-02T02:51:10.086Z:
HTML Generation stderr: .

2025-04-02T02:51:10.103Z:
HTML Generation stderr: .

2025-04-02T02:51:10.126Z:
HTML Generation stderr: .

2025-04-02T02:51:10.142Z:
HTML Generation stderr: .

2025-04-02T02:51:10.162Z:
HTML Generation stderr: .

2025-04-02T02:51:10.183Z:
HTML Generation stderr: .

2025-04-02T02:51:10.216Z:
HTML Generation stderr: .

2025-04-02T02:51:10.223Z:
HTML Generation stderr: .

2025-04-02T02:51:10.259Z:
HTML Generation stderr: .

2025-04-02T02:51:10.267Z:
HTML Generation stderr: .

2025-04-02T02:51:10.299Z:
HTML Generation stderr: .

2025-04-02T02:51:10.316Z:
HTML Generation stderr: .

2025-04-02T02:51:10.341Z:
HTML Generation stderr: .

2025-04-02T02:51:10.357Z:
HTML Generation stderr: .

2025-04-02T02:51:10.375Z:
HTML Generation stderr: .

2025-04-02T02:51:10.399Z:
HTML Generation stderr: .

2025-04-02T02:51:10.416Z:
HTML Generation stderr: .

2025-04-02T02:51:10.440Z:
HTML Generation stderr: .

2025-04-02T02:51:10.456Z:
HTML Generation stderr: .

2025-04-02T02:51:10.472Z:
HTML Generation stderr: .

2025-04-02T02:51:10.495Z:
HTML Generation stderr: .

2025-04-02T02:51:10.511Z:
HTML Generation stderr: .

2025-04-02T02:51:10.530Z:
HTML Generation stderr: .

2025-04-02T02:51:10.549Z:
HTML Generation stderr: .

2025-04-02T02:51:10.569Z:
HTML Generation stderr: .

2025-04-02T02:51:10.588Z:
HTML Generation stderr: .

2025-04-02T02:51:10.608Z:
HTML Generation stderr: .

2025-04-02T02:51:10.643Z:
HTML Generation stderr: .

2025-04-02T02:51:10.650Z:
HTML Generation stderr: .

2025-04-02T02:51:10.683Z:
HTML Generation stderr: .

2025-04-02T02:51:10.690Z:
HTML Generation stderr: .

2025-04-02T02:51:10.725Z:
HTML Generation stderr: ....

2025-04-02T02:51:10.725Z:
HTML Generation stderr: ........

2025-04-02T02:51:10.725Z:
HTML Generation stderr: ....

2025-04-02T02:51:10.726Z:
HTML Generation stderr: ..


2025-04-02T02:51:10.727Z:
HTML Generation stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:51:10.727Z:
HTML Generation stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:51:10.764Z:
HTML Generation stderr: init:        CPU KV buffer size =   224.00 MiB
llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB


2025-04-02T02:51:10.769Z:
HTML Generation stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:51:11.011Z:
HTML Generation stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>


2025-04-02T02:51:11.011Z:
HTML Generation stderr: 
system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-02T02:51:11.014Z:
HTML Generation stderr: main: interactive mode on.


2025-04-02T02:51:11.014Z:
HTML Generation stderr: sampler seed: 3714995482
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 4096, n_batch = 2048, n_predict = 4096, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:55:16.709Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:55:16.709Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:55:16.709Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:55:16.710Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:55:17.381Z:
Unloading model...

2025-04-02T02:55:17.381Z:
Model unloaded successfully

2025-04-02T02:55:17.382Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:55:17.382Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:55:17.382Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T02:55:17.382Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T02:55:22.244Z:
Generate response called with message: give me a quick recipe...

2025-04-02T02:55:22.245Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a quick recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T02:55:22.271Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:55:22.301Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:55:22.323Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:55:22.332Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:55:22.355Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:55:22.409Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:55:22.409Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T02:55:22.426Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0


2025-04-02T02:55:22.426Z:
Process stderr: print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:55:22.664Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:55:22.688Z:
Process stderr: .

2025-04-02T02:55:22.708Z:
Process stderr: .

2025-04-02T02:55:22.729Z:
Process stderr: .

2025-04-02T02:55:22.748Z:
Process stderr: .

2025-04-02T02:55:22.768Z:
Process stderr: .

2025-04-02T02:55:22.787Z:
Process stderr: .

2025-04-02T02:55:22.807Z:
Process stderr: .

2025-04-02T02:55:22.839Z:
Process stderr: .

2025-04-02T02:55:22.846Z:
Process stderr: .

2025-04-02T02:55:22.877Z:
Process stderr: .

2025-04-02T02:55:22.894Z:
Process stderr: .

2025-04-02T02:55:22.916Z:
Process stderr: .

2025-04-02T02:55:22.932Z:
Process stderr: .

2025-04-02T02:55:22.955Z:
Process stderr: .

2025-04-02T02:55:22.971Z:
Process stderr: .

2025-04-02T02:55:22.986Z:
Process stderr: .

2025-04-02T02:55:23.009Z:
Process stderr: .

2025-04-02T02:55:23.025Z:
Process stderr: .

2025-04-02T02:55:23.041Z:
Process stderr: .

2025-04-02T02:55:23.063Z:
Process stderr: .

2025-04-02T02:55:23.080Z:
Process stderr: .

2025-04-02T02:55:23.103Z:
Process stderr: .

2025-04-02T02:55:23.118Z:
Process stderr: .

2025-04-02T02:55:23.138Z:
Process stderr: .

2025-04-02T02:55:23.158Z:
Process stderr: .

2025-04-02T02:55:23.189Z:
Process stderr: .

2025-04-02T02:55:23.212Z:
Process stderr: .

2025-04-02T02:55:23.229Z:
Process stderr: .

2025-04-02T02:55:23.252Z:
Process stderr: .

2025-04-02T02:55:23.267Z:
Process stderr: .

2025-04-02T02:55:23.283Z:
Process stderr: .

2025-04-02T02:55:23.306Z:
Process stderr: .

2025-04-02T02:55:23.322Z:
Process stderr: .

2025-04-02T02:55:23.338Z:
Process stderr: .

2025-04-02T02:55:23.361Z:
Process stderr: .

2025-04-02T02:55:23.377Z:
Process stderr: .

2025-04-02T02:55:23.401Z:
Process stderr: .

2025-04-02T02:55:23.416Z:
Process stderr: .

2025-04-02T02:55:23.436Z:
Process stderr: .

2025-04-02T02:55:23.457Z:
Process stderr: .

2025-04-02T02:55:23.490Z:
Process stderr: .

2025-04-02T02:55:23.497Z:
Process stderr: .

2025-04-02T02:55:23.531Z:
Process stderr: .

2025-04-02T02:55:23.538Z:
Process stderr: .

2025-04-02T02:55:23.571Z:
Process stderr: .

2025-04-02T02:55:23.588Z:
Process stderr: .

2025-04-02T02:55:23.613Z:
Process stderr: .

2025-04-02T02:55:23.632Z:
Process stderr: .

2025-04-02T02:55:23.649Z:
Process stderr: .

2025-04-02T02:55:23.674Z:
Process stderr: .

2025-04-02T02:55:23.691Z:
Process stderr: .

2025-04-02T02:55:23.715Z:
Process stderr: .

2025-04-02T02:55:23.731Z:
Process stderr: .

2025-04-02T02:55:23.748Z:
Process stderr: .

2025-04-02T02:55:23.770Z:
Process stderr: .

2025-04-02T02:55:23.785Z:
Process stderr: .

2025-04-02T02:55:23.804Z:
Process stderr: .

2025-04-02T02:55:23.824Z:
Process stderr: .

2025-04-02T02:55:23.843Z:
Process stderr: .

2025-04-02T02:55:23.862Z:
Process stderr: .

2025-04-02T02:55:23.881Z:
Process stderr: .

2025-04-02T02:55:23.916Z:
Process stderr: .

2025-04-02T02:55:23.923Z:
Process stderr: .

2025-04-02T02:55:23.955Z:
Process stderr: .

2025-04-02T02:55:23.961Z:
Process stderr: .

2025-04-02T02:55:23.993Z:
Process stderr: ....

2025-04-02T02:55:23.993Z:
Process stderr: .........

2025-04-02T02:55:23.994Z:
Process stderr: .....


2025-04-02T02:55:23.996Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:55:23.996Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:55:24.014Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T02:55:24.020Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:55:24.265Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T02:55:24.265Z:
Process stderr: main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T02:55:24.266Z:
Process stderr: sampler seed: 2928672492
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:55:24.510Z:
Received chunk: <think>...

2025-04-02T02:55:24.595Z:
Received chunk: 
...

2025-04-02T02:55:24.683Z:
Received chunk: Okay...

2025-04-02T02:55:24.771Z:
Received chunk: ,...

2025-04-02T02:55:24.857Z:
Received chunk:  the...

2025-04-02T02:55:24.942Z:
Received chunk:  user...

2025-04-02T02:55:25.025Z:
Received chunk:  just...

2025-04-02T02:55:25.110Z:
Received chunk:  asked...

2025-04-02T02:55:25.195Z:
Received chunk:  for...

2025-04-02T02:55:25.285Z:
Received chunk:  a...

2025-04-02T02:55:25.378Z:
Received chunk:  quick...

2025-04-02T02:55:25.463Z:
Received chunk:  recipe...

2025-04-02T02:55:25.549Z:
Received chunk: ....

2025-04-02T02:55:25.637Z:
Received chunk:  Hmm...

2025-04-02T02:55:25.729Z:
Received chunk: ,...

2025-04-02T02:55:25.816Z:
Received chunk:  I...

2025-04-02T02:55:25.901Z:
Received chunk:  need...

2025-04-02T02:55:25.986Z:
Received chunk:  to...

2025-04-02T02:55:26.070Z:
Received chunk:  figure...

2025-04-02T02:55:26.154Z:
Received chunk:  out...

2025-04-02T02:55:26.238Z:
Received chunk:  what...

2025-04-02T02:55:26.323Z:
Received chunk:  they...

2025-04-02T02:55:26.408Z:
Received chunk:  might...

2025-04-02T02:55:26.492Z:
Received chunk:  be...

2025-04-02T02:55:26.577Z:
Received chunk:  looking...

2025-04-02T02:55:26.667Z:
Received chunk:  for...

2025-04-02T02:55:26.758Z:
Received chunk: ....

2025-04-02T02:55:26.845Z:
Received chunk:  They...

2025-04-02T02:55:26.929Z:
Received chunk:  probably...

2025-04-02T02:55:27.014Z:
Received chunk:  want...

2025-04-02T02:55:27.101Z:
Received chunk:  something...

2025-04-02T02:55:27.187Z:
Received chunk:  easy...

2025-04-02T02:55:27.274Z:
Received chunk:  and...

2025-04-02T02:55:27.360Z:
Received chunk:  tasty...

2025-04-02T02:55:27.447Z:
Received chunk:  but...

2025-04-02T02:55:27.534Z:
Received chunk:  not...

2025-04-02T02:55:27.624Z:
Received chunk:  too...

2025-04-02T02:55:27.715Z:
Received chunk:  complicated...

2025-04-02T02:55:27.806Z:
Received chunk: .

...

2025-04-02T02:55:27.903Z:
Received chunk: Maybe...

2025-04-02T02:55:27.994Z:
Received chunk:  they...

2025-04-02T02:55:28.081Z:
Received chunk: 're...

2025-04-02T02:55:28.167Z:
Received chunk:  in...

2025-04-02T02:55:28.253Z:
Received chunk:  a...

2025-04-02T02:55:28.340Z:
Received chunk:  hurry...

2025-04-02T02:55:28.426Z:
Received chunk:  or...

2025-04-02T02:55:28.513Z:
Received chunk:  don...

2025-04-02T02:55:28.601Z:
Received chunk: 't...

2025-04-02T02:55:28.693Z:
Received chunk:  have...

2025-04-02T02:55:28.784Z:
Received chunk:  many...

2025-04-02T02:55:28.872Z:
Received chunk:  ingredients...

2025-04-02T02:55:28.956Z:
Received chunk:  at...

2025-04-02T02:55:29.040Z:
Received chunk:  home...

2025-04-02T02:55:29.125Z:
Received chunk: ....

2025-04-02T02:55:29.210Z:
Received chunk:  I...

2025-04-02T02:55:29.294Z:
Received chunk:  should...

2025-04-02T02:55:29.378Z:
Received chunk:  go...

2025-04-02T02:55:29.464Z:
Received chunk:  with...

2025-04-02T02:55:29.552Z:
Received chunk:  a...

2025-04-02T02:55:29.640Z:
Received chunk:  popular...

2025-04-02T02:55:29.732Z:
Received chunk:  dish...

2025-04-02T02:55:29.824Z:
Received chunk:  that...

2025-04-02T02:55:29.909Z:
Received chunk:  uses...

2025-04-02T02:55:29.994Z:
Received chunk:  common...

2025-04-02T02:55:30.080Z:
Received chunk:  ingredients...

2025-04-02T02:55:30.165Z:
Received chunk: ....

2025-04-02T02:55:30.251Z:
Received chunk:  Pasta...

2025-04-02T02:55:30.337Z:
Received chunk:  is...

2025-04-02T02:55:30.422Z:
Received chunk:  always...

2025-04-02T02:55:30.506Z:
Received chunk:  a...

2025-04-02T02:55:30.593Z:
Received chunk:  safe...

2025-04-02T02:55:30.679Z:
Received chunk:  bet...

2025-04-02T02:55:30.769Z:
Received chunk:  because...

2025-04-02T02:55:30.859Z:
Received chunk:  it...

2025-04-02T02:55:30.944Z:
Received chunk: 's...

2025-04-02T02:55:31.028Z:
Received chunk:  quick...

2025-04-02T02:55:31.111Z:
Received chunk:  and...

2025-04-02T02:55:31.198Z:
Received chunk:  can...

2025-04-02T02:55:31.284Z:
Received chunk:  be...

2025-04-02T02:55:31.371Z:
Received chunk:  paired...

2025-04-02T02:55:31.464Z:
Received chunk:  with...

2025-04-02T02:55:31.555Z:
Received chunk:  various...

2025-04-02T02:55:31.648Z:
Received chunk:  veggies...

2025-04-02T02:55:31.748Z:
Received chunk:  or...

2025-04-02T02:55:31.838Z:
Received chunk:  meats...

2025-04-02T02:55:31.925Z:
Received chunk: .

...

2025-04-02T02:55:32.013Z:
Received chunk: Let...

2025-04-02T02:55:32.100Z:
Received chunk:  me...

2025-04-02T02:55:32.185Z:
Received chunk:  think...

2025-04-02T02:55:32.268Z:
Received chunk:  about...

2025-04-02T02:55:32.352Z:
Received chunk:  what...

2025-04-02T02:55:32.439Z:
Received chunk: 's...

2025-04-02T02:55:32.527Z:
Received chunk:  seasonal...

2025-04-02T02:55:32.623Z:
Received chunk:  right...

2025-04-02T02:55:32.705Z:
Received chunk:  now...

2025-04-02T02:55:32.799Z:
Received chunk: ....

2025-04-02T02:55:32.890Z:
Received chunk:  Spin...

2025-04-02T02:55:32.977Z:
Received chunk: ach...

2025-04-02T02:55:33.065Z:
Received chunk:  and...

2025-04-02T02:55:33.151Z:
Received chunk:  tomatoes...

2025-04-02T02:55:33.241Z:
Received chunk:  are...

2025-04-02T02:55:33.328Z:
Received chunk:  fresh...

2025-04-02T02:55:33.419Z:
Received chunk:  and...

2025-04-02T02:55:33.507Z:
Received chunk:  widely...

2025-04-02T02:55:33.596Z:
Received chunk:  available...

2025-04-02T02:55:33.687Z:
Received chunk: ,...

2025-04-02T02:55:33.782Z:
Received chunk:  so...

2025-04-02T02:55:33.879Z:
Received chunk:  a...

2025-04-02T02:55:33.969Z:
Received chunk:  spinach...

2025-04-02T02:55:34.055Z:
Received chunk:  and...

2025-04-02T02:55:34.142Z:
Received chunk:  tomato...

2025-04-02T02:55:34.231Z:
Received chunk:  pasta...

2025-04-02T02:55:34.321Z:
Received chunk:  would...

2025-04-02T02:55:34.417Z:
Received chunk:  be...

2025-04-02T02:55:34.514Z:
Received chunk:  a...

2025-04-02T02:55:34.611Z:
Received chunk:  good...

2025-04-02T02:55:34.711Z:
Received chunk:  choice...

2025-04-02T02:55:34.804Z:
Received chunk: ....

2025-04-02T02:55:34.893Z:
Received chunk:  It...

2025-04-02T02:55:34.983Z:
Received chunk: 's...

2025-04-02T02:55:35.072Z:
Received chunk:  colorful...

2025-04-02T02:55:35.163Z:
Received chunk:  and...

2025-04-02T02:55:35.256Z:
Received chunk:  hearty...

2025-04-02T02:55:35.351Z:
Received chunk: ,...

2025-04-02T02:55:35.443Z:
Received chunk:  which...

2025-04-02T02:55:35.530Z:
Received chunk:  sounds...

2025-04-02T02:55:35.616Z:
Received chunk:  appealing...

2025-04-02T02:55:35.706Z:
Received chunk: .

...

2025-04-02T02:55:35.798Z:
Received chunk: I...

2025-04-02T02:55:35.888Z:
Received chunk:  should...

2025-04-02T02:55:35.974Z:
Received chunk:  outline...

2025-04-02T02:55:36.061Z:
Received chunk:  the...

2025-04-02T02:55:36.148Z:
Received chunk:  steps...

2025-04-02T02:55:36.235Z:
Received chunk:  clearly...

2025-04-02T02:55:36.324Z:
Received chunk:  but...

2025-04-02T02:55:36.414Z:
Received chunk:  conc...

2025-04-02T02:55:36.502Z:
Received chunk: is...

2025-04-02T02:55:36.589Z:
Received chunk: ely...

2025-04-02T02:55:36.677Z:
Received chunk:  so...

2025-04-02T02:55:36.769Z:
Received chunk:  they...

2025-04-02T02:55:36.863Z:
Received chunk:  can...

2025-04-02T02:55:36.951Z:
Received chunk:  make...

2025-04-02T02:55:37.041Z:
Received chunk:  it...

2025-04-02T02:55:37.129Z:
Received chunk:  in...

2025-04-02T02:55:37.217Z:
Received chunk:  no...

2025-04-02T02:55:37.305Z:
Received chunk:  time...

2025-04-02T02:55:37.392Z:
Received chunk: ....

2025-04-02T02:55:37.481Z:
Received chunk:  Starting...

2025-04-02T02:55:37.569Z:
Received chunk:  with...

2025-04-02T02:55:37.664Z:
Received chunk:  cooking...

2025-04-02T02:55:37.745Z:
Received chunk:  the...

2025-04-02T02:55:37.837Z:
Received chunk:  pasta...

2025-04-02T02:55:37.930Z:
Received chunk:  is...

2025-04-02T02:55:38.018Z:
Received chunk:  essential...

2025-04-02T02:55:38.108Z:
Received chunk:  because...

2025-04-02T02:55:38.195Z:
Received chunk:  that...

2025-04-02T02:55:38.281Z:
Received chunk: 's...

2025-04-02T02:55:38.366Z:
Received chunk:  usually...

2025-04-02T02:55:38.451Z:
Received chunk:  the...

2025-04-02T02:55:38.536Z:
Received chunk:  longest...

2025-04-02T02:55:38.621Z:
Received chunk:  part...

2025-04-02T02:55:38.708Z:
Received chunk: ....

2025-04-02T02:55:38.795Z:
Received chunk:  Then...

2025-04-02T02:55:38.885Z:
Received chunk: ,...

2025-04-02T02:55:38.970Z:
Received chunk:  preparing...

2025-04-02T02:55:39.053Z:
Received chunk:  the...

2025-04-02T02:55:39.137Z:
Received chunk:  veggies...

2025-04-02T02:55:39.221Z:
Received chunk:  and...

2025-04-02T02:55:39.305Z:
Received chunk:  sauce...

2025-04-02T02:55:39.390Z:
Received chunk:  together...

2025-04-02T02:55:39.474Z:
Received chunk:  will...

2025-04-02T02:55:39.561Z:
Received chunk:  save...

2025-04-02T02:55:39.648Z:
Received chunk:  time...

2025-04-02T02:55:39.735Z:
Received chunk: .

...

2025-04-02T02:55:39.823Z:
Received chunk: Oh...

2025-04-02T02:55:39.912Z:
Received chunk: ,...

2025-04-02T02:55:39.996Z:
Received chunk:  and...

2025-04-02T02:55:40.080Z:
Received chunk:  adding...

2025-04-02T02:55:40.165Z:
Received chunk:  some...

2025-04-02T02:55:40.253Z:
Received chunk:  ric...

2025-04-02T02:55:40.338Z:
Received chunk: otta...

2025-04-02T02:55:40.423Z:
Received chunk:  for...

2025-04-02T02:55:40.507Z:
Received chunk:  cream...

2025-04-02T02:55:40.592Z:
Received chunk: iness...

2025-04-02T02:55:40.682Z:
Received chunk:  would...

2025-04-02T02:55:40.763Z:
Received chunk:  make...

2025-04-02T02:55:40.853Z:
Received chunk:  it...

2025-04-02T02:55:40.940Z:
Received chunk:  more...

2025-04-02T02:55:41.025Z:
Received chunk:  satisfying...

2025-04-02T02:55:41.110Z:
Received chunk: ....

2025-04-02T02:55:41.194Z:
Received chunk:  Plus...

2025-04-02T02:55:41.278Z:
Received chunk: ,...

2025-04-02T02:55:41.366Z:
Received chunk:  a...

2025-04-02T02:55:41.454Z:
Received chunk:  quick...

2025-04-02T02:55:41.542Z:
Received chunk:  toss...

2025-04-02T02:55:41.631Z:
Received chunk:  before...

2025-04-02T02:55:41.719Z:
Received chunk:  serving...

2025-04-02T02:55:41.812Z:
Received chunk:  keeps...

2025-04-02T02:55:41.899Z:
Received chunk:  everything...

2025-04-02T02:55:41.987Z:
Received chunk:  from...

2025-04-02T02:55:42.078Z:
Received chunk:  getting...

2025-04-02T02:55:42.166Z:
Received chunk:  too...

2025-04-02T02:55:42.254Z:
Received chunk:  mush...

2025-04-02T02:55:42.343Z:
Received chunk: y...

2025-04-02T02:55:42.431Z:
Received chunk: .

...

2025-04-02T02:55:42.518Z:
Received chunk: I...

2025-04-02T02:55:42.611Z:
Received chunk:  hope...

2025-04-02T02:55:42.699Z:
Received chunk:  this...

2025-04-02T02:55:42.787Z:
Received chunk:  meets...

2025-04-02T02:55:42.882Z:
Received chunk:  their...

2025-04-02T02:55:42.974Z:
Received chunk:  needs...

2025-04-02T02:55:43.066Z:
Received chunk: ....

2025-04-02T02:55:43.159Z:
Received chunk:  It...

2025-04-02T02:55:43.245Z:
Received chunk:  should...

2025-04-02T02:55:43.332Z:
Received chunk:  be...

2025-04-02T02:55:43.422Z:
Received chunk:  both...

2025-04-02T02:55:43.506Z:
Received chunk:  quick...

2025-04-02T02:55:43.593Z:
Received chunk:  and...

2025-04-02T02:55:43.688Z:
Received chunk:  delicious...

2025-04-02T02:55:43.771Z:
Received chunk: !...

2025-04-02T02:55:43.862Z:
Received chunk:  Let...

2025-04-02T02:55:43.950Z:
Received chunk:  me...

2025-04-02T02:55:44.035Z:
Received chunk:  present...

2025-04-02T02:55:44.118Z:
Received chunk:  it...

2025-04-02T02:55:44.202Z:
Received chunk:  in...

2025-04-02T02:55:44.286Z:
Received chunk:  a...

2025-04-02T02:55:44.371Z:
Received chunk:  friendly...

2025-04-02T02:55:44.455Z:
Received chunk:  and...

2025-04-02T02:55:44.539Z:
Received chunk:  helpful...

2025-04-02T02:55:44.624Z:
Received chunk:  manner...

2025-04-02T02:55:44.709Z:
Received chunk: .
...

2025-04-02T02:55:44.794Z:
Received chunk: </think>...

2025-04-02T02:55:44.885Z:
Received chunk: 

...

2025-04-02T02:55:44.973Z:
Received chunk: Sure...

2025-04-02T02:55:45.058Z:
Received chunk: !...

2025-04-02T02:55:45.143Z:
Received chunk:  Here...

2025-04-02T02:55:45.228Z:
Received chunk: 's...

2025-04-02T02:55:45.314Z:
Received chunk:  a...

2025-04-02T02:55:45.399Z:
Received chunk:  quick...

2025-04-02T02:55:45.483Z:
Received chunk:  and...

2025-04-02T02:55:45.567Z:
Received chunk:  easy...

2025-04-02T02:55:45.653Z:
Received chunk:  recipe...

2025-04-02T02:55:45.740Z:
Received chunk:  for...

2025-04-02T02:55:45.828Z:
Received chunk:  **...

2025-04-02T02:55:45.917Z:
Received chunk: Spin...

2025-04-02T02:55:46.003Z:
Received chunk: ach...

2025-04-02T02:55:46.088Z:
Received chunk:  and...

2025-04-02T02:55:46.172Z:
Received chunk:  Tomato...

2025-04-02T02:55:46.257Z:
Received chunk:  Pasta...

2025-04-02T02:55:46.341Z:
Received chunk: **...

2025-04-02T02:55:46.425Z:
Received chunk:  with...

2025-04-02T02:55:46.509Z:
Received chunk:  some...

2025-04-02T02:55:46.592Z:
Received chunk:  added...

2025-04-02T02:55:46.677Z:
Received chunk:  ric...

2025-04-02T02:55:46.763Z:
Received chunk: otta...

2025-04-02T02:55:46.850Z:
Received chunk:  for...

2025-04-02T02:55:46.942Z:
Received chunk:  cream...

2025-04-02T02:55:47.028Z:
Received chunk: iness...

2025-04-02T02:55:47.112Z:
Received chunk: :

...

2025-04-02T02:55:47.197Z:
Received chunk: ---

...

2025-04-02T02:55:47.281Z:
Received chunk: ###...

2025-04-02T02:55:47.366Z:
Received chunk:  Spin...

2025-04-02T02:55:47.451Z:
Received chunk: ach...

2025-04-02T02:55:47.535Z:
Received chunk:  and...

2025-04-02T02:55:47.619Z:
Received chunk:  Tomato...

2025-04-02T02:55:47.712Z:
Received chunk:  Pasta...

2025-04-02T02:55:47.791Z:
Received chunk: 

...

2025-04-02T02:55:47.880Z:
Received chunk: ####...

2025-04-02T02:55:47.969Z:
Received chunk:  Ingredients...

2025-04-02T02:55:48.055Z:
Received chunk: :
...

2025-04-02T02:55:48.138Z:
Received chunk: -...

2025-04-02T02:55:48.222Z:
Received chunk:  ...

2025-04-02T02:55:48.308Z:
Received chunk: 2...

2025-04-02T02:55:48.394Z:
Received chunk:  cups...

2025-04-02T02:55:48.477Z:
Received chunk:  elbow...

2025-04-02T02:55:48.562Z:
Received chunk:  mac...

2025-04-02T02:55:48.647Z:
Received chunk: aron...

2025-04-02T02:55:48.733Z:
Received chunk: i...

2025-04-02T02:55:48.820Z:
Received chunk: 
...

2025-04-02T02:55:48.910Z:
Received chunk: -...

2025-04-02T02:55:48.998Z:
Received chunk:  ...

2025-04-02T02:55:49.083Z:
Received chunk: 1...

2025-04-02T02:55:49.167Z:
Received chunk:  cup...

2025-04-02T02:55:49.251Z:
Received chunk:  frozen...

2025-04-02T02:55:49.337Z:
Received chunk:  spinach...

2025-04-02T02:55:49.422Z:
Received chunk:  (...

2025-04-02T02:55:49.507Z:
Received chunk: or...

2025-04-02T02:55:49.591Z:
Received chunk:  ...

2025-04-02T02:55:49.676Z:
Received chunk: 3...

2025-04-02T02:55:49.762Z:
Received chunk:  cups...

2025-04-02T02:55:49.850Z:
Received chunk:  fresh...

2025-04-02T02:55:49.939Z:
Received chunk:  spinach...

2025-04-02T02:55:50.026Z:
Received chunk: ,...

2025-04-02T02:55:50.112Z:
Received chunk:  frozen...

2025-04-02T02:55:50.197Z:
Received chunk: )
...

2025-04-02T02:55:50.282Z:
Received chunk: -...

2025-04-02T02:55:50.369Z:
Received chunk:  ...

2025-04-02T02:55:50.454Z:
Received chunk: 1...

2025-04-02T02:55:50.539Z:
Received chunk:  cup...

2025-04-02T02:55:50.623Z:
Received chunk:  crushed...

2025-04-02T02:55:50.709Z:
Received chunk:  tomatoes...

2025-04-02T02:55:50.795Z:
Received chunk:  (...

2025-04-02T02:55:50.884Z:
Received chunk: I...

2025-04-02T02:55:50.972Z:
Received chunk:  like...

2025-04-02T02:55:51.059Z:
Received chunk:  to...

2025-04-02T02:55:51.143Z:
Received chunk:  use...

2025-04-02T02:55:51.229Z:
Received chunk:  crushed...

2025-04-02T02:55:51.314Z:
Received chunk:  for...

2025-04-02T02:55:51.400Z:
Received chunk:  a...

2025-04-02T02:55:51.484Z:
Received chunk:  burst...

2025-04-02T02:55:51.568Z:
Received chunk:  of...

2025-04-02T02:55:51.653Z:
Received chunk:  flavor...

2025-04-02T02:55:51.745Z:
Received chunk: )
...

2025-04-02T02:55:51.825Z:
Received chunk: -...

2025-04-02T02:55:51.917Z:
Received chunk:  ...

2025-04-02T02:55:52.004Z:
Received chunk: 1...

2025-04-02T02:55:52.090Z:
Received chunk: /...

2025-04-02T02:55:52.175Z:
Received chunk: 4...

2025-04-02T02:55:52.259Z:
Received chunk:  cup...

2025-04-02T02:55:52.344Z:
Received chunk:  olive...

2025-04-02T02:55:52.431Z:
Received chunk:  oil...

2025-04-02T02:55:52.515Z:
Received chunk:  or...

2025-04-02T02:55:52.600Z:
Received chunk:  butter...

2025-04-02T02:55:52.685Z:
Received chunk:  (...

2025-04-02T02:55:52.773Z:
Received chunk: for...

2025-04-02T02:55:52.861Z:
Received chunk:  cooking...

2025-04-02T02:55:52.950Z:
Received chunk:  the...

2025-04-02T02:55:53.037Z:
Received chunk:  pasta...

2025-04-02T02:55:53.124Z:
Received chunk: )
...

2025-04-02T02:55:53.209Z:
Received chunk: -...

2025-04-02T02:55:53.294Z:
Received chunk:  Salt...

2025-04-02T02:55:53.381Z:
Received chunk:  and...

2025-04-02T02:55:53.464Z:
Received chunk:  pepper...

2025-04-02T02:55:53.549Z:
Received chunk:  to...

2025-04-02T02:55:53.634Z:
Received chunk:  taste...

2025-04-02T02:55:53.720Z:
Received chunk: 
...

2025-04-02T02:55:53.808Z:
Received chunk: -...

2025-04-02T02:55:53.898Z:
Received chunk:  Fresh...

2025-04-02T02:55:53.988Z:
Received chunk:  basil...

2025-04-02T02:55:54.073Z:
Received chunk:  or...

2025-04-02T02:55:54.159Z:
Received chunk:  parsley...

2025-04-02T02:55:54.244Z:
Received chunk:  for...

2025-04-02T02:55:54.329Z:
Received chunk:  garn...

2025-04-02T02:55:54.415Z:
Received chunk: ish...

2025-04-02T02:55:54.500Z:
Received chunk:  (...

2025-04-02T02:55:54.585Z:
Received chunk: optional...

2025-04-02T02:55:54.671Z:
Received chunk: )
...

2025-04-02T02:55:54.763Z:
Received chunk: -...

2025-04-02T02:55:54.844Z:
Received chunk:  ...

2025-04-02T02:55:54.934Z:
Received chunk: 1...

2025-04-02T02:55:55.022Z:
Received chunk: /...

2025-04-02T02:55:55.108Z:
Received chunk: 2...

2025-04-02T02:55:55.195Z:
Received chunk:  cup...

2025-04-02T02:55:55.279Z:
Received chunk:  ric...

2025-04-02T02:55:55.362Z:
Received chunk: otta...

2025-04-02T02:55:55.448Z:
Received chunk:  cheese...

2025-04-02T02:55:55.533Z:
Received chunk:  (...

2025-04-02T02:55:55.618Z:
Received chunk: optional...

2025-04-02T02:55:55.702Z:
Received chunk: ,...

2025-04-02T02:55:55.788Z:
Received chunk:  for...

2025-04-02T02:55:55.876Z:
Received chunk:  extra...

2025-04-02T02:55:55.969Z:
Received chunk:  cream...

2025-04-02T02:55:56.057Z:
Received chunk: iness...

2025-04-02T02:55:56.142Z:
Received chunk: )

...

2025-04-02T02:55:56.228Z:
Received chunk: ---

...

2025-04-02T02:55:56.313Z:
Received chunk: ####...

2025-04-02T02:55:56.398Z:
Received chunk:  Instructions...

2025-04-02T02:55:56.482Z:
Received chunk: :
...

2025-04-02T02:55:56.567Z:
Received chunk: 1...

2025-04-02T02:55:56.653Z:
Received chunk: ....

2025-04-02T02:55:56.738Z:
Received chunk:  **...

2025-04-02T02:55:56.825Z:
Received chunk: Cook...

2025-04-02T02:55:56.917Z:
Received chunk:  the...

2025-04-02T02:55:57.008Z:
Received chunk:  Pasta...

2025-04-02T02:55:57.094Z:
Received chunk: **:...

2025-04-02T02:55:57.180Z:
Received chunk:  Bring...

2025-04-02T02:55:57.266Z:
Received chunk:  a...

2025-04-02T02:55:57.352Z:
Received chunk:  large...

2025-04-02T02:55:57.437Z:
Received chunk:  pot...

2025-04-02T02:55:57.522Z:
Received chunk:  of...

2025-04-02T02:55:57.607Z:
Received chunk:  salt...

2025-04-02T02:55:57.693Z:
Received chunk: ed...

2025-04-02T02:55:57.787Z:
Received chunk:  water...

2025-04-02T02:55:57.867Z:
Received chunk:  to...

2025-04-02T02:55:57.958Z:
Received chunk:  a...

2025-04-02T02:55:58.047Z:
Received chunk:  boil...

2025-04-02T02:55:58.132Z:
Received chunk: ....

2025-04-02T02:55:58.218Z:
Received chunk:  Add...

2025-04-02T02:55:58.303Z:
Received chunk:  the...

2025-04-02T02:55:58.388Z:
Received chunk:  elbow...

2025-04-02T02:55:58.473Z:
Received chunk:  mac...

2025-04-02T02:55:58.558Z:
Received chunk: aron...

2025-04-02T02:55:58.643Z:
Received chunk: i...

2025-04-02T02:55:58.728Z:
Received chunk:  and...

2025-04-02T02:55:58.815Z:
Received chunk:  cook...

2025-04-02T02:55:58.904Z:
Received chunk:  until...

2025-04-02T02:55:58.994Z:
Received chunk:  al...

2025-04-02T02:55:59.082Z:
Received chunk:  d...

2025-04-02T02:55:59.167Z:
Received chunk: ente...

2025-04-02T02:55:59.254Z:
Received chunk: ,...

2025-04-02T02:55:59.340Z:
Received chunk:  about...

2025-04-02T02:55:59.425Z:
Received chunk:  ...

2025-04-02T02:55:59.511Z:
Received chunk: 8...

2025-04-02T02:55:59.595Z:
Received chunk: -...

2025-04-02T02:55:59.681Z:
Received chunk: 1...

2025-04-02T02:55:59.766Z:
Received chunk: 0...

2025-04-02T02:55:59.853Z:
Received chunk:  minutes...

2025-04-02T02:55:59.941Z:
Received chunk: ....

2025-04-02T02:56:00.031Z:
Received chunk:  Drain...

2025-04-02T02:56:00.117Z:
Received chunk:  well...

2025-04-02T02:56:00.202Z:
Received chunk: .

...

2025-04-02T02:56:00.288Z:
Received chunk: 2...

2025-04-02T02:56:00.373Z:
Received chunk: ....

2025-04-02T02:56:00.460Z:
Received chunk:  **...

2025-04-02T02:56:00.545Z:
Received chunk: Prepare...

2025-04-02T02:56:00.630Z:
Received chunk:  the...

2025-04-02T02:56:00.715Z:
Received chunk:  Spin...

2025-04-02T02:56:00.801Z:
Received chunk: ach...

2025-04-02T02:56:00.888Z:
Received chunk: **:...

2025-04-02T02:56:00.979Z:
Received chunk:  If...

2025-04-02T02:56:01.071Z:
Received chunk:  using...

2025-04-02T02:56:01.161Z:
Received chunk:  fresh...

2025-04-02T02:56:01.247Z:
Received chunk:  spinach...

2025-04-02T02:56:01.333Z:
Received chunk: ,...

2025-04-02T02:56:01.419Z:
Received chunk:  chop...

2025-04-02T02:56:01.504Z:
Received chunk:  it...

2025-04-02T02:56:01.589Z:
Received chunk:  into...

2025-04-02T02:56:01.675Z:
Received chunk:  bite...

2025-04-02T02:56:01.761Z:
Received chunk: -sized...

2025-04-02T02:56:01.847Z:
Received chunk:  pieces...

2025-04-02T02:56:01.936Z:
Received chunk:  and...

2025-04-02T02:56:02.026Z:
Received chunk:  toss...

2025-04-02T02:56:02.113Z:
Received chunk:  in...

2025-04-02T02:56:02.198Z:
Received chunk:  a...

2025-04-02T02:56:02.285Z:
Received chunk:  small...

2025-04-02T02:56:02.370Z:
Received chunk:  bowl...

2025-04-02T02:56:02.455Z:
Received chunk:  with...

2025-04-02T02:56:02.540Z:
Received chunk:  salt...

2025-04-02T02:56:02.624Z:
Received chunk: ,...

2025-04-02T02:56:02.710Z:
Received chunk:  pepper...

2025-04-02T02:56:02.802Z:
Received chunk: ,...

2025-04-02T02:56:02.882Z:
Received chunk:  and...

2025-04-02T02:56:02.972Z:
Received chunk:  some...

2025-04-02T02:56:03.060Z:
Received chunk:  olive...

2025-04-02T02:56:03.146Z:
Received chunk:  oil...

2025-04-02T02:56:03.231Z:
Received chunk:  or...

2025-04-02T02:56:03.318Z:
Received chunk:  butter...

2025-04-02T02:56:03.404Z:
Received chunk:  until...

2025-04-02T02:56:03.489Z:
Received chunk:  wilt...

2025-04-02T02:56:03.575Z:
Received chunk: ed...

2025-04-02T02:56:03.660Z:
Received chunk: .

...

2025-04-02T02:56:03.745Z:
Received chunk: 3...

2025-04-02T02:56:03.832Z:
Received chunk: ....

2025-04-02T02:56:03.922Z:
Received chunk:  **...

2025-04-02T02:56:04.009Z:
Received chunk: Combine...

2025-04-02T02:56:04.099Z:
Received chunk:  Pasta...

2025-04-02T02:56:04.184Z:
Received chunk:  and...

2025-04-02T02:56:04.269Z:
Received chunk:  Spin...

2025-04-02T02:56:04.355Z:
Received chunk: ach...

2025-04-02T02:56:04.441Z:
Received chunk: **:...

2025-04-02T02:56:04.525Z:
Received chunk:  Add...

2025-04-02T02:56:04.609Z:
Received chunk:  the...

2025-04-02T02:56:04.695Z:
Received chunk:  cooked...

2025-04-02T02:56:04.779Z:
Received chunk:  pasta...

2025-04-02T02:56:04.866Z:
Received chunk:  to...

2025-04-02T02:56:04.955Z:
Received chunk:  a...

2025-04-02T02:56:05.046Z:
Received chunk:  large...

2025-04-02T02:56:05.131Z:
Received chunk:  pot...

2025-04-02T02:56:05.216Z:
Received chunk: ....

2025-04-02T02:56:05.302Z:
Received chunk:  T...

2025-04-02T02:56:05.389Z:
Received chunk: oss...

2025-04-02T02:56:05.474Z:
Received chunk:  in...

2025-04-02T02:56:05.560Z:
Received chunk:  the...

2025-04-02T02:56:05.646Z:
Received chunk:  frozen...

2025-04-02T02:56:05.731Z:
Received chunk:  spinach...

2025-04-02T02:56:05.818Z:
Received chunk:  (...

2025-04-02T02:56:05.904Z:
Received chunk: or...

2025-04-02T02:56:05.997Z:
Received chunk:  half...

2025-04-02T02:56:06.086Z:
Received chunk:  of...

2025-04-02T02:56:06.170Z:
Received chunk:  the...

2025-04-02T02:56:06.255Z:
Received chunk:  fresh...

2025-04-02T02:56:06.341Z:
Received chunk:  spinach...

2025-04-02T02:56:06.427Z:
Received chunk:  if...

2025-04-02T02:56:06.513Z:
Received chunk:  you...

2025-04-02T02:56:06.598Z:
Received chunk: ’re...

2025-04-02T02:56:06.683Z:
Received chunk:  using...

2025-04-02T02:56:06.769Z:
Received chunk:  that...

2025-04-02T02:56:06.856Z:
Received chunk: )...

2025-04-02T02:56:06.945Z:
Received chunk:  and...

2025-04-02T02:56:07.037Z:
Received chunk:  any...

2025-04-02T02:56:07.123Z:
Received chunk:  remaining...

2025-04-02T02:56:07.209Z:
Received chunk:  olive...

2025-04-02T02:56:07.295Z:
Received chunk:  oil...

2025-04-02T02:56:07.381Z:
Received chunk: /b...

2025-04-02T02:56:07.465Z:
Received chunk: utter...

2025-04-02T02:56:07.551Z:
Received chunk: .

...

2025-04-02T02:56:07.637Z:
Received chunk: 4...

2025-04-02T02:56:07.722Z:
Received chunk: ....

2025-04-02T02:56:07.813Z:
Received chunk:  **...

2025-04-02T02:56:07.895Z:
Received chunk: Add...

2025-04-02T02:56:07.985Z:
Received chunk:  Tom...

2025-04-02T02:56:08.075Z:
Received chunk: atoes...

2025-04-02T02:56:08.161Z:
Received chunk: **:...

2025-04-02T02:56:08.246Z:
Received chunk:  Stir...

2025-04-02T02:56:08.331Z:
Received chunk:  in...

2025-04-02T02:56:08.417Z:
Received chunk:  the...

2025-04-02T02:56:08.503Z:
Received chunk:  crushed...

2025-04-02T02:56:08.588Z:
Received chunk:  tomatoes...

2025-04-02T02:56:08.674Z:
Received chunk: ,...

2025-04-02T02:56:08.760Z:
Received chunk:  making...

2025-04-02T02:56:08.846Z:
Received chunk:  sure...

2025-04-02T02:56:08.935Z:
Received chunk:  they...

2025-04-02T02:56:09.026Z:
Received chunk:  combine...

2025-04-02T02:56:09.115Z:
Received chunk:  well...

2025-04-02T02:56:09.201Z:
Received chunk:  with...

2025-04-02T02:56:09.287Z:
Received chunk:  the...

2025-04-02T02:56:09.372Z:
Received chunk:  pasta...

2025-04-02T02:56:09.460Z:
Received chunk:  and...

2025-04-02T02:56:09.546Z:
Received chunk:  spinach...

2025-04-02T02:56:09.632Z:
Received chunk: .

...

2025-04-02T02:56:09.718Z:
Received chunk: 5...

2025-04-02T02:56:09.805Z:
Received chunk: ....

2025-04-02T02:56:09.892Z:
Received chunk:  **...

2025-04-02T02:56:09.982Z:
Received chunk: Season...

2025-04-02T02:56:10.073Z:
Received chunk: **:...

2025-04-02T02:56:10.159Z:
Received chunk:  Spr...

2025-04-02T02:56:10.245Z:
Received chunk: inkle...

2025-04-02T02:56:10.332Z:
Received chunk:  salt...

2025-04-02T02:56:10.419Z:
Received chunk:  and...

2025-04-02T02:56:10.509Z:
Received chunk:  pepper...

2025-04-02T02:56:10.600Z:
Received chunk:  to...

2025-04-02T02:56:10.689Z:
Received chunk:  taste...

2025-04-02T02:56:10.781Z:
Received chunk: ....

2025-04-02T02:56:10.870Z:
Received chunk:  If...

2025-04-02T02:56:10.963Z:
Received chunk:  desired...

2025-04-02T02:56:11.059Z:
Received chunk: ,...

2025-04-02T02:56:11.150Z:
Received chunk:  toss...

2025-04-02T02:56:11.237Z:
Received chunk:  in...

2025-04-02T02:56:11.325Z:
Received chunk:  some...

2025-04-02T02:56:11.413Z:
Received chunk:  ric...

2025-04-02T02:56:11.501Z:
Received chunk: otta...

2025-04-02T02:56:11.589Z:
Received chunk:  for...

2025-04-02T02:56:11.677Z:
Received chunk:  extra...

2025-04-02T02:56:11.765Z:
Received chunk:  cream...

2025-04-02T02:56:11.852Z:
Received chunk: iness...

2025-04-02T02:56:11.939Z:
Received chunk: .

...

2025-04-02T02:56:12.029Z:
Received chunk: 6...

2025-04-02T02:56:12.119Z:
Received chunk: ....

2025-04-02T02:56:12.205Z:
Received chunk:  **...

2025-04-02T02:56:12.294Z:
Received chunk: Serve...

2025-04-02T02:56:12.387Z:
Received chunk: **:...

2025-04-02T02:56:12.478Z:
Received chunk:  Serve...

2025-04-02T02:56:12.565Z:
Received chunk:  hot...

2025-04-02T02:56:12.654Z:
Received chunk:  with...

2025-04-02T02:56:12.747Z:
Received chunk:  a...

2025-04-02T02:56:12.842Z:
Received chunk:  dol...

2025-04-02T02:56:12.941Z:
Received chunk: lop...

2025-04-02T02:56:13.041Z:
Received chunk:  of...

2025-04-02T02:56:13.134Z:
Received chunk:  fresh...

2025-04-02T02:56:13.221Z:
Received chunk:  basil...

2025-04-02T02:56:13.307Z:
Received chunk:  or...

2025-04-02T02:56:13.394Z:
Received chunk:  parsley...

2025-04-02T02:56:13.479Z:
Received chunk:  on...

2025-04-02T02:56:13.566Z:
Received chunk:  top...

2025-04-02T02:56:13.654Z:
Received chunk:  if...

2025-04-02T02:56:13.744Z:
Received chunk:  you...

2025-04-02T02:56:13.831Z:
Received chunk: ’re...

2025-04-02T02:56:13.919Z:
Received chunk:  using...

2025-04-02T02:56:14.009Z:
Received chunk:  fresh...

2025-04-02T02:56:14.101Z:
Received chunk:  spinach...

2025-04-02T02:56:14.187Z:
Received chunk: .

...

2025-04-02T02:56:14.273Z:
Received chunk: ---

...

2025-04-02T02:56:14.359Z:
Received chunk: This...

2025-04-02T02:56:14.445Z:
Received chunk:  is...

2025-04-02T02:56:14.531Z:
Received chunk:  a...

2025-04-02T02:56:14.617Z:
Received chunk:  quick...

2025-04-02T02:56:14.702Z:
Received chunk: ,...

2025-04-02T02:56:14.788Z:
Received chunk:  hearty...

2025-04-02T02:56:14.875Z:
Received chunk: ,...

2025-04-02T02:56:14.969Z:
Received chunk:  and...

2025-04-02T02:56:15.055Z:
Received chunk:  flavorful...

2025-04-02T02:56:15.144Z:
Received chunk:  dish...

2025-04-02T02:56:15.230Z:
Received chunk:  that...

2025-04-02T02:56:15.317Z:
Received chunk: ’s...

2025-04-02T02:56:15.408Z:
Received chunk:  perfect...

2025-04-02T02:56:15.498Z:
Received chunk:  for...

2025-04-02T02:56:15.590Z:
Received chunk:  busy...

2025-04-02T02:56:15.679Z:
Received chunk:  days...

2025-04-02T02:56:15.764Z:
Received chunk: !...

2025-04-02T02:56:15.854Z:
Received chunk:  Enjoy...

2025-04-02T02:56:15.939Z:
Received chunk: !...

2025-04-02T02:56:16.029Z:
Received chunk:  �...

2025-04-02T02:56:16.118Z:
Received chunk: �...

2025-04-02T02:56:16.205Z:
Received chunk: �...

2025-04-02T02:56:16.291Z:
Received chunk: 

> ...

2025-04-02T02:56:25.448Z:
Generating HTML from 2 messages

2025-04-02T02:56:25.448Z:
Spawning process for HTML generation: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:56:25.587Z:
HTML Generation stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:56:25.610Z:
HTML Generation stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:56:25.632Z:
HTML Generation stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:56:25.642Z:
HTML Generation stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:56:25.664Z:
HTML Generation stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:56:25.717Z:
HTML Generation stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:56:25.718Z:
HTML Generation stderr: load: special tokens cache size = 22


2025-04-02T02:56:25.735Z:
HTML Generation stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0


2025-04-02T02:56:25.735Z:
HTML Generation stderr: print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:56:25.968Z:
HTML Generation stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:56:25.989Z:
HTML Generation stderr: .

2025-04-02T02:56:26.007Z:
HTML Generation stderr: .

2025-04-02T02:56:26.027Z:
HTML Generation stderr: .

2025-04-02T02:56:26.044Z:
HTML Generation stderr: .

2025-04-02T02:56:26.062Z:
HTML Generation stderr: .

2025-04-02T02:56:26.080Z:
HTML Generation stderr: .

2025-04-02T02:56:26.101Z:
HTML Generation stderr: .

2025-04-02T02:56:26.133Z:
HTML Generation stderr: .

2025-04-02T02:56:26.141Z:
HTML Generation stderr: .

2025-04-02T02:56:26.173Z:
HTML Generation stderr: .

2025-04-02T02:56:26.190Z:
HTML Generation stderr: .

2025-04-02T02:56:26.212Z:
HTML Generation stderr: .

2025-04-02T02:56:26.228Z:
HTML Generation stderr: .

2025-04-02T02:56:26.252Z:
HTML Generation stderr: .

2025-04-02T02:56:26.267Z:
HTML Generation stderr: .

2025-04-02T02:56:26.283Z:
HTML Generation stderr: .

2025-04-02T02:56:26.307Z:
HTML Generation stderr: .

2025-04-02T02:56:26.323Z:
HTML Generation stderr: .

2025-04-02T02:56:26.339Z:
HTML Generation stderr: .

2025-04-02T02:56:26.363Z:
HTML Generation stderr: .

2025-04-02T02:56:26.380Z:
HTML Generation stderr: .

2025-04-02T02:56:26.405Z:
HTML Generation stderr: .

2025-04-02T02:56:26.421Z:
HTML Generation stderr: .

2025-04-02T02:56:26.442Z:
HTML Generation stderr: .

2025-04-02T02:56:26.462Z:
HTML Generation stderr: .

2025-04-02T02:56:26.495Z:
HTML Generation stderr: .

2025-04-02T02:56:26.519Z:
HTML Generation stderr: .

2025-04-02T02:56:26.537Z:
HTML Generation stderr: .

2025-04-02T02:56:26.562Z:
HTML Generation stderr: .

2025-04-02T02:56:26.578Z:
HTML Generation stderr: .

2025-04-02T02:56:26.597Z:
HTML Generation stderr: .

2025-04-02T02:56:26.621Z:
HTML Generation stderr: .

2025-04-02T02:56:26.637Z:
HTML Generation stderr: .

2025-04-02T02:56:26.654Z:
HTML Generation stderr: .

2025-04-02T02:56:26.679Z:
HTML Generation stderr: .

2025-04-02T02:56:26.698Z:
HTML Generation stderr: .

2025-04-02T02:56:26.722Z:
HTML Generation stderr: .

2025-04-02T02:56:26.738Z:
HTML Generation stderr: .

2025-04-02T02:56:26.759Z:
HTML Generation stderr: .

2025-04-02T02:56:26.782Z:
HTML Generation stderr: .

2025-04-02T02:56:26.817Z:
HTML Generation stderr: .

2025-04-02T02:56:26.825Z:
HTML Generation stderr: .

2025-04-02T02:56:26.864Z:
HTML Generation stderr: .

2025-04-02T02:56:26.877Z:
HTML Generation stderr: .

2025-04-02T02:56:26.912Z:
HTML Generation stderr: .

2025-04-02T02:56:26.931Z:
HTML Generation stderr: .

2025-04-02T02:56:26.963Z:
HTML Generation stderr: .

2025-04-02T02:56:26.976Z:
HTML Generation stderr: .

2025-04-02T02:56:26.995Z:
HTML Generation stderr: .

2025-04-02T02:56:27.021Z:
HTML Generation stderr: .

2025-04-02T02:56:27.040Z:
HTML Generation stderr: .

2025-04-02T02:56:27.068Z:
HTML Generation stderr: .

2025-04-02T02:56:27.086Z:
HTML Generation stderr: .

2025-04-02T02:56:27.103Z:
HTML Generation stderr: .

2025-04-02T02:56:27.128Z:
HTML Generation stderr: .

2025-04-02T02:56:27.144Z:
HTML Generation stderr: .

2025-04-02T02:56:27.165Z:
HTML Generation stderr: .

2025-04-02T02:56:27.186Z:
HTML Generation stderr: .

2025-04-02T02:56:27.207Z:
HTML Generation stderr: .

2025-04-02T02:56:27.228Z:
HTML Generation stderr: .

2025-04-02T02:56:27.249Z:
HTML Generation stderr: .

2025-04-02T02:56:27.286Z:
HTML Generation stderr: .

2025-04-02T02:56:27.293Z:
HTML Generation stderr: .

2025-04-02T02:56:27.328Z:
HTML Generation stderr: .

2025-04-02T02:56:27.335Z:
HTML Generation stderr: .

2025-04-02T02:56:27.369Z:
HTML Generation stderr: .....

2025-04-02T02:56:27.369Z:
HTML Generation stderr: ........

2025-04-02T02:56:27.369Z:
HTML Generation stderr: .....


2025-04-02T02:56:27.372Z:
HTML Generation stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T02:56:27.372Z:
HTML Generation stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T02:56:27.411Z:
HTML Generation stderr: init:        CPU KV buffer size =   224.00 MiB
llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB


2025-04-02T02:56:27.417Z:
HTML Generation stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T02:56:27.671Z:
HTML Generation stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
You are a helpful assistant

<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>

system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 


2025-04-02T02:56:27.671Z:
HTML Generation stderr: 


2025-04-02T02:56:27.674Z:
HTML Generation stderr: main: interactive mode on.


2025-04-02T02:56:27.675Z:
HTML Generation stderr: sampler seed: 2438162892
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 4096, n_batch = 2048, n_predict = 4096, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T02:56:55.460Z:
Generating fallback HTML from 2 messages

2025-04-02T02:56:55.461Z:
Spawning process for fallback HTML generation: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T02:56:55.517Z:
Fallback HTML stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T02:56:55.563Z:
Fallback HTML stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T02:56:55.593Z:
Fallback HTML stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T02:56:55.604Z:
Fallback HTML stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T02:56:55.642Z:
Fallback HTML stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T02:56:55.816Z:
Fallback HTML stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T02:56:55.817Z:
Fallback HTML stderr: load: special tokens cache size = 22


2025-04-02T02:56:55.845Z:
Fallback HTML stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T02:56:56.175Z:
Fallback HTML stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T02:56:56.213Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.245Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.286Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.313Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.345Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.381Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.417Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.475Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.488Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.543Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.571Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.610Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.639Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.680Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.705Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.734Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.774Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.801Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.830Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.868Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.897Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.940Z:
Fallback HTML stderr: .

2025-04-02T02:56:56.965Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.001Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.036Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.087Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.126Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.152Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.195Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.234Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.268Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.317Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.338Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.369Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.413Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.441Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.482Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.509Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.542Z:
Fallback HTML stderr: .

2025-04-02T02:56:57.578Z:
Fallback HTML stderr: .
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
2025-04-02T03:07:06.812Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T03:07:06.812Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:07:06.812Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T03:07:06.813Z:
Model mmproj-model-f16 file verified

2025-04-02T03:07:07.218Z:
Unloading model...

2025-04-02T03:07:07.218Z:
Model unloaded successfully

2025-04-02T03:07:07.218Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T03:07:07.218Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:07:07.218Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf

2025-04-02T03:07:07.219Z:
Model mmproj-model-f16 file verified

2025-04-02T03:07:09.953Z:
Generate response called with message: test...

2025-04-02T03:07:09.953Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf --prompt USER: test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:07:10.041Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:07:10.048Z:
Process stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-02T03:07:10.048Z:
Process stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T03:07:10.049Z:
Process exited with code 1

2025-04-02T03:07:10.049Z:
Process failed, falling back to simulation

2025-04-02T03:07:10.050Z:
Generating simulated response

2025-04-02T03:07:10.115Z:
Sent simulated chunk: I'm

2025-04-02T03:07:10.178Z:
Sent simulated chunk: a

2025-04-02T03:07:10.240Z:
Sent simulated chunk: locally

2025-04-02T03:07:10.287Z:
Sent simulated chunk: hosted

2025-04-02T03:07:10.334Z:
Sent simulated chunk: AI

2025-04-02T03:07:10.381Z:
Sent simulated chunk: assistant

2025-04-02T03:07:10.429Z:
Sent simulated chunk: running

2025-04-02T03:07:10.509Z:
Sent simulated chunk: directly

2025-04-02T03:07:10.586Z:
Sent simulated chunk: on

2025-04-02T03:07:10.663Z:
Sent simulated chunk: your

2025-04-02T03:07:10.757Z:
Sent simulated chunk: device.

2025-04-02T03:07:10.819Z:
Sent simulated chunk: I

2025-04-02T03:07:10.865Z:
Sent simulated chunk: process

2025-04-02T03:07:10.973Z:
Sent simulated chunk: information

2025-04-02T03:07:11.068Z:
Sent simulated chunk: using

2025-04-02T03:07:11.177Z:
Sent simulated chunk: the

2025-04-02T03:07:11.286Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T03:07:11.333Z:
Sent simulated chunk: model

2025-04-02T03:07:11.412Z:
Sent simulated chunk: loaded

2025-04-02T03:07:11.491Z:
Sent simulated chunk: in

2025-04-02T03:07:11.569Z:
Sent simulated chunk: the

2025-04-02T03:07:11.616Z:
Sent simulated chunk: LM

2025-04-02T03:07:11.687Z:
Sent simulated chunk: Terminal

2025-04-02T03:07:11.758Z:
Sent simulated chunk: application.

2025-04-02T03:07:11.805Z:
Sent simulated chunk: I've

2025-04-02T03:07:11.869Z:
Sent simulated chunk: received

2025-04-02T03:07:11.977Z:
Sent simulated chunk: your

2025-04-02T03:07:12.072Z:
Sent simulated chunk: message:

2025-04-02T03:07:12.120Z:
Sent simulated chunk: "test"

2025-04-02T03:07:12.213Z:
Sent simulated chunk: and

2025-04-02T03:07:12.277Z:
Sent simulated chunk: am

2025-04-02T03:07:12.385Z:
Sent simulated chunk: responding

2025-04-02T03:07:12.480Z:
Sent simulated chunk: without

2025-04-02T03:07:12.574Z:
Sent simulated chunk: requiring

2025-04-02T03:07:12.653Z:
Sent simulated chunk: internet

2025-04-02T03:07:12.699Z:
Sent simulated chunk: access.

(This

2025-04-02T03:07:12.793Z:
Sent simulated chunk: response

2025-04-02T03:07:12.856Z:
Sent simulated chunk: was

2025-04-02T03:07:12.918Z:
Sent simulated chunk: generated

2025-04-02T03:07:12.965Z:
Sent simulated chunk: using

2025-04-02T03:07:13.058Z:
Sent simulated chunk: a

2025-04-02T03:07:13.105Z:
Sent simulated chunk: simulated

2025-04-02T03:07:13.186Z:
Sent simulated chunk: version

2025-04-02T03:07:13.250Z:
Sent simulated chunk: of

2025-04-02T03:07:13.313Z:
Sent simulated chunk: the

2025-04-02T03:07:13.392Z:
Sent simulated chunk: mmproj-model-f16

2025-04-02T03:07:13.472Z:
Sent simulated chunk: model)

2025-04-02T03:07:13.473Z:
Simulated response generation completed

2025-04-02T03:07:14.865Z:
Generating HTML from 2 messages

2025-04-02T03:07:14.865Z:
Spawning process for HTML generation: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:07:14.888Z:
HTML Generation stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:07:14.889Z:
HTML Generation stderr: llama_model_loader: loaded meta data with 16 key-value pairs and 439 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = clip
llama_model_loader: - kv   1:                        clip.projector_type str              = gemma3
llama_model_loader: - kv   2:                      clip.has_text_encoder bool             = false
llama_model_loader: - kv   3:                    clip.has_vision_encoder bool             = true
llama_model_loader: - kv   4:                   clip.has_llava_projector bool             = false
llama_model_loader: - kv   5:                     clip.vision.image_size u32              = 896
llama_model_loader: - kv   6:                     clip.vision.patch_size u32              = 14
llama_model_loader: - kv   7:               clip.vision.embedding_length u32              = 1152
llama_model_loader: - kv   8:            clip.vision.feed_forward_length u32              = 4304
llama_model_loader: - kv   9:                 clip.vision.projection_dim u32              = 2560
llama_model_loader: - kv  10:                    clip.vision.block_count u32              = 27
llama_model_loader: - kv  11:           clip.vision.attention.head_count u32              = 16
llama_model_loader: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
llama_model_loader: - kv  15:                              clip.use_gelu bool             = true
llama_model_loader: - type  f32:  276 tensors
llama_model_loader: - type  f16:  163 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 811.79 MiB (16.22 BPW) 


2025-04-02T03:07:14.890Z:
HTML Generation stderr: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'C:\Users\tyler\.lmstudio\models\lmstudio-community\gemma-3-4b-it-GGUF\mmproj-model-f16.gguf'
main: error: unable to load model


2025-04-02T03:07:14.891Z:
HTML Generation process exited with code 1

2025-04-02T03:13:49.713Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T03:13:49.714Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:13:49.714Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T03:13:49.714Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T03:13:50.601Z:
Unloading model...

2025-04-02T03:13:50.601Z:
Model unloaded successfully

2025-04-02T03:13:50.601Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T03:13:50.601Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:13:50.601Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf

2025-04-02T03:13:50.602Z:
Model DeepSeek-R1-Distill-Qwen-7B-Q4_K_M file verified

2025-04-02T03:13:56.291Z:
Generate response called with message: give me a short recipe...

2025-04-02T03:13:56.291Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf --prompt USER: give me a short recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:13:56.326Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:13:56.354Z:
Process stderr: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen


2025-04-02T03:13:56.376Z:
Process stderr: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:13:56.385Z:
Process stderr: llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:13:56.408Z:
Process stderr: llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 


2025-04-02T03:13:56.463Z:
Process stderr: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect


2025-04-02T03:13:56.464Z:
Process stderr: load: special tokens cache size = 22


2025-04-02T03:13:56.480Z:
Process stderr: load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0


2025-04-02T03:13:56.481Z:
Process stderr: print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = DeepSeek R1 Distill Qwen 7B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:13:58.029Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/29 layers to GPU
load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB
load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB


2025-04-02T03:13:58.052Z:
Process stderr: .

2025-04-02T03:13:58.072Z:
Process stderr: .

2025-04-02T03:13:58.091Z:
Process stderr: .

2025-04-02T03:13:58.110Z:
Process stderr: .

2025-04-02T03:13:58.129Z:
Process stderr: .

2025-04-02T03:13:58.148Z:
Process stderr: .

2025-04-02T03:13:58.167Z:
Process stderr: .

2025-04-02T03:13:58.198Z:
Process stderr: .

2025-04-02T03:13:58.205Z:
Process stderr: .

2025-04-02T03:13:58.235Z:
Process stderr: .

2025-04-02T03:13:58.251Z:
Process stderr: .

2025-04-02T03:13:58.273Z:
Process stderr: .

2025-04-02T03:13:58.289Z:
Process stderr: .

2025-04-02T03:13:58.313Z:
Process stderr: .

2025-04-02T03:13:58.328Z:
Process stderr: .

2025-04-02T03:13:58.344Z:
Process stderr: .

2025-04-02T03:13:58.367Z:
Process stderr: .

2025-04-02T03:13:58.382Z:
Process stderr: .

2025-04-02T03:13:58.398Z:
Process stderr: .

2025-04-02T03:13:58.421Z:
Process stderr: .

2025-04-02T03:13:58.438Z:
Process stderr: .

2025-04-02T03:13:58.461Z:
Process stderr: .

2025-04-02T03:13:58.476Z:
Process stderr: .

2025-04-02T03:13:58.495Z:
Process stderr: .

2025-04-02T03:13:58.515Z:
Process stderr: .

2025-04-02T03:13:58.547Z:
Process stderr: .

2025-04-02T03:13:58.569Z:
Process stderr: .

2025-04-02T03:13:58.585Z:
Process stderr: .

2025-04-02T03:13:58.609Z:
Process stderr: .

2025-04-02T03:13:58.624Z:
Process stderr: .

2025-04-02T03:13:58.641Z:
Process stderr: .

2025-04-02T03:13:58.664Z:
Process stderr: .

2025-04-02T03:13:58.679Z:
Process stderr: .

2025-04-02T03:13:58.695Z:
Process stderr: .

2025-04-02T03:13:58.718Z:
Process stderr: .

2025-04-02T03:13:58.734Z:
Process stderr: .

2025-04-02T03:13:58.757Z:
Process stderr: .

2025-04-02T03:13:58.773Z:
Process stderr: .

2025-04-02T03:13:58.792Z:
Process stderr: .

2025-04-02T03:13:58.812Z:
Process stderr: .

2025-04-02T03:13:58.843Z:
Process stderr: .

2025-04-02T03:13:58.850Z:
Process stderr: .

2025-04-02T03:13:58.882Z:
Process stderr: .

2025-04-02T03:13:58.889Z:
Process stderr: .

2025-04-02T03:13:58.920Z:
Process stderr: .

2025-04-02T03:13:58.936Z:
Process stderr: .

2025-04-02T03:13:58.959Z:
Process stderr: .

2025-04-02T03:13:58.974Z:
Process stderr: .

2025-04-02T03:13:58.990Z:
Process stderr: .

2025-04-02T03:13:59.014Z:
Process stderr: .

2025-04-02T03:13:59.031Z:
Process stderr: .

2025-04-02T03:13:59.054Z:
Process stderr: .

2025-04-02T03:13:59.070Z:
Process stderr: .

2025-04-02T03:13:59.086Z:
Process stderr: .

2025-04-02T03:13:59.109Z:
Process stderr: .

2025-04-02T03:13:59.125Z:
Process stderr: .

2025-04-02T03:13:59.144Z:
Process stderr: .

2025-04-02T03:13:59.164Z:
Process stderr: .

2025-04-02T03:13:59.183Z:
Process stderr: .

2025-04-02T03:13:59.203Z:
Process stderr: .

2025-04-02T03:13:59.222Z:
Process stderr: .

2025-04-02T03:13:59.257Z:
Process stderr: .

2025-04-02T03:13:59.264Z:
Process stderr: .

2025-04-02T03:13:59.296Z:
Process stderr: .

2025-04-02T03:13:59.303Z:
Process stderr: .

2025-04-02T03:13:59.336Z:
Process stderr: .....

2025-04-02T03:13:59.336Z:
Process stderr: ........

2025-04-02T03:13:59.336Z:
Process stderr: .....


2025-04-02T03:13:59.339Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:13:59.339Z:
Process stderr: llama_context:        CPU  output buffer size =     0.58 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1


2025-04-02T03:13:59.358Z:
Process stderr: init:        CPU KV buffer size =   112.00 MiB
llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB


2025-04-02T03:13:59.363Z:
Process stderr: llama_context:        CPU compute buffer size =   304.00 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:22:01.858Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:22:01.858Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:22:01.859Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:22:01.859Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:22:02.840Z:
Unloading model...

2025-04-02T03:22:02.840Z:
Model unloaded successfully

2025-04-02T03:22:02.840Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:22:02.840Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:22:02.840Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:22:02.841Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:22:18.080Z:
Generate response called with message: write me a quick cooking recipe...

2025-04-02T03:22:18.081Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: write me a quick cooking recipe
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:22:18.178Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:22:18.204Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:22:18.222Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:22:18.230Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:22:18.270Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:22:18.350Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:22:18.365Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0


2025-04-02T03:22:18.365Z:
Process stderr: print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:22:18.437Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.....

2025-04-02T03:22:18.437Z:
Process stderr: .........................................................


2025-04-02T03:22:18.439Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:22:18.439Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:22:18.450Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:22:18.455Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:22:18.614Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T03:22:18.614Z:
Process stderr: main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:22:18.615Z:
Process stderr: sampler seed: 4101232493
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:22:18.693Z:
Received chunk: I...

2025-04-02T03:22:18.716Z:
Received chunk: 'd...

2025-04-02T03:22:18.741Z:
Received chunk:  be...

2025-04-02T03:22:18.764Z:
Received chunk:  happy...

2025-04-02T03:22:18.788Z:
Received chunk:  to...

2025-04-02T03:22:18.811Z:
Received chunk:  help...

2025-04-02T03:22:18.834Z:
Received chunk:  you...

2025-04-02T03:22:18.857Z:
Received chunk:  with...

2025-04-02T03:22:18.881Z:
Received chunk:  a...

2025-04-02T03:22:18.904Z:
Received chunk:  quick...

2025-04-02T03:22:18.928Z:
Received chunk:  and...

2025-04-02T03:22:18.950Z:
Received chunk:  delicious...

2025-04-02T03:22:18.973Z:
Received chunk:  cooking...

2025-04-02T03:22:18.996Z:
Received chunk:  recipe...

2025-04-02T03:22:19.019Z:
Received chunk: ....

2025-04-02T03:22:19.043Z:
Received chunk:  Here...

2025-04-02T03:22:19.067Z:
Received chunk: 's...

2025-04-02T03:22:19.090Z:
Received chunk:  one...

2025-04-02T03:22:19.114Z:
Received chunk:  of...

2025-04-02T03:22:19.138Z:
Received chunk:  my...

2025-04-02T03:22:19.161Z:
Received chunk:  favorites...

2025-04-02T03:22:19.185Z:
Received chunk: :

...

2025-04-02T03:22:19.210Z:
Received chunk: **...

2025-04-02T03:22:19.233Z:
Received chunk: Quick...

2025-04-02T03:22:19.257Z:
Received chunk:  Chicken...

2025-04-02T03:22:19.282Z:
Received chunk:  F...

2025-04-02T03:22:19.306Z:
Received chunk: aj...

2025-04-02T03:22:19.330Z:
Received chunk: itas...

2025-04-02T03:22:19.353Z:
Received chunk: **

...

2025-04-02T03:22:19.377Z:
Received chunk: Ingredients...

2025-04-02T03:22:19.400Z:
Received chunk: :

...

2025-04-02T03:22:19.426Z:
Received chunk: *...

2025-04-02T03:22:19.450Z:
Received chunk:  ...

2025-04-02T03:22:19.474Z:
Received chunk: 1...

2025-04-02T03:22:19.498Z:
Received chunk:  lb...

2025-04-02T03:22:19.530Z:
Received chunk:  bone...

2025-04-02T03:22:19.547Z:
Received chunk: less...

2025-04-02T03:22:19.571Z:
Received chunk: ,...

2025-04-02T03:22:19.595Z:
Received chunk:  skin...

2025-04-02T03:22:19.619Z:
Received chunk: less...

2025-04-02T03:22:19.643Z:
Received chunk:  chicken...

2025-04-02T03:22:19.668Z:
Received chunk:  breast...

2025-04-02T03:22:19.693Z:
Received chunk:  or...

2025-04-02T03:22:19.718Z:
Received chunk:  thighs...

2025-04-02T03:22:19.742Z:
Received chunk: ,...

2025-04-02T03:22:19.767Z:
Received chunk:  cut...

2025-04-02T03:22:19.792Z:
Received chunk:  into...

2025-04-02T03:22:19.816Z:
Received chunk:  thin...

2025-04-02T03:22:19.839Z:
Received chunk:  strips...

2025-04-02T03:22:19.863Z:
Received chunk: 
...

2025-04-02T03:22:19.886Z:
Received chunk: *...

2025-04-02T03:22:19.910Z:
Received chunk:  ...

2025-04-02T03:22:19.934Z:
Received chunk: 1...

2025-04-02T03:22:19.957Z:
Received chunk: /...

2025-04-02T03:22:19.980Z:
Received chunk: 2...

2025-04-02T03:22:20.003Z:
Received chunk:  cup...

2025-04-02T03:22:20.028Z:
Received chunk:  sliced...

2025-04-02T03:22:20.051Z:
Received chunk:  bell...

2025-04-02T03:22:20.075Z:
Received chunk:  peppers...

2025-04-02T03:22:20.098Z:
Received chunk:  (...

2025-04-02T03:22:20.122Z:
Received chunk: any...

2025-04-02T03:22:20.146Z:
Received chunk:  color...

2025-04-02T03:22:20.169Z:
Received chunk: )
...

2025-04-02T03:22:20.193Z:
Received chunk: *...

2025-04-02T03:22:20.217Z:
Received chunk:  ...

2025-04-02T03:22:20.241Z:
Received chunk: 1...

2025-04-02T03:22:20.266Z:
Received chunk: /...

2025-04-02T03:22:20.290Z:
Received chunk: 2...

2025-04-02T03:22:20.314Z:
Received chunk:  cup...

2025-04-02T03:22:20.338Z:
Received chunk:  sliced...

2025-04-02T03:22:20.361Z:
Received chunk:  onion...

2025-04-02T03:22:20.385Z:
Received chunk: 
...

2025-04-02T03:22:20.410Z:
Received chunk: *...

2025-04-02T03:22:20.435Z:
Received chunk:  ...

2025-04-02T03:22:20.459Z:
Received chunk: 2...

2025-04-02T03:22:20.484Z:
Received chunk:  cloves...

2025-04-02T03:22:20.508Z:
Received chunk:  garlic...

2025-04-02T03:22:20.539Z:
Received chunk: ,...

2025-04-02T03:22:20.558Z:
Received chunk:  minced...

2025-04-02T03:22:20.583Z:
Received chunk: 
...

2025-04-02T03:22:20.607Z:
Received chunk: *...

2025-04-02T03:22:20.632Z:
Received chunk:  ...

2025-04-02T03:22:20.656Z:
Received chunk: 1...

2025-04-02T03:22:20.681Z:
Received chunk:  tablespoon...

2025-04-02T03:22:20.706Z:
Received chunk:  olive...

2025-04-02T03:22:20.731Z:
Received chunk:  oil...

2025-04-02T03:22:20.756Z:
Received chunk: 
...

2025-04-02T03:22:20.780Z:
Received chunk: *...

2025-04-02T03:22:20.805Z:
Received chunk:  ...

2025-04-02T03:22:20.829Z:
Received chunk: 1...

2025-04-02T03:22:20.852Z:
Received chunk:  teaspoon...

2025-04-02T03:22:20.876Z:
Received chunk:  c...

2025-04-02T03:22:20.899Z:
Received chunk: umin...

2025-04-02T03:22:20.923Z:
Received chunk: 
...

2025-04-02T03:22:20.947Z:
Received chunk: *...

2025-04-02T03:22:20.971Z:
Received chunk:  Salt...

2025-04-02T03:22:20.994Z:
Received chunk:  and...

2025-04-02T03:22:21.018Z:
Received chunk:  pepper...

2025-04-02T03:22:21.041Z:
Received chunk:  to...

2025-04-02T03:22:21.065Z:
Received chunk:  taste...

2025-04-02T03:22:21.089Z:
Received chunk: 
...

2025-04-02T03:22:21.112Z:
Received chunk: *...

2025-04-02T03:22:21.135Z:
Received chunk:  ...

2025-04-02T03:22:21.159Z:
Received chunk: 4...

2025-04-02T03:22:21.182Z:
Received chunk:  small...

2025-04-02T03:22:21.205Z:
Received chunk:  flour...

2025-04-02T03:22:21.229Z:
Received chunk:  tort...

2025-04-02T03:22:21.253Z:
Received chunk: illas...

2025-04-02T03:22:21.277Z:
Received chunk: 
...

2025-04-02T03:22:21.300Z:
Received chunk: *...

2025-04-02T03:22:21.325Z:
Received chunk:  Optional...

2025-04-02T03:22:21.348Z:
Received chunk:  toppings...

2025-04-02T03:22:21.372Z:
Received chunk: :...

2025-04-02T03:22:21.396Z:
Received chunk:  avocado...

2025-04-02T03:22:21.421Z:
Received chunk: ,...

2025-04-02T03:22:21.445Z:
Received chunk:  sour...

2025-04-02T03:22:21.470Z:
Received chunk:  cream...

2025-04-02T03:22:21.494Z:
Received chunk: ,...

2025-04-02T03:22:21.518Z:
Received chunk:  shredded...

2025-04-02T03:22:21.546Z:
Received chunk:  cheese...

2025-04-02T03:22:21.568Z:
Received chunk: ,...

2025-04-02T03:22:21.594Z:
Received chunk:  cil...

2025-04-02T03:22:21.618Z:
Received chunk: antro...

2025-04-02T03:22:21.642Z:
Received chunk: 

...

2025-04-02T03:22:21.667Z:
Received chunk: Instructions...

2025-04-02T03:22:21.697Z:
Received chunk: :

...

2025-04-02T03:22:21.716Z:
Received chunk: 1...

2025-04-02T03:22:21.742Z:
Received chunk: ....

2025-04-02T03:22:21.766Z:
Received chunk:  Heat...

2025-04-02T03:22:21.791Z:
Received chunk:  the...

2025-04-02T03:22:21.816Z:
Received chunk:  olive...

2025-04-02T03:22:21.840Z:
Received chunk:  oil...

2025-04-02T03:22:21.863Z:
Received chunk:  in...

2025-04-02T03:22:21.887Z:
Received chunk:  a...

2025-04-02T03:22:21.911Z:
Received chunk:  large...

2025-04-02T03:22:21.934Z:
Received chunk:  skillet...

2025-04-02T03:22:21.958Z:
Received chunk:  over...

2025-04-02T03:22:21.982Z:
Received chunk:  medium...

2025-04-02T03:22:22.005Z:
Received chunk: -high...

2025-04-02T03:22:22.029Z:
Received chunk:  heat...

2025-04-02T03:22:22.053Z:
Received chunk: .
...

2025-04-02T03:22:22.077Z:
Received chunk: 2...

2025-04-02T03:22:22.101Z:
Received chunk: ....

2025-04-02T03:22:22.125Z:
Received chunk:  Add...

2025-04-02T03:22:22.149Z:
Received chunk:  the...

2025-04-02T03:22:22.172Z:
Received chunk:  chicken...

2025-04-02T03:22:22.196Z:
Received chunk:  and...

2025-04-02T03:22:22.219Z:
Received chunk:  cook...

2025-04-02T03:22:22.242Z:
Received chunk:  until...

2025-04-02T03:22:22.266Z:
Received chunk:  brown...

2025-04-02T03:22:22.290Z:
Received chunk: ed...

2025-04-02T03:22:22.314Z:
Received chunk:  and...

2025-04-02T03:22:22.338Z:
Received chunk:  cooked...

2025-04-02T03:22:22.362Z:
Received chunk:  through...

2025-04-02T03:22:22.386Z:
Received chunk:  (...

2025-04-02T03:22:22.411Z:
Received chunk: about...

2025-04-02T03:22:22.436Z:
Received chunk:  ...

2025-04-02T03:22:22.460Z:
Received chunk: 5...

2025-04-02T03:22:22.484Z:
Received chunk: -...

2025-04-02T03:22:22.509Z:
Received chunk: 6...

2025-04-02T03:22:22.534Z:
Received chunk:  minutes...

2025-04-02T03:22:22.561Z:
Received chunk: )....

2025-04-02T03:22:22.583Z:
Received chunk:  Remove...

2025-04-02T03:22:22.608Z:
Received chunk:  from...

2025-04-02T03:22:22.632Z:
Received chunk:  the...

2025-04-02T03:22:22.657Z:
Received chunk:  skillet...

2025-04-02T03:22:22.681Z:
Received chunk:  and...

2025-04-02T03:22:22.706Z:
Received chunk:  set...

2025-04-02T03:22:22.732Z:
Received chunk:  aside...

2025-04-02T03:22:22.758Z:
Received chunk: .
...

2025-04-02T03:22:22.783Z:
Received chunk: 3...

2025-04-02T03:22:22.807Z:
Received chunk: ....

2025-04-02T03:22:22.831Z:
Received chunk:  Add...

2025-04-02T03:22:22.854Z:
Received chunk:  more...

2025-04-02T03:22:22.878Z:
Received chunk:  oil...

2025-04-02T03:22:22.903Z:
Received chunk:  if...

2025-04-02T03:22:22.926Z:
Received chunk:  needed...

2025-04-02T03:22:22.950Z:
Received chunk:  to...

2025-04-02T03:22:22.973Z:
Received chunk:  coat...

2025-04-02T03:22:22.997Z:
Received chunk:  the...

2025-04-02T03:22:23.021Z:
Received chunk:  bottom...

2025-04-02T03:22:23.045Z:
Received chunk:  of...

2025-04-02T03:22:23.068Z:
Received chunk:  the...

2025-04-02T03:22:23.092Z:
Received chunk:  skillet...

2025-04-02T03:22:23.115Z:
Received chunk: .
...

2025-04-02T03:22:23.140Z:
Received chunk: 4...

2025-04-02T03:22:23.163Z:
Received chunk: ....

2025-04-02T03:22:23.187Z:
Received chunk:  Add...

2025-04-02T03:22:23.210Z:
Received chunk:  the...

2025-04-02T03:22:23.234Z:
Received chunk:  sliced...

2025-04-02T03:22:23.258Z:
Received chunk:  bell...

2025-04-02T03:22:23.282Z:
Received chunk:  peppers...

2025-04-02T03:22:23.306Z:
Received chunk:  and...

2025-04-02T03:22:23.330Z:
Received chunk:  onion...

2025-04-02T03:22:23.354Z:
Received chunk: ,...

2025-04-02T03:22:23.379Z:
Received chunk:  and...

2025-04-02T03:22:23.403Z:
Received chunk:  cook...

2025-04-02T03:22:23.428Z:
Received chunk:  until...

2025-04-02T03:22:23.452Z:
Received chunk:  they...

2025-04-02T03:22:23.476Z:
Received chunk:  start...

2025-04-02T03:22:23.500Z:
Received chunk:  to...

2025-04-02T03:22:23.525Z:
Received chunk:  soften...

2025-04-02T03:22:23.550Z:
Received chunk:  (...

2025-04-02T03:22:23.575Z:
Received chunk: about...

2025-04-02T03:22:23.599Z:
Received chunk:  ...

2025-04-02T03:22:23.624Z:
Received chunk: 3...

2025-04-02T03:22:23.648Z:
Received chunk: -...

2025-04-02T03:22:23.673Z:
Received chunk: 4...

2025-04-02T03:22:23.697Z:
Received chunk:  minutes...

2025-04-02T03:22:23.723Z:
Received chunk: ).
...

2025-04-02T03:22:23.748Z:
Received chunk: 5...

2025-04-02T03:22:23.774Z:
Received chunk: ....

2025-04-02T03:22:23.798Z:
Received chunk:  Add...

2025-04-02T03:22:23.822Z:
Received chunk:  the...

2025-04-02T03:22:23.846Z:
Received chunk:  minced...

2025-04-02T03:22:23.869Z:
Received chunk:  garlic...

2025-04-02T03:22:23.893Z:
Received chunk:  and...

2025-04-02T03:22:23.917Z:
Received chunk:  cook...

2025-04-02T03:22:23.941Z:
Received chunk:  for...

2025-04-02T03:22:23.965Z:
Received chunk:  an...

2025-04-02T03:22:23.988Z:
Received chunk:  additional...

2025-04-02T03:22:24.012Z:
Received chunk:  minute...

2025-04-02T03:22:24.036Z:
Received chunk: .
...

2025-04-02T03:22:24.059Z:
Received chunk: 6...

2025-04-02T03:22:24.082Z:
Received chunk: ....

2025-04-02T03:22:24.105Z:
Received chunk:  Return...

2025-04-02T03:22:24.129Z:
Received chunk:  the...

2025-04-02T03:22:24.153Z:
Received chunk:  cooked...

2025-04-02T03:22:24.176Z:
Received chunk:  chicken...

2025-04-02T03:22:24.199Z:
Received chunk:  to...

2025-04-02T03:22:24.223Z:
Received chunk:  the...

2025-04-02T03:22:24.247Z:
Received chunk:  skillet...

2025-04-02T03:22:24.271Z:
Received chunk:  and...

2025-04-02T03:22:24.295Z:
Received chunk:  stir...

2025-04-02T03:22:24.319Z:
Received chunk:  in...

2025-04-02T03:22:24.344Z:
Received chunk:  the...

2025-04-02T03:22:24.369Z:
Received chunk:  c...

2025-04-02T03:22:24.394Z:
Received chunk: umin...

2025-04-02T03:22:24.419Z:
Received chunk: .
...

2025-04-02T03:22:24.443Z:
Received chunk: 7...

2025-04-02T03:22:24.467Z:
Received chunk: ....

2025-04-02T03:22:24.492Z:
Received chunk:  Warm...

2025-04-02T03:22:24.517Z:
Received chunk:  the...

2025-04-02T03:22:24.541Z:
Received chunk:  tort...

2025-04-02T03:22:24.569Z:
Received chunk: illas...

2025-04-02T03:22:24.592Z:
Received chunk:  by...

2025-04-02T03:22:24.617Z:
Received chunk:  wrapping...

2025-04-02T03:22:24.642Z:
Received chunk:  them...

2025-04-02T03:22:24.666Z:
Received chunk:  in...

2025-04-02T03:22:24.691Z:
Received chunk:  a...

2025-04-02T03:22:24.716Z:
Received chunk:  damp...

2025-04-02T03:22:24.742Z:
Received chunk:  paper...

2025-04-02T03:22:24.767Z:
Received chunk:  towel...

2025-04-02T03:22:24.792Z:
Received chunk:  and...

2025-04-02T03:22:24.817Z:
Received chunk:  mic...

2025-04-02T03:22:24.841Z:
Received chunk: row...

2025-04-02T03:22:24.864Z:
Received chunk: aving...

2025-04-02T03:22:24.888Z:
Received chunk:  for...

2025-04-02T03:22:24.911Z:
Received chunk:  ...

2025-04-02T03:22:24.935Z:
Received chunk: 20...

2025-04-02T03:22:24.959Z:
Received chunk: -...

2025-04-02T03:22:24.982Z:
Received chunk: 30...

2025-04-02T03:22:25.006Z:
Received chunk:  seconds...

2025-04-02T03:22:25.029Z:
Received chunk: .
...

2025-04-02T03:22:25.053Z:
Received chunk: 8...

2025-04-02T03:22:25.077Z:
Received chunk: ....

2025-04-02T03:22:25.100Z:
Received chunk:  As...

2025-04-02T03:22:25.123Z:
Received chunk: semble...

2025-04-02T03:22:25.147Z:
Received chunk:  the...

2025-04-02T03:22:25.171Z:
Received chunk:  f...

2025-04-02T03:22:25.195Z:
Received chunk: aj...

2025-04-02T03:22:25.218Z:
Received chunk: itas...

2025-04-02T03:22:25.242Z:
Received chunk:  by...

2025-04-02T03:22:25.266Z:
Received chunk:  adding...

2025-04-02T03:22:25.291Z:
Received chunk:  the...

2025-04-02T03:22:25.315Z:
Received chunk:  chicken...

2025-04-02T03:22:25.340Z:
Received chunk:  and...

2025-04-02T03:22:25.364Z:
Received chunk:  vegetables...

2025-04-02T03:22:25.389Z:
Received chunk:  mixture...

2025-04-02T03:22:25.413Z:
Received chunk:  onto...

2025-04-02T03:22:25.439Z:
Received chunk:  the...

2025-04-02T03:22:25.464Z:
Received chunk:  tort...

2025-04-02T03:22:25.489Z:
Received chunk: illas...

2025-04-02T03:22:25.513Z:
Received chunk: ,...

2025-04-02T03:22:25.538Z:
Received chunk:  followed...

2025-04-02T03:22:25.571Z:
Received chunk:  by...

2025-04-02T03:22:25.588Z:
Received chunk:  any...

2025-04-02T03:22:25.613Z:
Received chunk:  desired...

2025-04-02T03:22:25.638Z:
Received chunk:  toppings...

2025-04-02T03:22:25.663Z:
Received chunk: .

...

2025-04-02T03:22:25.687Z:
Received chunk: **...

2025-04-02T03:22:25.714Z:
Received chunk: Pre...

2025-04-02T03:22:25.738Z:
Received chunk: p...

2025-04-02T03:22:25.763Z:
Received chunk:  Time...

2025-04-02T03:22:25.787Z:
Received chunk: :**...

2025-04-02T03:22:25.812Z:
Received chunk:  ...

2025-04-02T03:22:25.836Z:
Received chunk: 10...

2025-04-02T03:22:25.860Z:
Received chunk:  minutes...

2025-04-02T03:22:25.885Z:
Received chunk: 
...

2025-04-02T03:22:25.908Z:
Received chunk: **...

2025-04-02T03:22:25.932Z:
Received chunk: Cook...

2025-04-02T03:22:25.956Z:
Received chunk:  Time...

2025-04-02T03:22:25.980Z:
Received chunk: :**...

2025-04-02T03:22:26.004Z:
Received chunk:  ...

2025-04-02T03:22:26.027Z:
Received chunk: 15...

2025-04-02T03:22:26.051Z:
Received chunk: -...

2025-04-02T03:22:26.075Z:
Received chunk: 18...

2025-04-02T03:22:26.099Z:
Received chunk:  minutes...

2025-04-02T03:22:26.123Z:
Received chunk: 
...

2025-04-02T03:22:26.147Z:
Received chunk: **...

2025-04-02T03:22:26.171Z:
Received chunk: Total...

2025-04-02T03:22:26.195Z:
Received chunk:  Time...

2025-04-02T03:22:26.219Z:
Received chunk: :**...

2025-04-02T03:22:26.243Z:
Received chunk:  ...

2025-04-02T03:22:26.267Z:
Received chunk: 25...

2025-04-02T03:22:26.292Z:
Received chunk: -...

2025-04-02T03:22:26.316Z:
Received chunk: 28...

2025-04-02T03:22:26.340Z:
Received chunk:  minutes...

2025-04-02T03:22:26.365Z:
Received chunk: 

...

2025-04-02T03:22:26.389Z:
Received chunk: Enjoy...

2025-04-02T03:22:26.413Z:
Received chunk:  your...

2025-04-02T03:22:26.439Z:
Received chunk:  delicious...

2025-04-02T03:22:26.462Z:
Received chunk:  and...

2025-04-02T03:22:26.486Z:
Received chunk:  quick...

2025-04-02T03:22:26.511Z:
Received chunk:  Chicken...

2025-04-02T03:22:26.535Z:
Received chunk:  F...

2025-04-02T03:22:26.560Z:
Received chunk: aj...

2025-04-02T03:22:26.585Z:
Received chunk: itas...

2025-04-02T03:22:26.610Z:
Received chunk: !...

2025-04-02T03:22:26.635Z:
Received chunk: 

> ...

2025-04-02T03:29:13.213Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:29:13.213Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:29:13.214Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:29:13.214Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:29:13.667Z:
Unloading model...

2025-04-02T03:29:13.668Z:
Model unloaded successfully

2025-04-02T03:29:13.668Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:29:13.668Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:29:13.669Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:29:13.669Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:29:19.704Z:
Generate response called with message: MAKE A COOKING RECIPE...

2025-04-02T03:29:19.704Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: MAKE A COOKING RECIPE
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:29:19.733Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:29:19.761Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:29:19.780Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:29:19.787Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:29:19.828Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:29:19.927Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:29:19.942Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0


2025-04-02T03:29:19.942Z:
Process stderr: print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:29:20.014Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.....

2025-04-02T03:29:20.014Z:
Process stderr: ......................................................

2025-04-02T03:29:20.014Z:
Process stderr: ...


2025-04-02T03:29:20.017Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:29:20.017Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:29:20.027Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:29:20.032Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:29:20.186Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T03:29:20.187Z:
Process stderr: main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:29:20.187Z:
Process stderr: sampler seed: 1698219189
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:29:20.271Z:
Received chunk: I...

2025-04-02T03:29:20.295Z:
Received chunk: 'd...

2025-04-02T03:29:20.319Z:
Received chunk:  be...

2025-04-02T03:29:20.344Z:
Received chunk:  happy...

2025-04-02T03:29:20.369Z:
Received chunk:  to...

2025-04-02T03:29:20.394Z:
Received chunk:  help...

2025-04-02T03:29:20.422Z:
Received chunk:  you...

2025-04-02T03:29:20.450Z:
Received chunk:  with...

2025-04-02T03:29:20.477Z:
Received chunk:  a...

2025-04-02T03:29:20.503Z:
Received chunk:  recipe...

2025-04-02T03:29:20.530Z:
Received chunk: ....

2025-04-02T03:29:20.558Z:
Received chunk:  What...

2025-04-02T03:29:20.584Z:
Received chunk:  type...

2025-04-02T03:29:20.610Z:
Received chunk:  of...

2025-04-02T03:29:20.636Z:
Received chunk:  cuisine...

2025-04-02T03:29:20.661Z:
Received chunk:  are...

2025-04-02T03:29:20.685Z:
Received chunk:  you...

2025-04-02T03:29:20.709Z:
Received chunk:  in...

2025-04-02T03:29:20.732Z:
Received chunk:  the...

2025-04-02T03:29:20.756Z:
Received chunk:  mood...

2025-04-02T03:29:20.780Z:
Received chunk:  for...

2025-04-02T03:29:20.803Z:
Received chunk: ?...

2025-04-02T03:29:20.827Z:
Received chunk:  Do...

2025-04-02T03:29:20.851Z:
Received chunk:  you...

2025-04-02T03:29:20.874Z:
Received chunk:  have...

2025-04-02T03:29:20.898Z:
Received chunk:  any...

2025-04-02T03:29:20.922Z:
Received chunk:  dietary...

2025-04-02T03:29:20.946Z:
Received chunk:  restrictions...

2025-04-02T03:29:20.969Z:
Received chunk:  or...

2025-04-02T03:29:20.993Z:
Received chunk:  preferences...

2025-04-02T03:29:21.016Z:
Received chunk:  (...

2025-04-02T03:29:21.039Z:
Received chunk: e...

2025-04-02T03:29:21.063Z:
Received chunk: .g...

2025-04-02T03:29:21.088Z:
Received chunk: ....

2025-04-02T03:29:21.113Z:
Received chunk:  vegetarian...

2025-04-02T03:29:21.137Z:
Received chunk: ,...

2025-04-02T03:29:21.162Z:
Received chunk:  gluten...

2025-04-02T03:29:21.187Z:
Received chunk: -free...

2025-04-02T03:29:21.211Z:
Received chunk: ,...

2025-04-02T03:29:21.235Z:
Received chunk:  etc...

2025-04-02T03:29:21.259Z:
Received chunk: .)...

2025-04-02T03:29:21.283Z:
Received chunk: ?

...

2025-04-02T03:29:21.307Z:
Received chunk: Also...

2025-04-02T03:29:21.332Z:
Received chunk: ,...

2025-04-02T03:29:21.357Z:
Received chunk:  what...

2025-04-02T03:29:21.390Z:
Received chunk:  kind...

2025-04-02T03:29:21.405Z:
Received chunk:  of...

2025-04-02T03:29:21.430Z:
Received chunk:  dish...

2025-04-02T03:29:21.455Z:
Received chunk:  are...

2025-04-02T03:29:21.479Z:
Received chunk:  you...

2025-04-02T03:29:21.504Z:
Received chunk:  looking...

2025-04-02T03:29:21.533Z:
Received chunk:  to...

2025-04-02T03:29:21.555Z:
Received chunk:  make...

2025-04-02T03:29:21.581Z:
Received chunk: ?...

2025-04-02T03:29:21.607Z:
Received chunk:  Are...

2025-04-02T03:29:21.631Z:
Received chunk:  you...

2025-04-02T03:29:21.655Z:
Received chunk:  thinking...

2025-04-02T03:29:21.679Z:
Received chunk:  something...

2025-04-02T03:29:21.702Z:
Received chunk:  simple...

2025-04-02T03:29:21.726Z:
Received chunk:  and...

2025-04-02T03:29:21.749Z:
Received chunk:  quick...

2025-04-02T03:29:21.773Z:
Received chunk: ,...

2025-04-02T03:29:21.796Z:
Received chunk:  like...

2025-04-02T03:29:21.820Z:
Received chunk:  a...

2025-04-02T03:29:21.844Z:
Received chunk:  week...

2025-04-02T03:29:21.868Z:
Received chunk: night...

2025-04-02T03:29:21.892Z:
Received chunk:  dinner...

2025-04-02T03:29:21.915Z:
Received chunk: ,...

2025-04-02T03:29:21.938Z:
Received chunk:  or...

2025-04-02T03:29:21.963Z:
Received chunk:  more...

2025-04-02T03:29:21.986Z:
Received chunk:  elaborate...

2025-04-02T03:29:22.010Z:
Received chunk:  and...

2025-04-02T03:29:22.033Z:
Received chunk:  impressive...

2025-04-02T03:29:22.057Z:
Received chunk: ,...

2025-04-02T03:29:22.081Z:
Received chunk:  like...

2025-04-02T03:29:22.105Z:
Received chunk:  a...

2025-04-02T03:29:22.130Z:
Received chunk:  special...

2025-04-02T03:29:22.155Z:
Received chunk:  occasion...

2025-04-02T03:29:22.179Z:
Received chunk:  meal...

2025-04-02T03:29:22.203Z:
Received chunk: ?...

2025-04-02T03:29:22.227Z:
Received chunk: 

> ...

2025-04-02T03:29:29.737Z:
Generate response called with message: 5 TYPES OF COOKIES...

2025-04-02T03:29:29.738Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: 5 TYPES OF COOKIES
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:29:29.760Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:29:29.785Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:29:29.804Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:29:29.812Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:29:29.852Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:29:29.937Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:29:29.952Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05


2025-04-02T03:29:29.952Z:
Process stderr: print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:29:30.020Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
........

2025-04-02T03:29:30.021Z:
Process stderr: ......................................................


2025-04-02T03:29:30.023Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:29:30.023Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:29:30.034Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:29:30.038Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:29:30.186Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T03:29:30.187Z:
Process stderr: main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:29:30.187Z:
Process stderr: sampler seed: 394604102
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:29:30.268Z:
Received chunk: Here...

2025-04-02T03:29:30.292Z:
Received chunk:  are...

2025-04-02T03:29:30.317Z:
Received chunk:  ...

2025-04-02T03:29:30.342Z:
Received chunk: 5...

2025-04-02T03:29:30.366Z:
Received chunk:  types...

2025-04-02T03:29:30.390Z:
Received chunk:  of...

2025-04-02T03:29:30.414Z:
Received chunk:  cookies...

2025-04-02T03:29:30.438Z:
Received chunk: :

...

2025-04-02T03:29:30.471Z:
Received chunk: 1...

2025-04-02T03:29:30.487Z:
Received chunk: ....

2025-04-02T03:29:30.512Z:
Received chunk:  Chocolate...

2025-04-02T03:29:30.536Z:
Received chunk:  Chip...

2025-04-02T03:29:30.560Z:
Received chunk:  Cookies...

2025-04-02T03:29:30.585Z:
Received chunk: 
...

2025-04-02T03:29:30.614Z:
Received chunk: 2...

2025-04-02T03:29:30.636Z:
Received chunk: ....

2025-04-02T03:29:30.661Z:
Received chunk:  O...

2025-04-02T03:29:30.686Z:
Received chunk: at...

2025-04-02T03:29:30.711Z:
Received chunk: meal...

2025-04-02T03:29:30.736Z:
Received chunk:  R...

2025-04-02T03:29:30.760Z:
Received chunk: ais...

2025-04-02T03:29:30.783Z:
Received chunk: in...

2025-04-02T03:29:30.807Z:
Received chunk:  Cookies...

2025-04-02T03:29:30.831Z:
Received chunk: 
...

2025-04-02T03:29:30.855Z:
Received chunk: 3...

2025-04-02T03:29:30.879Z:
Received chunk: ....

2025-04-02T03:29:30.903Z:
Received chunk:  Peanut...

2025-04-02T03:29:30.926Z:
Received chunk:  Butter...

2025-04-02T03:29:30.949Z:
Received chunk:  Cookies...

2025-04-02T03:29:30.973Z:
Received chunk: 
...

2025-04-02T03:29:30.997Z:
Received chunk: 4...

2025-04-02T03:29:31.021Z:
Received chunk: ....

2025-04-02T03:29:31.044Z:
Received chunk:  Sn...

2025-04-02T03:29:31.069Z:
Received chunk: icker...

2025-04-02T03:29:31.094Z:
Received chunk: d...

2025-04-02T03:29:31.118Z:
Received chunk: oodles...

2025-04-02T03:29:31.142Z:
Received chunk: 
...

2025-04-02T03:29:31.166Z:
Received chunk: 5...

2025-04-02T03:29:31.190Z:
Received chunk: ....

2025-04-02T03:29:31.214Z:
Received chunk:  Sugar...

2025-04-02T03:29:31.239Z:
Received chunk:  Cookies...

2025-04-02T03:29:31.263Z:
Received chunk: 

> ...

2025-04-02T03:29:53.557Z:
Generate response called with message: EXPLAIN HOW TO COOK EACH ONE WITH A TAILORED RECIPE...

2025-04-02T03:29:53.557Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: EXPLAIN HOW TO COOK EACH ONE WITH A TAILORED RECIPE
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:29:53.579Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:29:53.605Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:29:53.624Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:29:53.631Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:29:53.672Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:29:53.764Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:29:53.779Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:29:53.849Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
..

2025-04-02T03:29:53.849Z:
Process stderr: .....................................................

2025-04-02T03:29:53.849Z:
Process stderr: .......


2025-04-02T03:29:53.851Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:29:53.851Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:29:53.863Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:29:53.868Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:29:54.011Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>





2025-04-02T03:29:54.012Z:
Process stderr: system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:29:54.012Z:
Process stderr: sampler seed: 2066895221
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:29:54.121Z:
Received chunk: I...

2025-04-02T03:29:54.144Z:
Received chunk: 'd...

2025-04-02T03:29:54.168Z:
Received chunk:  be...

2025-04-02T03:29:54.191Z:
Received chunk:  happy...

2025-04-02T03:29:54.215Z:
Received chunk:  to...

2025-04-02T03:29:54.238Z:
Received chunk:  help...

2025-04-02T03:29:54.262Z:
Received chunk:  you...

2025-04-02T03:29:54.286Z:
Received chunk:  with...

2025-04-02T03:29:54.309Z:
Received chunk:  cooking...

2025-04-02T03:29:54.334Z:
Received chunk:  each...

2025-04-02T03:29:54.358Z:
Received chunk:  one...

2025-04-02T03:29:54.382Z:
Received chunk:  in...

2025-04-02T03:29:54.406Z:
Received chunk:  a...

2025-04-02T03:29:54.431Z:
Received chunk:  tailored...

2025-04-02T03:29:54.455Z:
Received chunk:  recipe...

2025-04-02T03:29:54.480Z:
Received chunk: ....

2025-04-02T03:29:54.504Z:
Received chunk:  Please...

2025-04-02T03:29:54.529Z:
Received chunk:  go...

2025-04-02T03:29:54.553Z:
Received chunk:  ahead...

2025-04-02T03:29:54.577Z:
Received chunk:  and...

2025-04-02T03:29:54.601Z:
Received chunk:  let...

2025-04-02T03:29:54.625Z:
Received chunk:  me...

2025-04-02T03:29:54.649Z:
Received chunk:  know...

2025-04-02T03:29:54.673Z:
Received chunk:  which...

2025-04-02T03:29:54.706Z:
Received chunk:  dish...

2025-04-02T03:29:54.722Z:
Received chunk:  you...

2025-04-02T03:29:54.748Z:
Received chunk: 're...

2025-04-02T03:29:54.773Z:
Received chunk:  interested...

2025-04-02T03:29:54.798Z:
Received chunk:  in...

2025-04-02T03:29:54.823Z:
Received chunk: ,...

2025-04-02T03:29:54.851Z:
Received chunk:  and...

2025-04-02T03:29:54.876Z:
Received chunk:  I...

2025-04-02T03:29:54.901Z:
Received chunk: 'll...

2025-04-02T03:29:54.927Z:
Received chunk:  provide...

2025-04-02T03:29:54.951Z:
Received chunk:  you...

2025-04-02T03:29:54.977Z:
Received chunk:  with...

2025-04-02T03:29:55.000Z:
Received chunk:  a...

2025-04-02T03:29:55.024Z:
Received chunk:  step...

2025-04-02T03:29:55.049Z:
Received chunk: -by...

2025-04-02T03:29:55.074Z:
Received chunk: -step...

2025-04-02T03:29:55.098Z:
Received chunk:  guide...

2025-04-02T03:29:55.123Z:
Received chunk:  on...

2025-04-02T03:29:55.147Z:
Received chunk:  how...

2025-04-02T03:29:55.171Z:
Received chunk:  to...

2025-04-02T03:29:55.196Z:
Received chunk:  cook...

2025-04-02T03:29:55.219Z:
Received chunk:  it...

2025-04-02T03:29:55.243Z:
Received chunk: .

...

2025-04-02T03:29:55.267Z:
Received chunk: What...

2025-04-02T03:29:55.290Z:
Received chunk: 's...

2025-04-02T03:29:55.314Z:
Received chunk:  the...

2025-04-02T03:29:55.338Z:
Received chunk:  first...

2025-04-02T03:29:55.362Z:
Received chunk:  dish...

2025-04-02T03:29:55.386Z:
Received chunk:  you...

2025-04-02T03:29:55.410Z:
Received chunk: 'd...

2025-04-02T03:29:55.433Z:
Received chunk:  like...

2025-04-02T03:29:55.458Z:
Received chunk:  to...

2025-04-02T03:29:55.482Z:
Received chunk:  learn...

2025-04-02T03:29:55.505Z:
Received chunk:  about...

2025-04-02T03:29:55.529Z:
Received chunk: ?...

2025-04-02T03:29:55.553Z:
Received chunk: 

> ...

2025-04-02T03:30:00.388Z:
Generate response called with message: ALL OF THEM...

2025-04-02T03:30:00.388Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: ALL OF THEM
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:30:00.409Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:30:00.435Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:30:00.454Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:30:00.462Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:30:00.502Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:30:00.586Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:30:00.601Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0


2025-04-02T03:30:00.602Z:
Process stderr: print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:30:00.671Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.....

2025-04-02T03:30:00.671Z:
Process stderr: ....................................................

2025-04-02T03:30:00.671Z:
Process stderr: .....


2025-04-02T03:30:00.673Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:30:00.673Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:30:00.684Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:30:00.689Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:30:00.831Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>





2025-04-02T03:30:00.832Z:
Process stderr: system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:30:00.832Z:
Process stderr: sampler seed: 1988188736
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:30:00.903Z:
Received chunk: I...

2025-04-02T03:30:00.927Z:
Received chunk:  cannot...

2025-04-02T03:30:00.951Z:
Received chunk:  continue...

2025-04-02T03:30:00.976Z:
Received chunk:  a...

2025-04-02T03:30:00.999Z:
Received chunk:  conversation...

2025-04-02T03:30:01.022Z:
Received chunk:  that...

2025-04-02T03:30:01.046Z:
Received chunk:  discusses...

2025-04-02T03:30:01.069Z:
Received chunk:  or...

2025-04-02T03:30:01.093Z:
Received chunk:  promotes...

2025-04-02T03:30:01.117Z:
Received chunk:  harmful...

2025-04-02T03:30:01.141Z:
Received chunk:  or...

2025-04-02T03:30:01.164Z:
Received chunk:  violent...

2025-04-02T03:30:01.187Z:
Received chunk:  activities...

2025-04-02T03:30:01.211Z:
Received chunk:  towards...

2025-04-02T03:30:01.235Z:
Received chunk:  any...

2025-04-02T03:30:01.259Z:
Received chunk:  individual...

2025-04-02T03:30:01.283Z:
Received chunk:  or...

2025-04-02T03:30:01.306Z:
Received chunk:  group...

2025-04-02T03:30:01.330Z:
Received chunk: ....

2025-04-02T03:30:01.354Z:
Received chunk:  Is...

2025-04-02T03:30:01.377Z:
Received chunk:  there...

2025-04-02T03:30:01.401Z:
Received chunk:  anything...

2025-04-02T03:30:01.425Z:
Received chunk:  else...

2025-04-02T03:30:01.449Z:
Received chunk:  I...

2025-04-02T03:30:01.473Z:
Received chunk:  can...

2025-04-02T03:30:01.497Z:
Received chunk:  help...

2025-04-02T03:30:01.521Z:
Received chunk:  you...

2025-04-02T03:30:01.545Z:
Received chunk:  with...

2025-04-02T03:30:01.569Z:
Received chunk: ?...

2025-04-02T03:30:01.593Z:
Received chunk: 

> ...

2025-04-02T03:30:07.956Z:
Generating HTML from 8 messages

2025-04-02T03:30:07.956Z:
Spawning process for HTML generation: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:30:07.979Z:
HTML Generation stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:30:08.013Z:
HTML Generation stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:30:08.031Z:
HTML Generation stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:30:08.039Z:
HTML Generation stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:30:08.079Z:
HTML Generation stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:30:08.161Z:
HTML Generation stderr: load: special tokens cache size = 256


2025-04-02T03:30:08.175Z:
HTML Generation stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00


2025-04-02T03:30:08.176Z:
HTML Generation stderr: print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:30:08.245Z:
HTML Generation stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
...

2025-04-02T03:30:08.245Z:
HTML Generation stderr: ........................................................

2025-04-02T03:30:08.245Z:
HTML Generation stderr: ...


2025-04-02T03:30:08.247Z:
HTML Generation stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:30:08.247Z:
HTML Generation stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:30:08.270Z:
HTML Generation stderr: init:        CPU KV buffer size =   128.00 MiB
llama_context: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB


2025-04-02T03:30:08.276Z:
HTML Generation stderr: llama_context:        CPU compute buffer size =   280.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:30:08.423Z:
HTML Generation stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-02T03:30:08.424Z:
HTML Generation stderr: main: interactive mode on.


2025-04-02T03:30:08.424Z:
HTML Generation stderr: sampler seed: 4079700986
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 4096, n_batch = 2048, n_predict = 4096, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:37:10.896Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:37:10.897Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:37:10.898Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:37:10.898Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:37:11.515Z:
Unloading model...

2025-04-02T03:37:11.516Z:
Model unloaded successfully

2025-04-02T03:37:11.516Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:37:11.516Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:37:11.516Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:37:11.516Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:37:17.571Z:
Generate response called with message: quick test...

2025-04-02T03:37:17.571Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: quick test
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:37:17.774Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:37:17.807Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:37:17.825Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:37:17.837Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:37:17.874Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:37:17.966Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:37:17.981Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512


2025-04-02T03:37:17.982Z:
Process stderr: print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:37:18.817Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.........

2025-04-02T03:37:18.818Z:
Process stderr: .....................................................


2025-04-02T03:37:18.820Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:37:18.820Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:37:18.831Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:37:18.840Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:37:18.982Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-02T03:37:18.983Z:
Process stderr: main: interactive mode on.


2025-04-02T03:37:18.983Z:
Process stderr: sampler seed: 101061057
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:37:19.050Z:
Received chunk: What...

2025-04-02T03:37:19.074Z:
Received chunk:  kind...

2025-04-02T03:37:19.098Z:
Received chunk:  of...

2025-04-02T03:37:19.122Z:
Received chunk:  test...

2025-04-02T03:37:19.146Z:
Received chunk: ?...

2025-04-02T03:37:19.169Z:
Received chunk: 

> ...

2025-04-02T03:37:31.860Z:
Generate response called with message: tell me a detailing cooking recipe for sweets like baked goods....

2025-04-02T03:37:31.861Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: tell me a detailing cooking recipe for sweets like baked goods.
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:37:31.883Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:37:31.911Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:37:31.929Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:37:31.937Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:37:31.982Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:37:32.071Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:37:32.085Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown


2025-04-02T03:37:32.086Z:
Process stderr: print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:37:32.132Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.....

2025-04-02T03:37:32.132Z:
Process stderr: ..................................................

2025-04-02T03:37:32.133Z:
Process stderr: .......


2025-04-02T03:37:32.134Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:37:32.134Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:37:32.145Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:37:32.151Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:37:32.291Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 



2025-04-02T03:37:32.291Z:
Process stderr: main: interactive mode on.


2025-04-02T03:37:32.292Z:
Process stderr: sampler seed: 3444864782
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:37:32.382Z:
Received chunk: Here...

2025-04-02T03:37:32.405Z:
Received chunk: 's...

2025-04-02T03:37:32.429Z:
Received chunk:  a...

2025-04-02T03:37:32.452Z:
Received chunk:  detailed...

2025-04-02T03:37:32.476Z:
Received chunk:  recipe...

2025-04-02T03:37:32.501Z:
Received chunk:  for...

2025-04-02T03:37:32.524Z:
Received chunk:  some...

2025-04-02T03:37:32.547Z:
Received chunk:  delicious...

2025-04-02T03:37:32.571Z:
Received chunk:  sweets...

2025-04-02T03:37:32.595Z:
Received chunk: ,...

2025-04-02T03:37:32.619Z:
Received chunk:  including...

2025-04-02T03:37:32.643Z:
Received chunk:  baked...

2025-04-02T03:37:32.668Z:
Received chunk:  goods...

2025-04-02T03:37:32.692Z:
Received chunk: :

...

2025-04-02T03:37:32.716Z:
Received chunk: **...

2025-04-02T03:37:32.741Z:
Received chunk: Recipe...

2025-04-02T03:37:32.765Z:
Received chunk:  ...

2025-04-02T03:37:32.789Z:
Received chunk: 1...

2025-04-02T03:37:32.814Z:
Received chunk: :...

2025-04-02T03:37:32.839Z:
Received chunk:  Chocolate...

2025-04-02T03:37:32.863Z:
Received chunk:  Chip...

2025-04-02T03:37:32.887Z:
Received chunk:  Cookies...

2025-04-02T03:37:32.911Z:
Received chunk: **

...

2025-04-02T03:37:32.935Z:
Received chunk: Ingredients...

2025-04-02T03:37:32.959Z:
Received chunk: :

...

2025-04-02T03:37:32.992Z:
Received chunk: *...

2025-04-02T03:37:33.010Z:
Received chunk:  ...

2025-04-02T03:37:33.036Z:
Received chunk: 2...

2025-04-02T03:37:33.061Z:
Received chunk:  ...

2025-04-02T03:37:33.084Z:
Received chunk: 1...

2025-04-02T03:37:33.109Z:
Received chunk: /...

2025-04-02T03:37:33.137Z:
Received chunk: 4...

2025-04-02T03:37:33.158Z:
Received chunk:  cups...

2025-04-02T03:37:33.183Z:
Received chunk:  all...

2025-04-02T03:37:33.207Z:
Received chunk: -purpose...

2025-04-02T03:37:33.231Z:
Received chunk:  flour...

2025-04-02T03:37:33.256Z:
Received chunk: 
...

2025-04-02T03:37:33.280Z:
Received chunk: *...

2025-04-02T03:37:33.304Z:
Received chunk:  ...

2025-04-02T03:37:33.328Z:
Received chunk: 1...

2025-04-02T03:37:33.351Z:
Received chunk:  tsp...

2025-04-02T03:37:33.375Z:
Received chunk:  baking...

2025-04-02T03:37:33.399Z:
Received chunk:  soda...

2025-04-02T03:37:33.422Z:
Received chunk: 
...

2025-04-02T03:37:33.447Z:
Received chunk: *...

2025-04-02T03:37:33.471Z:
Received chunk:  ...

2025-04-02T03:37:33.494Z:
Received chunk: 1...

2025-04-02T03:37:33.518Z:
Received chunk:  tsp...

2025-04-02T03:37:33.541Z:
Received chunk:  salt...

2025-04-02T03:37:33.564Z:
Received chunk: 
...

2025-04-02T03:37:33.587Z:
Received chunk: *...

2025-04-02T03:37:33.610Z:
Received chunk:  ...

2025-04-02T03:37:33.634Z:
Received chunk: 1...

2025-04-02T03:37:33.657Z:
Received chunk:  cup...

2025-04-02T03:37:33.681Z:
Received chunk:  uns...

2025-04-02T03:37:33.705Z:
Received chunk: alted...

2025-04-02T03:37:33.729Z:
Received chunk:  butter...

2025-04-02T03:37:33.752Z:
Received chunk:  (...

2025-04-02T03:37:33.777Z:
Received chunk: at...

2025-04-02T03:37:33.801Z:
Received chunk:  room...

2025-04-02T03:37:33.825Z:
Received chunk:  temperature...

2025-04-02T03:37:33.850Z:
Received chunk: )
...

2025-04-02T03:37:33.874Z:
Received chunk: *...

2025-04-02T03:37:33.898Z:
Received chunk:  ...

2025-04-02T03:37:33.923Z:
Received chunk: 3...

2025-04-02T03:37:33.947Z:
Received chunk: /...

2025-04-02T03:37:33.971Z:
Received chunk: 4...

2025-04-02T03:37:33.995Z:
Received chunk:  cup...

2025-04-02T03:37:34.021Z:
Received chunk:  white...

2025-04-02T03:37:34.046Z:
Received chunk:  gran...

2025-04-02T03:37:34.071Z:
Received chunk: ulated...

2025-04-02T03:37:34.095Z:
Received chunk:  sugar...

2025-04-02T03:37:34.119Z:
Received chunk: 
...

2025-04-02T03:37:34.144Z:
Received chunk: *...

2025-04-02T03:37:34.169Z:
Received chunk:  ...

2025-04-02T03:37:34.193Z:
Received chunk: 3...

2025-04-02T03:37:34.218Z:
Received chunk: /...

2025-04-02T03:37:34.242Z:
Received chunk: 4...

2025-04-02T03:37:34.266Z:
Received chunk:  cup...

2025-04-02T03:37:34.290Z:
Received chunk:  brown...

2025-04-02T03:37:34.313Z:
Received chunk:  sugar...

2025-04-02T03:37:34.337Z:
Received chunk: 
...

2025-04-02T03:37:34.360Z:
Received chunk: *...

2025-04-02T03:37:34.384Z:
Received chunk:  ...

2025-04-02T03:37:34.407Z:
Received chunk: 2...

2025-04-02T03:37:34.432Z:
Received chunk:  large...

2025-04-02T03:37:34.455Z:
Received chunk:  eggs...

2025-04-02T03:37:34.479Z:
Received chunk: 
...

2025-04-02T03:37:34.502Z:
Received chunk: *...

2025-04-02T03:37:34.526Z:
Received chunk:  ...

2025-04-02T03:37:34.549Z:
Received chunk: 2...

2025-04-02T03:37:34.573Z:
Received chunk:  teaspoons...

2025-04-02T03:37:34.596Z:
Received chunk:  vanilla...

2025-04-02T03:37:34.620Z:
Received chunk:  extract...

2025-04-02T03:37:34.643Z:
Received chunk: 
...

2025-04-02T03:37:34.667Z:
Received chunk: *...

2025-04-02T03:37:34.692Z:
Received chunk:  ...

2025-04-02T03:37:34.716Z:
Received chunk: 2...

2025-04-02T03:37:34.740Z:
Received chunk:  cups...

2025-04-02T03:37:34.764Z:
Received chunk:  semi...

2025-04-02T03:37:34.788Z:
Received chunk: -s...

2025-04-02T03:37:34.812Z:
Received chunk: weet...

2025-04-02T03:37:34.836Z:
Received chunk:  chocolate...

2025-04-02T03:37:34.861Z:
Received chunk:  chips...

2025-04-02T03:37:34.885Z:
Received chunk: 

...

2025-04-02T03:37:34.909Z:
Received chunk: Instructions...

2025-04-02T03:37:34.933Z:
Received chunk: :

...

2025-04-02T03:37:34.957Z:
Received chunk: 1...

2025-04-02T03:37:34.982Z:
Received chunk: ....

2025-04-02T03:37:35.007Z:
Received chunk:  Pre...

2025-04-02T03:37:35.032Z:
Received chunk: heat...

2025-04-02T03:37:35.057Z:
Received chunk:  your...

2025-04-02T03:37:35.081Z:
Received chunk:  oven...

2025-04-02T03:37:35.107Z:
Received chunk:  to...

2025-04-02T03:37:35.131Z:
Received chunk:  ...

2025-04-02T03:37:35.156Z:
Received chunk: 375...

2025-04-02T03:37:35.182Z:
Received chunk: °F...

2025-04-02T03:37:35.207Z:
Received chunk:  (...

2025-04-02T03:37:35.232Z:
Received chunk: 190...

2025-04-02T03:37:35.256Z:
Received chunk: °C...

2025-04-02T03:37:35.280Z:
Received chunk: )....

2025-04-02T03:37:35.303Z:
Received chunk:  Line...

2025-04-02T03:37:35.327Z:
Received chunk:  a...

2025-04-02T03:37:35.351Z:
Received chunk:  baking...

2025-04-02T03:37:35.375Z:
Received chunk:  sheet...

2025-04-02T03:37:35.399Z:
Received chunk:  with...

2025-04-02T03:37:35.422Z:
Received chunk:  parchment...

2025-04-02T03:37:35.447Z:
Received chunk:  paper...

2025-04-02T03:37:35.470Z:
Received chunk:  or...

2025-04-02T03:37:35.494Z:
Received chunk:  a...

2025-04-02T03:37:35.517Z:
Received chunk:  silicone...

2025-04-02T03:37:35.540Z:
Received chunk:  mat...

2025-04-02T03:37:35.564Z:
Received chunk: .
...

2025-04-02T03:37:35.587Z:
Received chunk: 2...

2025-04-02T03:37:35.611Z:
Received chunk: ....

2025-04-02T03:37:35.634Z:
Received chunk:  In...

2025-04-02T03:37:35.658Z:
Received chunk:  a...

2025-04-02T03:37:35.682Z:
Received chunk:  medium...

2025-04-02T03:37:35.706Z:
Received chunk:  bowl...

2025-04-02T03:37:35.730Z:
Received chunk: ,...

2025-04-02T03:37:35.754Z:
Received chunk:  whisk...

2025-04-02T03:37:35.778Z:
Received chunk:  together...

2025-04-02T03:37:35.802Z:
Received chunk:  the...

2025-04-02T03:37:35.826Z:
Received chunk:  flour...

2025-04-02T03:37:35.851Z:
Received chunk: ,...

2025-04-02T03:37:35.874Z:
Received chunk:  baking...

2025-04-02T03:37:35.898Z:
Received chunk:  soda...

2025-04-02T03:37:35.923Z:
Received chunk: ,...

2025-04-02T03:37:35.947Z:
Received chunk:  and...

2025-04-02T03:37:35.972Z:
Received chunk:  salt...

2025-04-02T03:37:36.005Z:
Received chunk: ....

2025-04-02T03:37:36.022Z:
Received chunk:  Set...

2025-04-02T03:37:36.047Z:
Received chunk:  aside...

2025-04-02T03:37:36.071Z:
Received chunk: .
...

2025-04-02T03:37:36.096Z:
Received chunk: 3...

2025-04-02T03:37:36.120Z:
Received chunk: ....

2025-04-02T03:37:36.144Z:
Received chunk:  In...

2025-04-02T03:37:36.169Z:
Received chunk:  a...

2025-04-02T03:37:36.194Z:
Received chunk:  large...

2025-04-02T03:37:36.218Z:
Received chunk:  bowl...

2025-04-02T03:37:36.243Z:
Received chunk: ,...

2025-04-02T03:37:36.268Z:
Received chunk:  use...

2025-04-02T03:37:36.292Z:
Received chunk:  an...

2025-04-02T03:37:36.316Z:
Received chunk:  electric...

2025-04-02T03:37:36.340Z:
Received chunk:  mixer...

2025-04-02T03:37:36.363Z:
Received chunk:  to...

2025-04-02T03:37:36.387Z:
Received chunk:  cream...

2025-04-02T03:37:36.411Z:
Received chunk:  together...

2025-04-02T03:37:36.435Z:
Received chunk:  the...

2025-04-02T03:37:36.459Z:
Received chunk:  butter...

2025-04-02T03:37:36.483Z:
Received chunk:  and...

2025-04-02T03:37:36.506Z:
Received chunk:  sugars...

2025-04-02T03:37:36.530Z:
Received chunk:  until...

2025-04-02T03:37:36.554Z:
Received chunk:  light...

2025-04-02T03:37:36.577Z:
Received chunk:  and...

2025-04-02T03:37:36.601Z:
Received chunk:  fluffy...

2025-04-02T03:37:36.625Z:
Received chunk: ,...

2025-04-02T03:37:36.649Z:
Received chunk:  about...

2025-04-02T03:37:36.672Z:
Received chunk:  ...

2025-04-02T03:37:36.696Z:
Received chunk: 2...

2025-04-02T03:37:36.720Z:
Received chunk: -...

2025-04-02T03:37:36.744Z:
Received chunk: 3...

2025-04-02T03:37:36.768Z:
Received chunk:  minutes...

2025-04-02T03:37:36.792Z:
Received chunk: .
...

2025-04-02T03:37:36.816Z:
Received chunk: 4...

2025-04-02T03:37:36.841Z:
Received chunk: ....

2025-04-02T03:37:36.865Z:
Received chunk:  Beat...

2025-04-02T03:37:36.889Z:
Received chunk:  in...

2025-04-02T03:37:36.913Z:
Received chunk:  the...

2025-04-02T03:37:36.937Z:
Received chunk:  eggs...

2025-04-02T03:37:36.962Z:
Received chunk:  one...

2025-04-02T03:37:36.986Z:
Received chunk:  at...

2025-04-02T03:37:37.012Z:
Received chunk:  a...

2025-04-02T03:37:37.037Z:
Received chunk:  time...

2025-04-02T03:37:37.062Z:
Received chunk: ,...

2025-04-02T03:37:37.086Z:
Received chunk:  followed...

2025-04-02T03:37:37.111Z:
Received chunk:  by...

2025-04-02T03:37:37.135Z:
Received chunk:  the...

2025-04-02T03:37:37.164Z:
Received chunk:  vanilla...

2025-04-02T03:37:37.184Z:
Received chunk:  extract...

2025-04-02T03:37:37.209Z:
Received chunk: .
...

2025-04-02T03:37:37.234Z:
Received chunk: 5...

2025-04-02T03:37:37.258Z:
Received chunk: ....

2025-04-02T03:37:37.283Z:
Received chunk:  Grad...

2025-04-02T03:37:37.306Z:
Received chunk: ually...

2025-04-02T03:37:37.329Z:
Received chunk:  mix...

2025-04-02T03:37:37.354Z:
Received chunk:  in...

2025-04-02T03:37:37.378Z:
Received chunk:  the...

2025-04-02T03:37:37.402Z:
Received chunk:  dry...

2025-04-02T03:37:37.426Z:
Received chunk:  ingredients...

2025-04-02T03:37:37.450Z:
Received chunk:  (...

2025-04-02T03:37:37.474Z:
Received chunk: fl...

2025-04-02T03:37:37.498Z:
Received chunk: our...

2025-04-02T03:37:37.522Z:
Received chunk:  mixture...

2025-04-02T03:37:37.546Z:
Received chunk: )...

2025-04-02T03:37:37.570Z:
Received chunk:  until...

2025-04-02T03:37:37.595Z:
Received chunk:  just...

2025-04-02T03:37:37.619Z:
Received chunk:  combined...

2025-04-02T03:37:37.642Z:
Received chunk: ,...

2025-04-02T03:37:37.665Z:
Received chunk:  being...

2025-04-02T03:37:37.689Z:
Received chunk:  careful...

2025-04-02T03:37:37.714Z:
Received chunk:  not...

2025-04-02T03:37:37.738Z:
Received chunk:  to...

2025-04-02T03:37:37.762Z:
Received chunk:  over...

2025-04-02T03:37:37.787Z:
Received chunk: mix...

2025-04-02T03:37:37.811Z:
Received chunk: .
...

2025-04-02T03:37:37.836Z:
Received chunk: 6...

2025-04-02T03:37:37.860Z:
Received chunk: ....

2025-04-02T03:37:37.884Z:
Received chunk:  Stir...

2025-04-02T03:37:37.908Z:
Received chunk:  in...

2025-04-02T03:37:37.932Z:
Received chunk:  the...

2025-04-02T03:37:37.956Z:
Received chunk:  chocolate...

2025-04-02T03:37:37.980Z:
Received chunk:  chips...

2025-04-02T03:37:38.015Z:
Received chunk: .
...

2025-04-02T03:37:38.030Z:
Received chunk: 7...

2025-04-02T03:37:38.056Z:
Received chunk: ....

2025-04-02T03:37:38.080Z:
Received chunk:  Sco...

2025-04-02T03:37:38.104Z:
Received chunk: op...

2025-04-02T03:37:38.128Z:
Received chunk:  tablespoon...

2025-04-02T03:37:38.153Z:
Received chunk: -sized...

2025-04-02T03:37:38.178Z:
Received chunk:  balls...

2025-04-02T03:37:38.203Z:
Received chunk:  of...

2025-04-02T03:37:38.228Z:
Received chunk:  dough...

2025-04-02T03:37:38.253Z:
Received chunk:  onto...

2025-04-02T03:37:38.277Z:
Received chunk:  the...

2025-04-02T03:37:38.301Z:
Received chunk:  prepared...

2025-04-02T03:37:38.324Z:
Received chunk:  baking...

2025-04-02T03:37:38.349Z:
Received chunk:  sheet...

2025-04-02T03:37:38.373Z:
Received chunk: ,...

2025-04-02T03:37:38.396Z:
Received chunk:  leaving...

2025-04-02T03:37:38.420Z:
Received chunk:  about...

2025-04-02T03:37:38.443Z:
Received chunk:  ...

2025-04-02T03:37:38.467Z:
Received chunk: 2...

2025-04-02T03:37:38.491Z:
Received chunk:  inches...

2025-04-02T03:37:38.515Z:
Received chunk:  of...

2025-04-02T03:37:38.539Z:
Received chunk:  space...

2025-04-02T03:37:38.563Z:
Received chunk:  between...

2025-04-02T03:37:38.586Z:
Received chunk:  each...

2025-04-02T03:37:38.610Z:
Received chunk:  cookie...

2025-04-02T03:37:38.633Z:
Received chunk: .
...

2025-04-02T03:37:38.657Z:
Received chunk: 8...

2025-04-02T03:37:38.680Z:
Received chunk: ....

2025-04-02T03:37:38.704Z:
Received chunk:  Bake...

2025-04-02T03:37:38.729Z:
Received chunk:  for...

2025-04-02T03:37:38.753Z:
Received chunk:  ...

2025-04-02T03:37:38.778Z:
Received chunk: 10...

2025-04-02T03:37:38.803Z:
Received chunk: -...

2025-04-02T03:37:38.827Z:
Received chunk: 12...

2025-04-02T03:37:38.852Z:
Received chunk:  minutes...

2025-04-02T03:37:38.876Z:
Received chunk:  or...

2025-04-02T03:37:38.901Z:
Received chunk:  until...

2025-04-02T03:37:38.925Z:
Received chunk:  the...

2025-04-02T03:37:38.949Z:
Received chunk:  edges...

2025-04-02T03:37:38.974Z:
Received chunk:  are...

2025-04-02T03:37:38.999Z:
Received chunk:  lightly...

2025-04-02T03:37:39.023Z:
Received chunk:  golden...

2025-04-02T03:37:39.048Z:
Received chunk:  brown...

2025-04-02T03:37:39.073Z:
Received chunk: .
...

2025-04-02T03:37:39.098Z:
Received chunk: 9...

2025-04-02T03:37:39.123Z:
Received chunk: ....

2025-04-02T03:37:39.148Z:
Received chunk:  Remove...

2025-04-02T03:37:39.173Z:
Received chunk:  from...

2025-04-02T03:37:39.198Z:
Received chunk:  the...

2025-04-02T03:37:39.223Z:
Received chunk:  oven...

2025-04-02T03:37:39.248Z:
Received chunk:  and...

2025-04-02T03:37:39.273Z:
Received chunk:  let...

2025-04-02T03:37:39.297Z:
Received chunk:  cool...

2025-04-02T03:37:39.321Z:
Received chunk:  on...

2025-04-02T03:37:39.345Z:
Received chunk:  the...

2025-04-02T03:37:39.369Z:
Received chunk:  baking...

2025-04-02T03:37:39.393Z:
Received chunk:  sheet...

2025-04-02T03:37:39.417Z:
Received chunk:  for...

2025-04-02T03:37:39.440Z:
Received chunk:  ...

2025-04-02T03:37:39.464Z:
Received chunk: 5...

2025-04-02T03:37:39.488Z:
Received chunk:  minutes...

2025-04-02T03:37:39.512Z:
Received chunk:  before...

2025-04-02T03:37:39.536Z:
Received chunk:  transferring...

2025-04-02T03:37:39.560Z:
Received chunk:  to...

2025-04-02T03:37:39.584Z:
Received chunk:  a...

2025-04-02T03:37:39.607Z:
Received chunk:  wire...

2025-04-02T03:37:39.630Z:
Received chunk:  rack...

2025-04-02T03:37:39.654Z:
Received chunk:  to...

2025-04-02T03:37:39.678Z:
Received chunk:  cool...

2025-04-02T03:37:39.702Z:
Received chunk:  completely...

2025-04-02T03:37:39.727Z:
Received chunk: .

...

2025-04-02T03:37:39.751Z:
Received chunk: **...

2025-04-02T03:37:39.776Z:
Received chunk: Recipe...

2025-04-02T03:37:39.800Z:
Received chunk:  ...

2025-04-02T03:37:39.825Z:
Received chunk: 2...

2025-04-02T03:37:39.850Z:
Received chunk: :...

2025-04-02T03:37:39.874Z:
Received chunk:  Lemon...

2025-04-02T03:37:39.898Z:
Received chunk:  Bars...

2025-04-02T03:37:39.922Z:
Received chunk: **

...

2025-04-02T03:37:39.947Z:
Received chunk: Ingredients...

2025-04-02T03:37:39.971Z:
Received chunk: :

...

2025-04-02T03:37:39.996Z:
Received chunk: *...

2025-04-02T03:37:40.028Z:
Received chunk:  ...

2025-04-02T03:37:40.046Z:
Received chunk: 1...

2025-04-02T03:37:40.071Z:
Received chunk:  cup...

2025-04-02T03:37:40.096Z:
Received chunk:  all...

2025-04-02T03:37:40.120Z:
Received chunk: -purpose...

2025-04-02T03:37:40.144Z:
Received chunk:  flour...

2025-04-02T03:37:40.175Z:
Received chunk: 
...

2025-04-02T03:37:40.200Z:
Received chunk: *...

2025-04-02T03:37:40.226Z:
Received chunk:  ...

2025-04-02T03:37:40.251Z:
Received chunk: 1...

2025-04-02T03:37:40.275Z:
Received chunk: /...

2025-04-02T03:37:40.299Z:
Received chunk: 2...

2025-04-02T03:37:40.323Z:
Received chunk:  cup...

2025-04-02T03:37:40.347Z:
Received chunk:  gran...

2025-04-02T03:37:40.371Z:
Received chunk: ulated...

2025-04-02T03:37:40.396Z:
Received chunk:  sugar...

2025-04-02T03:37:40.420Z:
Received chunk: 
...

2025-04-02T03:37:40.444Z:
Received chunk: *...

2025-04-02T03:37:40.467Z:
Received chunk:  ...

2025-04-02T03:37:40.491Z:
Received chunk: 1...

2025-04-02T03:37:40.515Z:
Received chunk: /...

2025-04-02T03:37:40.539Z:
Received chunk: 4...

2025-04-02T03:37:40.564Z:
Received chunk:  cup...

2025-04-02T03:37:40.588Z:
Received chunk:  con...

2025-04-02T03:37:40.612Z:
Received chunk: fection...

2025-04-02T03:37:40.635Z:
Received chunk: ers...

2025-04-02T03:37:40.659Z:
Received chunk: '...

2025-04-02T03:37:40.682Z:
Received chunk:  sugar...

2025-04-02T03:37:40.706Z:
Received chunk: 
...

2025-04-02T03:37:40.730Z:
Received chunk: *...

2025-04-02T03:37:40.754Z:
Received chunk:  ...

2025-04-02T03:37:40.779Z:
Received chunk: 1...

2025-04-02T03:37:40.803Z:
Received chunk: /...

2025-04-02T03:37:40.828Z:
Received chunk: 4...

2025-04-02T03:37:40.852Z:
Received chunk:  teaspoon...

2025-04-02T03:37:40.876Z:
Received chunk:  salt...

2025-04-02T03:37:40.901Z:
Received chunk: 
...

2025-04-02T03:37:40.925Z:
Received chunk: *...

2025-04-02T03:37:40.950Z:
Received chunk:  ...

2025-04-02T03:37:40.974Z:
Received chunk: 1...

2025-04-02T03:37:40.999Z:
Received chunk: /...

2025-04-02T03:37:41.032Z:
Received chunk: 2...

2025-04-02T03:37:41.048Z:
Received chunk:  cup...

2025-04-02T03:37:41.073Z:
Received chunk:  uns...

2025-04-02T03:37:41.097Z:
Received chunk: alted...

2025-04-02T03:37:41.121Z:
Received chunk:  butter...

2025-04-02T03:37:41.146Z:
Received chunk: ,...

2025-04-02T03:37:41.170Z:
Received chunk:  melted...

2025-04-02T03:37:41.195Z:
Received chunk: 
...

2025-04-02T03:37:41.221Z:
Received chunk: *...

2025-04-02T03:37:41.247Z:
Received chunk:  ...

2025-04-02T03:37:41.271Z:
Received chunk: 2...

2025-04-02T03:37:41.296Z:
Received chunk:  large...

2025-04-02T03:37:41.320Z:
Received chunk:  eggs...

2025-04-02T03:37:41.344Z:
Received chunk: 
...

2025-04-02T03:37:41.368Z:
Received chunk: *...

2025-04-02T03:37:41.392Z:
Received chunk:  ...

2025-04-02T03:37:41.416Z:
Received chunk: 2...

2025-04-02T03:37:41.441Z:
Received chunk:  tablespoons...

2025-04-02T03:37:41.465Z:
Received chunk:  freshly...

2025-04-02T03:37:41.488Z:
Received chunk:  squeezed...

2025-04-02T03:37:41.512Z:
Received chunk:  lemon...

2025-04-02T03:37:41.536Z:
Received chunk:  juice...

2025-04-02T03:37:41.560Z:
Received chunk: 
...

2025-04-02T03:37:41.584Z:
Received chunk: *...

2025-04-02T03:37:41.609Z:
Received chunk:  ...

2025-04-02T03:37:41.633Z:
Received chunk: 1...

2025-04-02T03:37:41.657Z:
Received chunk:  tablespoon...

2025-04-02T03:37:41.681Z:
Received chunk:  grated...

2025-04-02T03:37:41.705Z:
Received chunk:  lemon...

2025-04-02T03:37:41.729Z:
Received chunk:  zest...

2025-04-02T03:37:41.753Z:
Received chunk: 

...

2025-04-02T03:37:41.778Z:
Received chunk: Instructions...

2025-04-02T03:37:41.803Z:
Received chunk: :

...

2025-04-02T03:37:41.828Z:
Received chunk: 1...

2025-04-02T03:37:41.852Z:
Received chunk: ....

2025-04-02T03:37:41.876Z:
Received chunk:  Pre...

2025-04-02T03:37:41.900Z:
Received chunk: heat...

2025-04-02T03:37:41.925Z:
Received chunk:  your...

2025-04-02T03:37:41.950Z:
Received chunk:  oven...

2025-04-02T03:37:41.974Z:
Received chunk:  to...

2025-04-02T03:37:42.000Z:
Received chunk:  ...

2025-04-02T03:37:42.034Z:
Received chunk: 350...

2025-04-02T03:37:42.050Z:
Received chunk: °F...

2025-04-02T03:37:42.075Z:
Received chunk:  (...

2025-04-02T03:37:42.100Z:
Received chunk: 180...

2025-04-02T03:37:42.124Z:
Received chunk: °C...

2025-04-02T03:37:42.149Z:
Received chunk: )....

2025-04-02T03:37:42.179Z:
Received chunk:  Line...

2025-04-02T03:37:42.199Z:
Received chunk:  an...

2025-04-02T03:37:42.224Z:
Received chunk:  ...

2025-04-02T03:37:42.249Z:
Received chunk: 8...

2025-04-02T03:37:42.274Z:
Received chunk: -inch...

2025-04-02T03:37:42.298Z:
Received chunk:  square...

2025-04-02T03:37:42.322Z:
Received chunk:  baking...

2025-04-02T03:37:42.346Z:
Received chunk:  dish...

2025-04-02T03:37:42.370Z:
Received chunk:  with...

2025-04-02T03:37:42.393Z:
Received chunk:  parchment...

2025-04-02T03:37:42.417Z:
Received chunk:  paper...

2025-04-02T03:37:42.441Z:
Received chunk:  or...

2025-04-02T03:37:42.465Z:
Received chunk:  a...

2025-04-02T03:37:42.489Z:
Received chunk:  silicone...

2025-04-02T03:37:42.513Z:
Received chunk:  mat...

2025-04-02T03:37:42.537Z:
Received chunk: .
...

2025-04-02T03:37:42.561Z:
Received chunk: 2...

2025-04-02T03:37:42.586Z:
Received chunk: ....

2025-04-02T03:37:42.610Z:
Received chunk:  In...

2025-04-02T03:37:42.633Z:
Received chunk:  a...

2025-04-02T03:37:42.657Z:
Received chunk:  medium...

2025-04-02T03:37:42.680Z:
Received chunk:  bowl...

2025-04-02T03:37:42.705Z:
Received chunk: ,...

2025-04-02T03:37:42.729Z:
Received chunk:  whisk...

2025-04-02T03:37:42.753Z:
Received chunk:  together...

2025-04-02T03:37:42.778Z:
Received chunk:  the...

2025-04-02T03:37:42.803Z:
Received chunk:  flour...

2025-04-02T03:37:42.828Z:
Received chunk: ,...

2025-04-02T03:37:42.853Z:
Received chunk:  gran...

2025-04-02T03:37:42.877Z:
Received chunk: ulated...

2025-04-02T03:37:42.902Z:
Received chunk:  sugar...

2025-04-02T03:37:42.927Z:
Received chunk: ,...

2025-04-02T03:37:42.951Z:
Received chunk:  con...

2025-04-02T03:37:42.976Z:
Received chunk: fection...

2025-04-02T03:37:43.001Z:
Received chunk: ers...

2025-04-02T03:37:43.036Z:
Received chunk: '...

2025-04-02T03:37:43.052Z:
Received chunk:  sugar...

2025-04-02T03:37:43.078Z:
Received chunk: ,...

2025-04-02T03:37:43.103Z:
Received chunk:  and...

2025-04-02T03:37:43.129Z:
Received chunk:  salt...

2025-04-02T03:37:43.153Z:
Received chunk: ....

2025-04-02T03:37:43.179Z:
Received chunk:  Set...

2025-04-02T03:37:43.204Z:
Received chunk:  aside...

2025-04-02T03:37:43.230Z:
Received chunk: .
...

2025-04-02T03:37:43.254Z:
Received chunk: 3...

2025-04-02T03:37:43.279Z:
Received chunk: ....

2025-04-02T03:37:43.303Z:
Received chunk:  Add...

2025-04-02T03:37:43.327Z:
Received chunk:  the...

2025-04-02T03:37:43.352Z:
Received chunk:  melted...

2025-04-02T03:37:43.376Z:
Received chunk:  butter...

2025-04-02T03:37:43.400Z:
Received chunk:  to...

2025-04-02T03:37:43.423Z:
Received chunk:  the...

2025-04-02T03:37:43.447Z:
Received chunk:  dry...

2025-04-02T03:37:43.471Z:
Received chunk:  ingredients...

2025-04-02T03:37:43.495Z:
Received chunk:  and...

2025-04-02T03:37:43.519Z:
Received chunk:  stir...

2025-04-02T03:37:43.543Z:
Received chunk:  until...

2025-04-02T03:37:43.567Z:
Received chunk:  a...

2025-04-02T03:37:43.592Z:
Received chunk:  cr...

2025-04-02T03:37:43.616Z:
Received chunk: umb...

2025-04-02T03:37:43.640Z:
Received chunk: ly...

2025-04-02T03:37:43.664Z:
Received chunk:  mixture...

2025-04-02T03:37:43.689Z:
Received chunk:  forms...

2025-04-02T03:37:43.713Z:
Received chunk: .
...

2025-04-02T03:37:43.738Z:
Received chunk: 4...

2025-04-02T03:37:43.763Z:
Received chunk: ....

2025-04-02T03:37:43.788Z:
Received chunk:  Press...

2025-04-02T03:37:43.813Z:
Received chunk:  half...

2025-04-02T03:37:43.838Z:
Received chunk:  of...

2025-04-02T03:37:43.863Z:
Received chunk:  the...

2025-04-02T03:37:43.887Z:
Received chunk:  cr...

2025-04-02T03:37:43.912Z:
Received chunk: umb...

2025-04-02T03:37:43.936Z:
Received chunk:  mixture...

2025-04-02T03:37:43.960Z:
Received chunk:  into...

2025-04-02T03:37:43.984Z:
Received chunk:  the...

2025-04-02T03:37:44.011Z:
Received chunk:  prepared...

2025-04-02T03:37:44.038Z:
Received chunk:  baking...

2025-04-02T03:37:44.061Z:
Received chunk:  dish...

2025-04-02T03:37:44.086Z:
Received chunk: .
...

2025-04-02T03:37:44.110Z:
Received chunk: 5...

2025-04-02T03:37:44.135Z:
Received chunk: ....

2025-04-02T03:37:44.160Z:
Received chunk:  Bake...

2025-04-02T03:37:44.186Z:
Received chunk:  for...

2025-04-02T03:37:44.212Z:
Received chunk:  ...

2025-04-02T03:37:44.238Z:
Received chunk: 20...

2025-04-02T03:37:44.262Z:
Received chunk: -...

2025-04-02T03:37:44.287Z:
Received chunk: 22...

2025-04-02T03:37:44.311Z:
Received chunk:  minutes...

2025-04-02T03:37:44.336Z:
Received chunk:  or...

2025-04-02T03:37:44.360Z:
Received chunk:  until...

2025-04-02T03:37:44.384Z:
Received chunk:  the...

2025-04-02T03:37:44.408Z:
Received chunk:  edges...

2025-04-02T03:37:44.432Z:
Received chunk:  are...

2025-04-02T03:37:44.456Z:
Received chunk:  lightly...

2025-04-02T03:37:44.480Z:
Received chunk:  golden...

2025-04-02T03:37:44.504Z:
Received chunk:  brown...

2025-04-02T03:37:44.528Z:
Received chunk: .
...

2025-04-02T03:37:44.552Z:
Received chunk: 6...

2025-04-02T03:37:44.576Z:
Received chunk: ....

2025-04-02T03:37:44.601Z:
Received chunk:  While...

2025-04-02T03:37:44.625Z:
Received chunk:  the...

2025-04-02T03:37:44.649Z:
Received chunk:  crust...

2025-04-02T03:37:44.673Z:
Received chunk:  is...

2025-04-02T03:37:44.696Z:
Received chunk:  baking...

2025-04-02T03:37:44.721Z:
Received chunk: ,...

2025-04-02T03:37:44.745Z:
Received chunk:  prepare...

2025-04-02T03:37:44.770Z:
Received chunk:  the...

2025-04-02T03:37:44.795Z:
Received chunk:  lemon...

2025-04-02T03:37:44.820Z:
Received chunk:  filling...

2025-04-02T03:37:44.845Z:
Received chunk:  by...

2025-04-02T03:37:44.869Z:
Received chunk:  whisk...

2025-04-02T03:37:44.894Z:
Received chunk: ing...

2025-04-02T03:37:44.918Z:
Received chunk:  together...

2025-04-02T03:37:44.943Z:
Received chunk:  the...

2025-04-02T03:37:44.967Z:
Received chunk:  eggs...

2025-04-02T03:37:44.992Z:
Received chunk: ,...

2025-04-02T03:37:45.018Z:
Received chunk:  lemon...

2025-04-02T03:37:45.048Z:
Received chunk:  juice...

2025-04-02T03:37:45.067Z:
Received chunk: ,...

2025-04-02T03:37:45.092Z:
Received chunk:  and...

2025-04-02T03:37:45.117Z:
Received chunk:  grated...

2025-04-02T03:37:45.142Z:
Received chunk:  lemon...

2025-04-02T03:37:45.167Z:
Received chunk:  zest...

2025-04-02T03:37:45.192Z:
Received chunk:  in...

2025-04-02T03:37:45.218Z:
Received chunk:  a...

2025-04-02T03:37:45.245Z:
Received chunk:  small...

2025-04-02T03:37:45.270Z:
Received chunk:  bowl...

2025-04-02T03:37:45.295Z:
Received chunk: .
...

2025-04-02T03:37:45.318Z:
Received chunk: 7...

2025-04-02T03:37:45.343Z:
Received chunk: ....

2025-04-02T03:37:45.367Z:
Received chunk:  Remove...

2025-04-02T03:37:45.391Z:
Received chunk:  from...

2025-04-02T03:37:45.415Z:
Received chunk:  the...

2025-04-02T03:37:45.439Z:
Received chunk:  oven...

2025-04-02T03:37:45.463Z:
Received chunk:  and...

2025-04-02T03:37:45.486Z:
Received chunk:  let...

2025-04-02T03:37:45.510Z:
Received chunk:  cool...

2025-04-02T03:37:45.534Z:
Received chunk:  for...

2025-04-02T03:37:45.558Z:
Received chunk:  ...

2025-04-02T03:37:45.582Z:
Received chunk: 5...

2025-04-02T03:37:45.607Z:
Received chunk:  minutes...

2025-04-02T03:37:45.631Z:
Received chunk:  before...

2025-04-02T03:37:45.655Z:
Received chunk:  pouring...

2025-04-02T03:37:45.678Z:
Received chunk:  the...

2025-04-02T03:37:45.703Z:
Received chunk:  lemon...

2025-04-02T03:37:45.727Z:
Received chunk:  filling...

2025-04-02T03:37:45.751Z:
Received chunk:  over...

2025-04-02T03:37:45.776Z:
Received chunk:  the...

2025-04-02T03:37:45.801Z:
Received chunk:  warm...

2025-04-02T03:37:45.826Z:
Received chunk:  crust...

2025-04-02T03:37:45.852Z:
Received chunk: .
...

2025-04-02T03:37:45.876Z:
Received chunk: 8...

2025-04-02T03:37:45.901Z:
Received chunk: ....

2025-04-02T03:37:45.925Z:
Received chunk:  Return...

2025-04-02T03:37:45.950Z:
Received chunk:  to...

2025-04-02T03:37:45.975Z:
Received chunk:  the...

2025-04-02T03:37:46.001Z:
Received chunk:  oven...

2025-04-02T03:37:46.025Z:
Received chunk:  and...

2025-04-02T03:37:46.060Z:
Received chunk:  bake...

2025-04-02T03:37:46.076Z:
Received chunk:  for...

2025-04-02T03:37:46.101Z:
Received chunk:  an...

2025-04-02T03:37:46.126Z:
Received chunk:  additional...

2025-04-02T03:37:46.150Z:
Received chunk:  ...

2025-04-02T03:37:46.175Z:
Received chunk: 20...

2025-04-02T03:37:46.199Z:
Received chunk: -...

2025-04-02T03:37:46.225Z:
Received chunk: 22...

2025-04-02T03:37:46.251Z:
Received chunk:  minutes...

2025-04-02T03:37:46.276Z:
Received chunk:  or...

2025-04-02T03:37:46.302Z:
Received chunk:  until...

2025-04-02T03:37:46.326Z:
Received chunk:  the...

2025-04-02T03:37:46.350Z:
Received chunk:  filling...

2025-04-02T03:37:46.373Z:
Received chunk:  is...

2025-04-02T03:37:46.398Z:
Received chunk:  set...

2025-04-02T03:37:46.422Z:
Received chunk:  and...

2025-04-02T03:37:46.445Z:
Received chunk:  the...

2025-04-02T03:37:46.469Z:
Received chunk:  edges...

2025-04-02T03:37:46.493Z:
Received chunk:  are...

2025-04-02T03:37:46.517Z:
Received chunk:  lightly...

2025-04-02T03:37:46.541Z:
Received chunk:  golden...

2025-04-02T03:37:46.565Z:
Received chunk:  brown...

2025-04-02T03:37:46.589Z:
Received chunk: .

...

2025-04-02T03:37:46.613Z:
Received chunk: **...

2025-04-02T03:37:46.638Z:
Received chunk: Recipe...

2025-04-02T03:37:46.663Z:
Received chunk:  ...

2025-04-02T03:37:46.687Z:
Received chunk: 3...

2025-04-02T03:37:46.712Z:
Received chunk: :...

2025-04-02T03:37:46.736Z:
Received chunk:  C...

2025-04-02T03:37:46.761Z:
Received chunk: innamon...

2025-04-02T03:37:46.787Z:
Received chunk:  Sugar...

2025-04-02T03:37:46.812Z:
Received chunk:  Don...

2025-04-02T03:37:46.837Z:
Received chunk: uts...

2025-04-02T03:37:46.861Z:
Received chunk: **

...

2025-04-02T03:37:46.886Z:
Received chunk: Ingredients...

2025-04-02T03:37:46.911Z:
Received chunk: :

...

2025-04-02T03:37:46.936Z:
Received chunk: *...

2025-04-02T03:37:46.961Z:
Received chunk:  ...

2025-04-02T03:37:46.985Z:
Received chunk: 2...

2025-04-02T03:37:47.011Z:
Received chunk:  cups...

2025-04-02T03:37:47.036Z:
Received chunk:  all...

2025-04-02T03:37:47.061Z:
Received chunk: -purpose...

2025-04-02T03:37:47.086Z:
Received chunk:  flour...

2025-04-02T03:37:47.112Z:
Received chunk: 
...

2025-04-02T03:37:47.137Z:
Received chunk: *...

2025-04-02T03:37:47.161Z:
Received chunk:  ...

2025-04-02T03:37:47.187Z:
Received chunk: 1...

2025-04-02T03:37:47.212Z:
Received chunk: /...

2025-04-02T03:37:47.238Z:
Received chunk: 2...

2025-04-02T03:37:47.263Z:
Received chunk:  cup...

2025-04-02T03:37:47.290Z:
Received chunk:  gran...

2025-04-02T03:37:47.315Z:
Received chunk: ulated...

2025-04-02T03:37:47.339Z:
Received chunk:  sugar...

2025-04-02T03:37:47.362Z:
Received chunk: 
...

2025-04-02T03:37:47.387Z:
Received chunk: *...

2025-04-02T03:37:47.412Z:
Received chunk:  ...

2025-04-02T03:37:47.436Z:
Received chunk: 1...

2025-04-02T03:37:47.459Z:
Received chunk: /...

2025-04-02T03:37:47.484Z:
Received chunk: 4...

2025-04-02T03:37:47.508Z:
Received chunk:  cup...

2025-04-02T03:37:47.532Z:
Received chunk:  whole...

2025-04-02T03:37:47.556Z:
Received chunk:  milk...

2025-04-02T03:37:47.580Z:
Received chunk: 
...

2025-04-02T03:37:47.605Z:
Received chunk: *...

2025-04-02T03:37:47.629Z:
Received chunk:  ...

2025-04-02T03:37:47.654Z:
Received chunk: 2...

2025-04-02T03:37:47.678Z:
Received chunk:  teaspoons...

2025-04-02T03:37:47.702Z:
Received chunk:  active...

2025-04-02T03:37:47.725Z:
Received chunk:  dry...

2025-04-02T03:37:47.750Z:
Received chunk:  yeast...

2025-04-02T03:37:47.774Z:
Received chunk: 
...

2025-04-02T03:37:47.799Z:
Received chunk: *...

2025-04-02T03:37:47.824Z:
Received chunk:  ...

2025-04-02T03:37:47.848Z:
Received chunk: 1...

2025-04-02T03:37:47.873Z:
Received chunk: /...

2025-04-02T03:37:47.898Z:
Received chunk: 4...

2025-04-02T03:37:47.922Z:
Received chunk:  teaspoon...

2025-04-02T03:37:47.947Z:
Received chunk:  salt...

2025-04-02T03:37:47.971Z:
Received chunk: 
...

2025-04-02T03:37:47.996Z:
Received chunk: *...

2025-04-02T03:37:48.021Z:
Received chunk:  ...

2025-04-02T03:37:48.046Z:
Received chunk: 2...

2025-04-02T03:37:48.075Z:
Received chunk:  large...

2025-04-02T03:37:48.098Z:
Received chunk:  eggs...

2025-04-02T03:37:48.123Z:
Received chunk: 
...

2025-04-02T03:37:48.148Z:
Received chunk: *...

2025-04-02T03:37:48.173Z:
Received chunk:  ...

2025-04-02T03:37:48.198Z:
Received chunk: 2...

2025-04-02T03:37:48.224Z:
Received chunk:  tablespoons...

2025-04-02T03:37:48.250Z:
Received chunk:  uns...

2025-04-02T03:37:48.275Z:
Received chunk: alted...

2025-04-02T03:37:48.301Z:
Received chunk:  butter...

2025-04-02T03:37:48.326Z:
Received chunk: ,...

2025-04-02T03:37:48.350Z:
Received chunk:  melted...

2025-04-02T03:37:48.374Z:
Received chunk: 
...

2025-04-02T03:37:48.398Z:
Received chunk: *...

2025-04-02T03:37:48.423Z:
Received chunk:  ...

2025-04-02T03:37:48.446Z:
Received chunk: 1...

2025-04-02T03:37:48.470Z:
Received chunk: /...

2025-04-02T03:37:48.495Z:
Received chunk: 2...

2025-04-02T03:37:48.519Z:
Received chunk:  cup...

2025-04-02T03:37:48.544Z:
Received chunk:  cinnamon...

2025-04-02T03:37:48.567Z:
Received chunk:  sugar...

2025-04-02T03:37:48.591Z:
Received chunk:  (...

2025-04-02T03:37:48.616Z:
Received chunk: a...

2025-04-02T03:37:48.639Z:
Received chunk:  mixture...

2025-04-02T03:37:48.664Z:
Received chunk:  of...

2025-04-02T03:37:48.688Z:
Received chunk:  equal...

2025-04-02T03:37:48.713Z:
Received chunk:  parts...

2025-04-02T03:37:48.737Z:
Received chunk:  gran...

2025-04-02T03:37:48.762Z:
Received chunk: ulated...

2025-04-02T03:37:48.787Z:
Received chunk:  sugar...

2025-04-02T03:37:48.811Z:
Received chunk:  and...

2025-04-02T03:37:48.836Z:
Received chunk:  ground...

2025-04-02T03:37:48.862Z:
Received chunk:  cinnamon...

2025-04-02T03:37:48.887Z:
Received chunk: )

...

2025-04-02T03:37:48.913Z:
Received chunk: Instructions...

2025-04-02T03:37:48.938Z:
Received chunk: :

...

2025-04-02T03:37:48.963Z:
Received chunk: 1...

2025-04-02T03:37:48.988Z:
Received chunk: ....

2025-04-02T03:37:49.014Z:
Received chunk:  Pre...

2025-04-02T03:37:49.038Z:
Received chunk: heat...

2025-04-02T03:37:49.063Z:
Received chunk:  your...

2025-04-02T03:37:49.089Z:
Received chunk:  oil...

2025-04-02T03:37:49.114Z:
Received chunk:  in...

2025-04-02T03:37:49.140Z:
Received chunk:  a...

2025-04-02T03:37:49.165Z:
Received chunk:  deep...

2025-04-02T03:37:49.192Z:
Received chunk:  frying...

2025-04-02T03:37:49.217Z:
Received chunk:  pan...

2025-04-02T03:37:49.243Z:
Received chunk:  or...

2025-04-02T03:37:49.269Z:
Received chunk:  a...

2025-04-02T03:37:49.295Z:
Received chunk:  deep...

2025-04-02T03:37:49.321Z:
Received chunk:  fry...

2025-04-02T03:37:49.346Z:
Received chunk: er...

2025-04-02T03:37:49.371Z:
Received chunk:  to...

2025-04-02T03:37:49.396Z:
Received chunk:  ...

2025-04-02T03:37:49.420Z:
Received chunk: 350...

2025-04-02T03:37:49.445Z:
Received chunk: °F...

2025-04-02T03:37:49.469Z:
Received chunk:  (...

2025-04-02T03:37:49.493Z:
Received chunk: 180...

2025-04-02T03:37:49.518Z:
Received chunk: °C...

2025-04-02T03:37:49.543Z:
Received chunk: ).
...

2025-04-02T03:37:49.567Z:
Received chunk: 2...

2025-04-02T03:37:49.591Z:
Received chunk: ....

2025-04-02T03:37:49.616Z:
Received chunk:  In...

2025-04-02T03:37:49.640Z:
Received chunk:  a...

2025-04-02T03:37:49.665Z:
Received chunk:  medium...

2025-04-02T03:37:49.690Z:
Received chunk:  bowl...

2025-04-02T03:37:49.715Z:
Received chunk: ,...

2025-04-02T03:37:49.739Z:
Received chunk:  whisk...

2025-04-02T03:37:49.763Z:
Received chunk:  together...

2025-04-02T03:37:49.788Z:
Received chunk:  the...

2025-04-02T03:37:49.814Z:
Received chunk:  flour...

2025-04-02T03:37:49.839Z:
Received chunk: ,...

2025-04-02T03:37:49.864Z:
Received chunk:  sugar...

2025-04-02T03:37:49.889Z:
Received chunk: ,...

2025-04-02T03:37:49.913Z:
Received chunk:  yeast...

2025-04-02T03:37:49.938Z:
Received chunk: ,...

2025-04-02T03:37:49.963Z:
Received chunk:  and...

2025-04-02T03:37:49.987Z:
Received chunk:  salt...

2025-04-02T03:37:50.013Z:
Received chunk: .
...

2025-04-02T03:37:50.037Z:
Received chunk: 3...

2025-04-02T03:37:50.062Z:
Received chunk: ....

2025-04-02T03:37:50.097Z:
Received chunk:  Add...

2025-04-02T03:37:50.113Z:
Received chunk:  the...

2025-04-02T03:37:50.139Z:
Received chunk:  milk...

2025-04-02T03:37:50.164Z:
Received chunk: ,...

2025-04-02T03:37:50.189Z:
Received chunk:  eggs...

2025-04-02T03:37:50.214Z:
Received chunk: ,...

2025-04-02T03:37:50.239Z:
Received chunk:  and...

2025-04-02T03:37:50.265Z:
Received chunk:  melted...

2025-04-02T03:37:50.291Z:
Received chunk:  butter...

2025-04-02T03:37:50.317Z:
Received chunk:  to...

2025-04-02T03:37:50.342Z:
Received chunk:  the...

2025-04-02T03:37:50.366Z:
Received chunk:  dry...

2025-04-02T03:37:50.391Z:
Received chunk:  ingredients...

2025-04-02T03:37:50.415Z:
Received chunk:  and...

2025-04-02T03:37:50.439Z:
Received chunk:  stir...

2025-04-02T03:37:50.463Z:
Received chunk:  until...

2025-04-02T03:37:50.487Z:
Received chunk:  a...

2025-04-02T03:37:50.511Z:
Received chunk:  smooth...

2025-04-02T03:37:50.535Z:
Received chunk:  batter...

2025-04-02T03:37:50.559Z:
Received chunk:  forms...

2025-04-02T03:37:50.583Z:
Received chunk: .
...

2025-04-02T03:37:50.607Z:
Received chunk: 4...

2025-04-02T03:37:50.632Z:
Received chunk: ....

2025-04-02T03:37:50.656Z:
Received chunk:  Cover...

2025-04-02T03:37:50.680Z:
Received chunk:  the...

2025-04-02T03:37:50.704Z:
Received chunk:  bowl...

2025-04-02T03:37:50.729Z:
Received chunk:  with...

2025-04-02T03:37:50.752Z:
Received chunk:  plastic...

2025-04-02T03:37:50.776Z:
Received chunk:  wrap...

2025-04-02T03:37:50.801Z:
Received chunk:  and...

2025-04-02T03:37:50.827Z:
Received chunk:  let...

2025-04-02T03:37:50.851Z:
Received chunk:  the...

2025-04-02T03:37:50.876Z:
Received chunk:  dough...

2025-04-02T03:37:50.901Z:
Received chunk:  rise...

2025-04-02T03:37:50.926Z:
Received chunk:  in...

2025-04-02T03:37:50.951Z:
Received chunk:  a...

2025-04-02T03:37:50.976Z:
Received chunk:  warm...

2025-04-02T03:37:51.002Z:
Received chunk:  place...

2025-04-02T03:37:51.026Z:
Received chunk:  for...

2025-04-02T03:37:51.051Z:
Received chunk:  ...

2025-04-02T03:37:51.077Z:
Received chunk: 1...

2025-04-02T03:37:51.102Z:
Received chunk:  hour...

2025-04-02T03:37:51.128Z:
Received chunk:  or...

2025-04-02T03:37:51.153Z:
Received chunk:  until...

2025-04-02T03:37:51.178Z:
Received chunk:  doubled...

2025-04-02T03:37:51.203Z:
Received chunk:  in...

2025-04-02T03:37:51.234Z:
Received chunk:  size...

2025-04-02T03:37:51.254Z:
Received chunk: .
...

2025-04-02T03:37:51.280Z:
Received chunk: 5...

2025-04-02T03:37:51.306Z:
Received chunk: ....

2025-04-02T03:37:51.331Z:
Received chunk:  Punch...

2025-04-02T03:37:51.356Z:
Received chunk:  down...

2025-04-02T03:37:51.380Z:
Received chunk:  the...

2025-04-02T03:37:51.405Z:
Received chunk:  dough...

2025-04-02T03:37:51.429Z:
Received chunk:  and...

2025-04-02T03:37:51.454Z:
Received chunk:  divide...

2025-04-02T03:37:51.478Z:
Received chunk:  into...

2025-04-02T03:37:51.502Z:
Received chunk:  small...

2025-04-02T03:37:51.526Z:
Received chunk:  balls...

2025-04-02T03:37:51.551Z:
Received chunk:  (...

2025-04-02T03:37:51.575Z:
Received chunk: about...

2025-04-02T03:37:51.600Z:
Received chunk:  ...

2025-04-02T03:37:51.624Z:
Received chunk: 1...

2025-04-02T03:37:51.648Z:
Received chunk: -inch...

2025-04-02T03:37:51.672Z:
Received chunk:  in...

2025-04-02T03:37:51.697Z:
Received chunk:  diameter...

2025-04-02T03:37:51.721Z:
Received chunk: ).
...

2025-04-02T03:37:51.746Z:
Received chunk: 6...

2025-04-02T03:37:51.770Z:
Received chunk: ....

2025-04-02T03:37:51.795Z:
Received chunk:  Fry...

2025-04-02T03:37:51.821Z:
Received chunk:  the...

2025-04-02T03:37:51.846Z:
Received chunk:  don...

2025-04-02T03:37:51.871Z:
Received chunk: uts...

2025-04-02T03:37:51.896Z:
Received chunk:  for...

2025-04-02T03:37:51.921Z:
Received chunk:  ...

2025-04-02T03:37:51.945Z:
Received chunk: 2...

2025-04-02T03:37:51.970Z:
Received chunk: -...

2025-04-02T03:37:51.995Z:
Received chunk: 3...

2025-04-02T03:37:52.021Z:
Received chunk:  minutes...

2025-04-02T03:37:52.046Z:
Received chunk:  on...

2025-04-02T03:37:52.071Z:
Received chunk:  each...

2025-04-02T03:37:52.104Z:
Received chunk:  side...

2025-04-02T03:37:52.122Z:
Received chunk:  or...

2025-04-02T03:37:52.148Z:
Received chunk:  until...

2025-04-02T03:37:52.173Z:
Received chunk:  golden...

2025-04-02T03:37:52.199Z:
Received chunk:  brown...

2025-04-02T03:37:52.224Z:
Received chunk: .
...

2025-04-02T03:37:52.250Z:
Received chunk: 7...

2025-04-02T03:37:52.275Z:
Received chunk: ....

2025-04-02T03:37:52.301Z:
Received chunk:  Drain...

2025-04-02T03:37:52.326Z:
Received chunk:  on...

2025-04-02T03:37:52.352Z:
Received chunk:  paper...

2025-04-02T03:37:52.376Z:
Received chunk:  towels...

2025-04-02T03:37:52.401Z:
Received chunk:  and...

2025-04-02T03:37:52.425Z:
Received chunk:  toss...

2025-04-02T03:37:52.449Z:
Received chunk:  with...

2025-04-02T03:37:52.473Z:
Received chunk:  cinnamon...

2025-04-02T03:37:52.498Z:
Received chunk:  sugar...

2025-04-02T03:37:52.523Z:
Received chunk:  while...

2025-04-02T03:37:52.548Z:
Received chunk:  still...

2025-04-02T03:37:52.572Z:
Received chunk:  warm...

2025-04-02T03:37:52.596Z:
Received chunk: .

...

2025-04-02T03:37:52.621Z:
Received chunk: **...

2025-04-02T03:37:52.646Z:
Received chunk: Recipe...

2025-04-02T03:37:52.670Z:
Received chunk:  ...

2025-04-02T03:37:52.695Z:
Received chunk: 4...

2025-04-02T03:37:52.719Z:
Received chunk: :...

2025-04-02T03:37:52.743Z:
Received chunk:  Che...

2025-04-02T03:37:52.768Z:
Received chunk: es...

2025-04-02T03:37:52.793Z:
Received chunk: ecake...

2025-04-02T03:37:52.817Z:
Received chunk: **

...

2025-04-02T03:37:52.843Z:
Received chunk: Ingredients...

2025-04-02T03:37:52.869Z:
Received chunk: :

...

2025-04-02T03:37:52.894Z:
Received chunk: *...

2025-04-02T03:37:52.919Z:
Received chunk:  ...

2025-04-02T03:37:52.943Z:
Received chunk: 1...

2025-04-02T03:37:52.968Z:
Received chunk:  ...

2025-04-02T03:37:52.993Z:
Received chunk: 1...

2025-04-02T03:37:53.019Z:
Received chunk: /...

2025-04-02T03:37:53.043Z:
Received chunk: 2...

2025-04-02T03:37:53.069Z:
Received chunk:  cups...

2025-04-02T03:37:53.094Z:
Received chunk:  g...

2025-04-02T03:37:53.119Z:
Received chunk: raham...

2025-04-02T03:37:53.145Z:
Received chunk:  cr...

2025-04-02T03:37:53.170Z:
Received chunk: acker...

2025-04-02T03:37:53.195Z:
Received chunk:  crumbs...

2025-04-02T03:37:53.220Z:
Received chunk: 
...

2025-04-02T03:37:53.246Z:
Received chunk: *...

2025-04-02T03:37:53.272Z:
Received chunk:  ...

2025-04-02T03:37:53.298Z:
Received chunk: 1...

2025-04-02T03:37:53.324Z:
Received chunk: /...

2025-04-02T03:37:53.349Z:
Received chunk: 4...

2025-04-02T03:37:53.374Z:
Received chunk:  cup...

2025-04-02T03:37:53.398Z:
Received chunk:  gran...

2025-04-02T03:37:53.422Z:
Received chunk: ulated...

2025-04-02T03:37:53.446Z:
Received chunk:  sugar...

2025-04-02T03:37:53.470Z:
Received chunk: 
...

2025-04-02T03:37:53.494Z:
Received chunk: *...

2025-04-02T03:37:53.519Z:
Received chunk:  ...

2025-04-02T03:37:53.543Z:
Received chunk: 6...

2025-04-02T03:37:53.567Z:
Received chunk:  ounces...

2025-04-02T03:37:53.591Z:
Received chunk:  cream...

2025-04-02T03:37:53.615Z:
Received chunk:  cheese...

2025-04-02T03:37:53.639Z:
Received chunk: ,...

2025-04-02T03:37:53.663Z:
Received chunk:  softened...

2025-04-02T03:37:53.687Z:
Received chunk: 
...

2025-04-02T03:37:53.712Z:
Received chunk: *...

2025-04-02T03:37:53.736Z:
Received chunk:  ...

2025-04-02T03:37:53.760Z:
Received chunk: 3...

2025-04-02T03:37:53.785Z:
Received chunk:  large...

2025-04-02T03:37:53.811Z:
Received chunk:  eggs...

2025-04-02T03:37:53.836Z:
Received chunk: 
...

2025-04-02T03:37:53.861Z:
Received chunk: *...

2025-04-02T03:37:53.886Z:
Received chunk:  ...

2025-04-02T03:37:53.911Z:
Received chunk: 1...

2025-04-02T03:37:53.936Z:
Received chunk: /...

2025-04-02T03:37:53.960Z:
Received chunk: 2...

2025-04-02T03:37:53.986Z:
Received chunk:  cup...

2025-04-02T03:37:54.012Z:
Received chunk:  gran...

2025-04-02T03:37:54.037Z:
Received chunk: ulated...

2025-04-02T03:37:54.061Z:
Received chunk:  sugar...

2025-04-02T03:37:54.087Z:
Received chunk: 
...

2025-04-02T03:37:54.112Z:
Received chunk: *...

2025-04-02T03:37:54.138Z:
Received chunk:  ...

2025-04-02T03:37:54.164Z:
Received chunk: 1...

2025-04-02T03:37:54.189Z:
Received chunk:  teaspoon...

2025-04-02T03:37:54.214Z:
Received chunk:  vanilla...

2025-04-02T03:37:54.239Z:
Received chunk:  extract...

2025-04-02T03:37:54.264Z:
Received chunk: 
...

2025-04-02T03:37:54.290Z:
Received chunk: *...

2025-04-02T03:37:54.316Z:
Received chunk:  ...

2025-04-02T03:37:54.342Z:
Received chunk: 1...

2025-04-02T03:37:54.367Z:
Received chunk:  cup...

2025-04-02T03:37:54.392Z:
Received chunk:  sour...

2025-04-02T03:37:54.417Z:
Received chunk:  cream...

2025-04-02T03:37:54.442Z:
Received chunk: 

...

2025-04-02T03:37:54.466Z:
Received chunk: Instructions...

2025-04-02T03:37:54.491Z:
Received chunk: :

...

2025-04-02T03:37:54.516Z:
Received chunk: 1...

2025-04-02T03:37:54.540Z:
Received chunk: ....

2025-04-02T03:37:54.564Z:
Received chunk:  Pre...

2025-04-02T03:37:54.589Z:
Received chunk: heat...

2025-04-02T03:37:54.612Z:
Received chunk:  your...

2025-04-02T03:37:54.636Z:
Received chunk:  oven...

2025-04-02T03:37:54.661Z:
Received chunk:  to...

2025-04-02T03:37:54.685Z:
Received chunk:  ...

2025-04-02T03:37:54.710Z:
Received chunk: 350...

2025-04-02T03:37:54.734Z:
Received chunk: °F...

2025-04-02T03:37:54.759Z:
Received chunk:  (...

2025-04-02T03:37:54.783Z:
Received chunk: 180...

2025-04-02T03:37:54.807Z:
Received chunk: °C...

2025-04-02T03:37:54.833Z:
Received chunk: ).
...

2025-04-02T03:37:54.858Z:
Received chunk: 2...

2025-04-02T03:37:54.883Z:
Received chunk: ....

2025-04-02T03:37:54.908Z:
Received chunk:  In...

2025-04-02T03:37:54.933Z:
Received chunk:  a...

2025-04-02T03:37:54.957Z:
Received chunk:  medium...

2025-04-02T03:37:54.982Z:
Received chunk:  bowl...

2025-04-02T03:37:55.007Z:
Received chunk: ,...

2025-04-02T03:37:55.032Z:
Received chunk:  mix...

2025-04-02T03:37:55.057Z:
Received chunk:  together...

2025-04-02T03:37:55.083Z:
Received chunk:  the...

2025-04-02T03:37:55.116Z:
Received chunk:  g...

2025-04-02T03:37:55.134Z:
Received chunk: raham...

2025-04-02T03:37:55.160Z:
Received chunk:  cr...

2025-04-02T03:37:55.185Z:
Received chunk: acker...

2025-04-02T03:37:55.210Z:
Received chunk:  crumbs...

2025-04-02T03:37:55.235Z:
Received chunk:  and...

2025-04-02T03:37:55.261Z:
Received chunk:  sugar...

2025-04-02T03:37:55.293Z:
Received chunk:  for...

2025-04-02T03:37:55.321Z:
Received chunk:  the...

2025-04-02T03:37:55.346Z:
Received chunk:  crust...

2025-04-02T03:37:55.372Z:
Received chunk: .
...

2025-04-02T03:37:55.397Z:
Received chunk: 3...

2025-04-02T03:37:55.422Z:
Received chunk: ....

2025-04-02T03:37:55.446Z:
Received chunk:  Press...

2025-04-02T03:37:55.470Z:
Received chunk:  the...

2025-04-02T03:37:55.494Z:
Received chunk:  mixture...

2025-04-02T03:37:55.519Z:
Received chunk:  into...

2025-04-02T03:37:55.544Z:
Received chunk:  the...

2025-04-02T03:37:55.568Z:
Received chunk:  bottom...

2025-04-02T03:37:55.593Z:
Received chunk:  of...

2025-04-02T03:37:55.618Z:
Received chunk:  a...

2025-04-02T03:37:55.643Z:
Received chunk:  ...

2025-04-02T03:37:55.667Z:
Received chunk: 9...

2025-04-02T03:37:55.691Z:
Received chunk: -inch...

2025-04-02T03:37:55.715Z:
Received chunk:  spring...

2025-04-02T03:37:55.739Z:
Received chunk: form...

2025-04-02T03:37:55.764Z:
Received chunk:  pan...

2025-04-02T03:37:55.789Z:
Received chunk: .
...

2025-04-02T03:37:55.814Z:
Received chunk: 4...

2025-04-02T03:37:55.839Z:
Received chunk: ....

2025-04-02T03:37:55.863Z:
Received chunk:  Bake...

2025-04-02T03:37:55.888Z:
Received chunk:  for...

2025-04-02T03:37:55.913Z:
Received chunk:  ...

2025-04-02T03:37:55.938Z:
Received chunk: 10...

2025-04-02T03:37:55.963Z:
Received chunk: -...

2025-04-02T03:37:55.988Z:
Received chunk: 12...

2025-04-02T03:37:56.014Z:
Received chunk:  minutes...

2025-04-02T03:37:56.039Z:
Received chunk:  or...

2025-04-02T03:37:56.064Z:
Received chunk:  until...

2025-04-02T03:37:56.089Z:
Received chunk:  lightly...

2025-04-02T03:37:56.123Z:
Received chunk:  brown...

2025-04-02T03:37:56.140Z:
Received chunk: ed...

2025-04-02T03:37:56.166Z:
Received chunk: .
...

2025-04-02T03:37:56.191Z:
Received chunk: 5...

2025-04-02T03:37:56.217Z:
Received chunk: ....

2025-04-02T03:37:56.241Z:
Received chunk:  In...

2025-04-02T03:37:56.272Z:
Received chunk:  a...

2025-04-02T03:37:56.293Z:
Received chunk:  large...

2025-04-02T03:37:56.318Z:
Received chunk:  bowl...

2025-04-02T03:37:56.344Z:
Received chunk: ,...

2025-04-02T03:37:56.369Z:
Received chunk:  beat...

2025-04-02T03:37:56.395Z:
Received chunk:  the...

2025-04-02T03:37:56.420Z:
Received chunk:  cream...

2025-04-02T03:37:56.444Z:
Received chunk:  cheese...

2025-04-02T03:37:56.469Z:
Received chunk:  until...

2025-04-02T03:37:56.494Z:
Received chunk:  smooth...

2025-04-02T03:37:56.518Z:
Received chunk: .
...

2025-04-02T03:37:56.543Z:
Received chunk: 6...

2025-04-02T03:37:56.567Z:
Received chunk: ....

2025-04-02T03:37:56.592Z:
Received chunk:  Add...

2025-04-02T03:37:56.617Z:
Received chunk:  the...

2025-04-02T03:37:56.641Z:
Received chunk:  eggs...

2025-04-02T03:37:56.665Z:
Received chunk:  one...

2025-04-02T03:37:56.690Z:
Received chunk:  at...

2025-04-02T03:37:56.714Z:
Received chunk:  a...

2025-04-02T03:37:56.738Z:
Received chunk:  time...

2025-04-02T03:37:56.762Z:
Received chunk: ,...

2025-04-02T03:37:56.786Z:
Received chunk:  beating...

2025-04-02T03:37:56.810Z:
Received chunk:  well...

2025-04-02T03:37:56.836Z:
Received chunk:  after...

2025-04-02T03:37:56.861Z:
Received chunk:  each...

2025-04-02T03:37:56.887Z:
Received chunk:  addition...

2025-04-02T03:37:56.912Z:
Received chunk: .
...

2025-04-02T03:37:56.937Z:
Received chunk: 7...

2025-04-02T03:37:56.962Z:
Received chunk: ....

2025-04-02T03:37:56.987Z:
Received chunk:  Beat...

2025-04-02T03:37:57.013Z:
Received chunk:  in...

2025-04-02T03:37:57.038Z:
Received chunk:  the...

2025-04-02T03:37:57.063Z:
Received chunk:  gran...

2025-04-02T03:37:57.088Z:
Received chunk: ulated...

2025-04-02T03:37:57.113Z:
Received chunk:  sugar...

2025-04-02T03:37:57.139Z:
Received chunk:  and...

2025-04-02T03:37:57.166Z:
Received chunk:  vanilla...

2025-04-02T03:37:57.192Z:
Received chunk:  extract...

2025-04-02T03:37:57.217Z:
Received chunk: .
...

2025-04-02T03:37:57.243Z:
Received chunk: 8...

2025-04-02T03:37:57.273Z:
Received chunk: ....

2025-04-02T03:37:57.295Z:
Received chunk:  Pour...

2025-04-02T03:37:57.323Z:
Received chunk:  the...

2025-04-02T03:37:57.349Z:
Received chunk:  chees...

2025-04-02T03:37:57.375Z:
Received chunk: ecake...

2025-04-02T03:37:57.401Z:
Received chunk:  batter...

2025-04-02T03:37:57.425Z:
Received chunk:  into...

2025-04-02T03:37:57.450Z:
Received chunk:  the...

2025-04-02T03:37:57.476Z:
Received chunk:  prepared...

2025-04-02T03:37:57.501Z:
Received chunk:  pan...

2025-04-02T03:37:57.525Z:
Received chunk:  over...

2025-04-02T03:37:57.550Z:
Received chunk: 
> ...

2025-04-02T03:38:18.030Z:
Unloading model...

2025-04-02T03:38:18.030Z:
Terminating running process

2025-04-02T03:38:18.030Z:
Model unloaded successfully

2025-04-02T03:38:18.039Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:38:18.039Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:38:18.040Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:38:18.040Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:38:18.054Z:
Process exited with code null

2025-04-02T03:38:18.055Z:
Process failed, falling back to simulation

2025-04-02T03:38:18.055Z:
Generating simulated response

2025-04-02T03:38:18.098Z:
Sent simulated chunk: I'm

2025-04-02T03:38:18.192Z:
Sent simulated chunk: a

2025-04-02T03:38:18.270Z:
Sent simulated chunk: locally

2025-04-02T03:38:18.316Z:
Sent simulated chunk: hosted

2025-04-02T03:38:18.394Z:
Sent simulated chunk: AI

2025-04-02T03:38:18.439Z:
Sent simulated chunk: assistant

2025-04-02T03:38:18.519Z:
Sent simulated chunk: running

2025-04-02T03:38:18.627Z:
Sent simulated chunk: directly

2025-04-02T03:38:18.721Z:
Sent simulated chunk: on

2025-04-02T03:38:18.798Z:
Sent simulated chunk: your

2025-04-02T03:38:18.843Z:
Sent simulated chunk: device.

2025-04-02T03:38:18.923Z:
Sent simulated chunk: I

2025-04-02T03:38:18.984Z:
Sent simulated chunk: process

2025-04-02T03:38:19.015Z:
Sent simulated chunk: information

2025-04-02T03:38:19.077Z:
Sent simulated chunk: using

2025-04-02T03:38:19.155Z:
Sent simulated chunk: the

2025-04-02T03:38:19.249Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:38:19.312Z:
Sent simulated chunk: model

2025-04-02T03:38:19.389Z:
Sent simulated chunk: loaded

2025-04-02T03:38:19.482Z:
Sent simulated chunk: in

2025-04-02T03:38:19.529Z:
Sent simulated chunk: the

2025-04-02T03:38:19.621Z:
Sent simulated chunk: LM

2025-04-02T03:38:19.716Z:
Sent simulated chunk: Terminal

2025-04-02T03:38:19.778Z:
Sent simulated chunk: application.

2025-04-02T03:38:19.825Z:
Sent simulated chunk: I've

2025-04-02T03:38:19.871Z:
Sent simulated chunk: received

2025-04-02T03:38:19.917Z:
Sent simulated chunk: your

2025-04-02T03:38:20.026Z:
Sent simulated chunk: message:

2025-04-02T03:38:20.122Z:
Sent simulated chunk: "tell

2025-04-02T03:38:20.184Z:
Sent simulated chunk: me

2025-04-02T03:38:20.231Z:
Sent simulated chunk: a

2025-04-02T03:38:20.293Z:
Sent simulated chunk: detailing

2025-04-02T03:38:20.370Z:
Sent simulated chunk: cooking

2025-04-02T03:38:20.432Z:
Sent simulated chunk: recipe

2025-04-02T03:38:20.510Z:
Sent simulated chunk: for

2025-04-02T03:38:20.556Z:
Sent simulated chunk: sweets

2025-04-02T03:38:20.649Z:
Sent simulated chunk: like

2025-04-02T03:38:20.727Z:
Sent simulated chunk: baked

2025-04-02T03:38:20.819Z:
Sent simulated chunk: goods."

2025-04-02T03:38:20.866Z:
Sent simulated chunk: and

2025-04-02T03:38:20.912Z:
Sent simulated chunk: am

2025-04-02T03:38:20.974Z:
Sent simulated chunk: responding

2025-04-02T03:38:21.051Z:
Sent simulated chunk: without

2025-04-02T03:38:21.129Z:
Sent simulated chunk: requiring

2025-04-02T03:38:21.176Z:
Sent simulated chunk: internet

2025-04-02T03:38:21.223Z:
Sent simulated chunk: access.

(This

2025-04-02T03:38:21.317Z:
Sent simulated chunk: response

2025-04-02T03:38:21.379Z:
Sent simulated chunk: was

2025-04-02T03:38:21.441Z:
Sent simulated chunk: generated

2025-04-02T03:38:21.488Z:
Sent simulated chunk: using

2025-04-02T03:38:21.535Z:
Sent simulated chunk: a

2025-04-02T03:38:21.581Z:
Sent simulated chunk: simulated

2025-04-02T03:38:21.675Z:
Sent simulated chunk: version

2025-04-02T03:38:21.770Z:
Sent simulated chunk: of

2025-04-02T03:38:21.863Z:
Sent simulated chunk: the

2025-04-02T03:38:21.940Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:38:22.033Z:
Sent simulated chunk: model)

2025-04-02T03:38:22.034Z:
Simulated response generation completed

2025-04-02T03:39:55.471Z:
Unloading model...

2025-04-02T03:39:55.471Z:
Model unloaded successfully

2025-04-02T03:39:55.471Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:39:55.472Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:39:55.472Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:39:55.472Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:39:56.981Z:
Unloading model...

2025-04-02T03:39:56.981Z:
Model unloaded successfully

2025-04-02T03:39:56.981Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:39:56.981Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:39:56.982Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:39:56.982Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:40:05.406Z:
Generate response called with message: write me a story ...

2025-04-02T03:40:05.406Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: write me a story 
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:40:05.427Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:40:05.453Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:40:05.471Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:40:05.479Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:40:05.519Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:40:05.601Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:40:05.615Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1


2025-04-02T03:40:05.616Z:
Process stderr: print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:40:05.662Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
.....

2025-04-02T03:40:05.662Z:
Process stderr: ..................................................

2025-04-02T03:40:05.663Z:
Process stderr: .......


2025-04-02T03:40:05.664Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:40:05.665Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:40:05.676Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:40:05.681Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:40:05.822Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?


2025-04-02T03:40:05.822Z:
Process stderr: main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>



system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:40:05.823Z:
Process stderr: sampler seed: 1941352795
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:40:05.896Z:
Received chunk: I...

2025-04-02T03:40:05.920Z:
Received chunk: 'll...

2025-04-02T03:40:05.944Z:
Received chunk:  start...

2025-04-02T03:40:05.969Z:
Received chunk:  writing...

2025-04-02T03:40:05.993Z:
Received chunk:  the...

2025-04-02T03:40:06.018Z:
Received chunk:  story...

2025-04-02T03:40:06.041Z:
Received chunk: ....

2025-04-02T03:40:06.066Z:
Received chunk:  Here...

2025-04-02T03:40:06.089Z:
Received chunk: 's...

2025-04-02T03:40:06.115Z:
Received chunk:  the...

2025-04-02T03:40:06.140Z:
Received chunk:  first...

2025-04-02T03:40:06.164Z:
Received chunk:  part...

2025-04-02T03:40:06.188Z:
Received chunk: :

...

2025-04-02T03:40:06.211Z:
Received chunk: In...

2025-04-02T03:40:06.235Z:
Received chunk:  the...

2025-04-02T03:40:06.263Z:
Received chunk:  small...

2025-04-02T03:40:06.284Z:
Received chunk:  town...

2025-04-02T03:40:06.310Z:
Received chunk:  of...

2025-04-02T03:40:06.335Z:
Received chunk:  Willow...

2025-04-02T03:40:06.359Z:
Received chunk:  Creek...

2025-04-02T03:40:06.382Z:
Received chunk: ,...

2025-04-02T03:40:06.407Z:
Received chunk:  nestled...

2025-04-02T03:40:06.430Z:
Received chunk:  in...

2025-04-02T03:40:06.454Z:
Received chunk:  the...

2025-04-02T03:40:06.478Z:
Received chunk:  heart...

2025-04-02T03:40:06.501Z:
Received chunk:  of...

2025-04-02T03:40:06.525Z:
Received chunk:  the...

2025-04-02T03:40:06.548Z:
Received chunk:  Appalachian...

2025-04-02T03:40:06.571Z:
Received chunk:  Mountains...

2025-04-02T03:40:06.595Z:
Received chunk: ,...

2025-04-02T03:40:06.618Z:
Received chunk:  a...

2025-04-02T03:40:06.642Z:
Received chunk:  mysterious...

2025-04-02T03:40:06.665Z:
Received chunk:  stranger...

2025-04-02T03:40:06.689Z:
Received chunk:  arrived...

2025-04-02T03:40:06.712Z:
Received chunk:  on...

2025-04-02T03:40:06.736Z:
Received chunk:  a...

2025-04-02T03:40:06.759Z:
Received chunk:  cold...

2025-04-02T03:40:06.783Z:
Received chunk:  and...

2025-04-02T03:40:06.806Z:
Received chunk:  storm...

2025-04-02T03:40:06.830Z:
Received chunk: y...

2025-04-02T03:40:06.854Z:
Received chunk:  night...

2025-04-02T03:40:06.879Z:
Received chunk: ....

2025-04-02T03:40:06.902Z:
Received chunk:  The...

2025-04-02T03:40:06.926Z:
Received chunk:  stranger...

2025-04-02T03:40:06.951Z:
Received chunk:  was...

2025-04-02T03:40:06.975Z:
Received chunk:  a...

2025-04-02T03:40:06.999Z:
Received chunk:  woman...

2025-04-02T03:40:07.023Z:
Received chunk:  with...

2025-04-02T03:40:07.047Z:
Received chunk:  piercing...

2025-04-02T03:40:07.071Z:
Received chunk:  green...

2025-04-02T03:40:07.097Z:
Received chunk:  eyes...

2025-04-02T03:40:07.124Z:
Received chunk:  and...

2025-04-02T03:40:07.146Z:
Received chunk:  long...

2025-04-02T03:40:07.171Z:
Received chunk: ,...

2025-04-02T03:40:07.195Z:
Received chunk:  curly...

2025-04-02T03:40:07.219Z:
Received chunk:  brown...

2025-04-02T03:40:07.244Z:
Received chunk:  hair...

2025-04-02T03:40:07.268Z:
Received chunk: ....

2025-04-02T03:40:07.293Z:
Received chunk:  She...

2025-04-02T03:40:07.318Z:
Received chunk:  wore...

2025-04-02T03:40:07.343Z:
Received chunk:  a...

2025-04-02T03:40:07.367Z:
Received chunk:  worn...

2025-04-02T03:40:07.391Z:
Received chunk:  leather...

2025-04-02T03:40:07.415Z:
Received chunk:  coat...

2025-04-02T03:40:07.439Z:
Received chunk:  and...

2025-04-02T03:40:07.463Z:
Received chunk:  carried...

2025-04-02T03:40:07.486Z:
Received chunk:  a...

2025-04-02T03:40:07.509Z:
Received chunk:  large...

2025-04-02T03:40:07.533Z:
Received chunk:  backpack...

2025-04-02T03:40:07.557Z:
Received chunk:  sl...

2025-04-02T03:40:07.581Z:
Received chunk: ung...

2025-04-02T03:40:07.604Z:
Received chunk:  over...

2025-04-02T03:40:07.628Z:
Received chunk:  her...

2025-04-02T03:40:07.651Z:
Received chunk:  shoulder...

2025-04-02T03:40:07.674Z:
Received chunk: .

...

2025-04-02T03:40:07.698Z:
Received chunk: No...

2025-04-02T03:40:07.721Z:
Received chunk:  one...

2025-04-02T03:40:07.745Z:
Received chunk:  knew...

2025-04-02T03:40:07.768Z:
Received chunk:  much...

2025-04-02T03:40:07.791Z:
Received chunk:  about...

2025-04-02T03:40:07.815Z:
Received chunk:  her...

2025-04-02T03:40:07.839Z:
Received chunk:  past...

2025-04-02T03:40:07.864Z:
Received chunk: ,...

2025-04-02T03:40:07.888Z:
Received chunk:  except...

2025-04-02T03:40:07.913Z:
Received chunk:  that...

2025-04-02T03:40:07.937Z:
Received chunk:  she...

2025-04-02T03:40:07.961Z:
Received chunk:  had...

2025-04-02T03:40:07.985Z:
Received chunk:  been...

2025-04-02T03:40:08.010Z:
Received chunk:  traveling...

2025-04-02T03:40:08.035Z:
Received chunk:  for...

2025-04-02T03:40:08.060Z:
Received chunk:  weeks...

2025-04-02T03:40:08.085Z:
Received chunk: ,...

2025-04-02T03:40:08.110Z:
Received chunk:  seeking...

2025-04-02T03:40:08.135Z:
Received chunk:  refuge...

2025-04-02T03:40:08.159Z:
Received chunk:  from...

2025-04-02T03:40:08.184Z:
Received chunk:  the...

2025-04-02T03:40:08.208Z:
Received chunk:  unknown...

2025-04-02T03:40:08.232Z:
Received chunk:  dangers...

2025-04-02T03:40:08.256Z:
Received chunk:  that...

2025-04-02T03:40:08.282Z:
Received chunk:  lur...

2025-04-02T03:40:08.307Z:
Received chunk: ked...

2025-04-02T03:40:08.332Z:
Received chunk:  in...

2025-04-02T03:40:08.357Z:
Received chunk:  the...

2025-04-02T03:40:08.382Z:
Received chunk:  shadows...

2025-04-02T03:40:08.405Z:
Received chunk:  of...

2025-04-02T03:40:08.429Z:
Received chunk:  the...

2025-04-02T03:40:08.453Z:
Received chunk:  world...

2025-04-02T03:40:08.476Z:
Received chunk:  beyond...

2025-04-02T03:40:08.499Z:
Received chunk:  her...

2025-04-02T03:40:08.523Z:
Received chunk:  small...

2025-04-02T03:40:08.546Z:
Received chunk:  town...

2025-04-02T03:40:08.570Z:
Received chunk: ....

2025-04-02T03:40:08.593Z:
Received chunk:  The...

2025-04-02T03:40:08.617Z:
Received chunk:  towns...

2025-04-02T03:40:08.641Z:
Received chunk: folk...

2025-04-02T03:40:08.664Z:
Received chunk:  whispered...

2025-04-02T03:40:08.688Z:
Received chunk:  among...

2025-04-02T03:40:08.711Z:
Received chunk:  themselves...

2025-04-02T03:40:08.735Z:
Received chunk:  about...

2025-04-02T03:40:08.759Z:
Received chunk:  the...

2025-04-02T03:40:08.782Z:
Received chunk:  stranger...

2025-04-02T03:40:08.806Z:
Received chunk: 's...

2025-04-02T03:40:08.831Z:
Received chunk:  true...

2025-04-02T03:40:08.855Z:
Received chunk:  identity...

2025-04-02T03:40:08.879Z:
Received chunk:  and...

2025-04-02T03:40:08.904Z:
Received chunk:  intentions...

2025-04-02T03:40:08.928Z:
Received chunk: ,...

2025-04-02T03:40:08.953Z:
Received chunk:  but...

2025-04-02T03:40:08.977Z:
Received chunk:  no...

2025-04-02T03:40:09.001Z:
Received chunk:  one...

2025-04-02T03:40:09.025Z:
Received chunk:  dared...

2025-04-02T03:40:09.049Z:
Received chunk:  to...

2025-04-02T03:40:09.073Z:
Received chunk:  ask...

2025-04-02T03:40:09.099Z:
Received chunk:  her...

2025-04-02T03:40:09.123Z:
Received chunk:  directly...

2025-04-02T03:40:09.146Z:
Received chunk: .

...

2025-04-02T03:40:09.171Z:
Received chunk: As...

2025-04-02T03:40:09.195Z:
Received chunk:  the...

2025-04-02T03:40:09.219Z:
Received chunk:  storm...

2025-04-02T03:40:09.243Z:
Received chunk:  r...

2025-04-02T03:40:09.268Z:
Received chunk: aged...

2025-04-02T03:40:09.293Z:
Received chunk:  on...

2025-04-02T03:40:09.319Z:
Received chunk:  outside...

2025-04-02T03:40:09.344Z:
Received chunk: ,...

2025-04-02T03:40:09.368Z:
Received chunk:  the...

2025-04-02T03:40:09.392Z:
Received chunk:  stranger...

2025-04-02T03:40:09.416Z:
Received chunk:  stumbled...

2025-04-02T03:40:09.440Z:
Received chunk:  into...

2025-04-02T03:40:09.463Z:
Received chunk:  the...

2025-04-02T03:40:09.487Z:
Received chunk:  local...

2025-04-02T03:40:09.510Z:
Received chunk:  diner...

2025-04-02T03:40:09.534Z:
Received chunk: ,...

2025-04-02T03:40:09.558Z:
Received chunk:  shaking...

2025-04-02T03:40:09.581Z:
Received chunk:  the...

2025-04-02T03:40:09.605Z:
Received chunk:  rain...

2025-04-02T03:40:09.629Z:
Received chunk:  off...

2025-04-02T03:40:09.652Z:
Received chunk:  her...

2025-04-02T03:40:09.676Z:
Received chunk:  coat...

2025-04-02T03:40:09.700Z:
Received chunk:  and...

2025-04-02T03:40:09.723Z:
Received chunk:  hat...

2025-04-02T03:40:09.747Z:
Received chunk: ....

2025-04-02T03:40:09.771Z:
Received chunk:  She...

2025-04-02T03:40:09.794Z:
Received chunk:  slid...

2025-04-02T03:40:09.817Z:
Received chunk:  onto...

2025-04-02T03:40:09.842Z:
Received chunk:  a...

2025-04-02T03:40:09.866Z:
Received chunk:  stool...

2025-04-02T03:40:09.890Z:
Received chunk:  at...

2025-04-02T03:40:09.913Z:
Received chunk:  the...

2025-04-02T03:40:09.937Z:
Received chunk:  counter...

2025-04-02T03:40:09.962Z:
Received chunk:  and...

2025-04-02T03:40:09.985Z:
Received chunk:  ordered...

2025-04-02T03:40:10.009Z:
Received chunk:  a...

2025-04-02T03:40:10.033Z:
Received chunk:  cup...

2025-04-02T03:40:10.057Z:
Received chunk:  of...

2025-04-02T03:40:10.081Z:
Received chunk:  coffee...

2025-04-02T03:40:10.107Z:
Received chunk: ,...

2025-04-02T03:40:10.132Z:
Received chunk:  her...

2025-04-02T03:40:10.158Z:
Received chunk:  eyes...

2025-04-02T03:40:10.180Z:
Received chunk:  scanning...

2025-04-02T03:40:10.204Z:
Received chunk:  the...

2025-04-02T03:40:10.228Z:
Received chunk:  diner...

2025-04-02T03:40:10.252Z:
Received chunk:  with...

2025-04-02T03:40:10.277Z:
Received chunk:  an...

2025-04-02T03:40:10.302Z:
Received chunk:  air...

2025-04-02T03:40:10.328Z:
Received chunk:  of...

2025-04-02T03:40:10.353Z:
Received chunk:  war...

2025-04-02T03:40:10.378Z:
Received chunk: iness...

2025-04-02T03:40:10.403Z:
Received chunk: .

...

2025-04-02T03:40:10.427Z:
Received chunk: The...

2025-04-02T03:40:10.451Z:
Received chunk:  waitress...

2025-04-02T03:40:10.475Z:
Received chunk: ,...

2025-04-02T03:40:10.499Z:
Received chunk:  a...

2025-04-02T03:40:10.523Z:
Received chunk:  friendly...

2025-04-02T03:40:10.546Z:
Received chunk:  woman...

2025-04-02T03:40:10.571Z:
Received chunk:  named...

2025-04-02T03:40:10.594Z:
Received chunk:  Maggie...

2025-04-02T03:40:10.618Z:
Received chunk: ,...

2025-04-02T03:40:10.641Z:
Received chunk:  noticed...

2025-04-02T03:40:10.665Z:
Received chunk:  the...

2025-04-02T03:40:10.689Z:
Received chunk:  stranger...

2025-04-02T03:40:10.712Z:
Received chunk: 's...

2025-04-02T03:40:10.736Z:
Received chunk:  gaze...

2025-04-02T03:40:10.760Z:
Received chunk:  lingering...

2025-04-02T03:40:10.783Z:
Received chunk:  on...

2025-04-02T03:40:10.806Z:
Received chunk:  the...

2025-04-02T03:40:10.830Z:
Received chunk:  other...

2025-04-02T03:40:10.855Z:
Received chunk:  patrons...

2025-04-02T03:40:10.879Z:
Received chunk:  and...

2025-04-02T03:40:10.903Z:
Received chunk:  asked...

2025-04-02T03:40:10.927Z:
Received chunk:  if...

2025-04-02T03:40:10.951Z:
Received chunk:  everything...

2025-04-02T03:40:10.976Z:
Received chunk:  was...

2025-04-02T03:40:11.000Z:
Received chunk:  okay...

2025-04-02T03:40:11.024Z:
Received chunk: ....

2025-04-02T03:40:11.048Z:
Received chunk:  The...

2025-04-02T03:40:11.072Z:
Received chunk:  stranger...

2025-04-02T03:40:11.097Z:
Received chunk:  hes...

2025-04-02T03:40:11.122Z:
Received chunk: itated...

2025-04-02T03:40:11.146Z:
Received chunk:  for...

2025-04-02T03:40:11.171Z:
Received chunk:  a...

2025-04-02T03:40:11.196Z:
Received chunk:  moment...

2025-04-02T03:40:11.220Z:
Received chunk:  before...

2025-04-02T03:40:11.245Z:
Received chunk:  responding...

2025-04-02T03:40:11.269Z:
Received chunk:  in...

2025-04-02T03:40:11.294Z:
Received chunk:  a...

2025-04-02T03:40:11.320Z:
Received chunk:  low...

2025-04-02T03:40:11.345Z:
Received chunk: ,...

2025-04-02T03:40:11.370Z:
Received chunk:  hus...

2025-04-02T03:40:11.395Z:
Received chunk: ky...

2025-04-02T03:40:11.420Z:
Received chunk:  voice...

2025-04-02T03:40:11.444Z:
Received chunk: ,...

2025-04-02T03:40:11.467Z:
Received chunk:  "...

2025-04-02T03:40:11.491Z:
Received chunk: Just...

2025-04-02T03:40:11.515Z:
Received chunk:  lost...

2025-04-02T03:40:11.538Z:
Received chunk:  my...

2025-04-02T03:40:11.561Z:
Received chunk:  way...

2025-04-02T03:40:11.585Z:
Received chunk: ,...

2025-04-02T03:40:11.608Z:
Received chunk:  I...

2025-04-02T03:40:11.633Z:
Received chunk:  guess...

2025-04-02T03:40:11.658Z:
Received chunk: ."

...

2025-04-02T03:40:11.681Z:
Received chunk: M...

2025-04-02T03:40:11.705Z:
Received chunk: agg...

2025-04-02T03:40:11.728Z:
Received chunk: ie...

2025-04-02T03:40:11.752Z:
Received chunk:  nodded...

2025-04-02T03:40:11.775Z:
Received chunk:  sympath...

2025-04-02T03:40:11.799Z:
Received chunk: etically...

2025-04-02T03:40:11.823Z:
Received chunk:  and...

2025-04-02T03:40:11.846Z:
Received chunk:  continued...

2025-04-02T03:40:11.871Z:
Received chunk:  to...

2025-04-02T03:40:11.895Z:
Received chunk:  serve...

2025-04-02T03:40:11.920Z:
Received chunk:  her...

2025-04-02T03:40:11.944Z:
Received chunk:  coffee...

2025-04-02T03:40:11.968Z:
Received chunk: ,...

2025-04-02T03:40:11.992Z:
Received chunk:  but...

2025-04-02T03:40:12.016Z:
Received chunk:  as...

2025-04-02T03:40:12.040Z:
Received chunk:  she...

2025-04-02T03:40:12.065Z:
Received chunk:  watched...

2025-04-02T03:40:12.089Z:
Received chunk:  the...

2025-04-02T03:40:12.115Z:
Received chunk:  stranger...

2025-04-02T03:40:12.140Z:
Received chunk:  sit...

2025-04-02T03:40:12.173Z:
Received chunk:  there...

2025-04-02T03:40:12.190Z:
Received chunk: ,...

2025-04-02T03:40:12.215Z:
Received chunk:  s...

2025-04-02T03:40:12.239Z:
Received chunk: ipping...

2025-04-02T03:40:12.263Z:
Received chunk:  her...

2025-04-02T03:40:12.287Z:
Received chunk:  drink...

2025-04-02T03:40:12.312Z:
Received chunk:  and...

2025-04-02T03:40:12.337Z:
Received chunk:  staring...

2025-04-02T03:40:12.362Z:
Received chunk:  into...

2025-04-02T03:40:12.387Z:
Received chunk:  space...

2025-04-02T03:40:12.412Z:
Received chunk: ,...

2025-04-02T03:40:12.436Z:
Received chunk:  Maggie...

2025-04-02T03:40:12.460Z:
Received chunk:  couldn...

2025-04-02T03:40:12.484Z:
Received chunk: 't...

2025-04-02T03:40:12.507Z:
Received chunk:  help...

2025-04-02T03:40:12.531Z:
Received chunk:  but...

2025-04-02T03:40:12.554Z:
Received chunk:  wonder...

2025-04-02T03:40:12.578Z:
Received chunk:  what...

2025-04-02T03:40:12.602Z:
Received chunk:  secrets...

2025-04-02T03:40:12.626Z:
Received chunk:  lay...

2025-04-02T03:40:12.649Z:
Received chunk:  behind...

2025-04-02T03:40:12.673Z:
Received chunk:  those...

2025-04-02T03:40:12.697Z:
Received chunk:  piercing...

2025-04-02T03:40:12.721Z:
Received chunk:  green...

2025-04-02T03:40:12.745Z:
Received chunk:  eyes...

2025-04-02T03:40:12.769Z:
Received chunk: .

...

2025-04-02T03:40:12.792Z:
Received chunk: And...

2025-04-02T03:40:12.817Z:
Received chunk:  that...

2025-04-02T03:40:12.841Z:
Received chunk: 's...

2025-04-02T03:40:12.864Z:
Received chunk:  where...

2025-04-02T03:40:12.889Z:
Received chunk:  our...

2025-04-02T03:40:12.913Z:
Received chunk:  story...

2025-04-02T03:40:12.938Z:
Received chunk:  begins...

2025-04-02T03:40:12.962Z:
Received chunk: ......

2025-04-02T03:40:12.987Z:
Received chunk: 

> ...

2025-04-02T03:40:14.829Z:
Unloading model...

2025-04-02T03:40:14.830Z:
Terminating running process

2025-04-02T03:40:14.830Z:
Model unloaded successfully

2025-04-02T03:40:14.831Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:40:14.831Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:40:14.831Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:40:14.832Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:40:14.853Z:
Process exited with code null

2025-04-02T03:40:14.854Z:
Process failed, falling back to simulation

2025-04-02T03:40:14.854Z:
Generating simulated response

2025-04-02T03:40:14.913Z:
Sent simulated chunk: I'm

2025-04-02T03:40:14.975Z:
Sent simulated chunk: a

2025-04-02T03:40:15.069Z:
Sent simulated chunk: locally

2025-04-02T03:40:15.131Z:
Sent simulated chunk: hosted

2025-04-02T03:40:15.178Z:
Sent simulated chunk: AI

2025-04-02T03:40:15.239Z:
Sent simulated chunk: assistant

2025-04-02T03:40:15.333Z:
Sent simulated chunk: running

2025-04-02T03:40:15.425Z:
Sent simulated chunk: directly

2025-04-02T03:40:15.504Z:
Sent simulated chunk: on

2025-04-02T03:40:15.581Z:
Sent simulated chunk: your

2025-04-02T03:40:15.629Z:
Sent simulated chunk: device.

2025-04-02T03:40:15.706Z:
Sent simulated chunk: I

2025-04-02T03:40:15.768Z:
Sent simulated chunk: process

2025-04-02T03:40:15.813Z:
Sent simulated chunk: information

2025-04-02T03:40:15.876Z:
Sent simulated chunk: using

2025-04-02T03:40:15.954Z:
Sent simulated chunk: the

2025-04-02T03:40:16.048Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:40:16.109Z:
Sent simulated chunk: model

2025-04-02T03:40:16.157Z:
Sent simulated chunk: loaded

2025-04-02T03:40:16.234Z:
Sent simulated chunk: in

2025-04-02T03:40:16.281Z:
Sent simulated chunk: the

2025-04-02T03:40:16.359Z:
Sent simulated chunk: LM

2025-04-02T03:40:16.389Z:
Sent simulated chunk: Terminal

2025-04-02T03:40:16.466Z:
Sent simulated chunk: application.

2025-04-02T03:40:16.544Z:
Sent simulated chunk: I've

2025-04-02T03:40:16.621Z:
Sent simulated chunk: received

2025-04-02T03:40:16.729Z:
Sent simulated chunk: your

2025-04-02T03:40:16.793Z:
Sent simulated chunk: message:

2025-04-02T03:40:16.892Z:
Sent simulated chunk: "write

2025-04-02T03:40:16.964Z:
Sent simulated chunk: me

2025-04-02T03:40:17.011Z:
Sent simulated chunk: a

2025-04-02T03:40:17.059Z:
Sent simulated chunk: story

2025-04-02T03:40:17.105Z:
Sent simulated chunk: "

2025-04-02T03:40:17.200Z:
Sent simulated chunk: and

2025-04-02T03:40:17.293Z:
Sent simulated chunk: am

2025-04-02T03:40:17.339Z:
Sent simulated chunk: responding

2025-04-02T03:40:17.384Z:
Sent simulated chunk: without

2025-04-02T03:40:17.462Z:
Sent simulated chunk: requiring

2025-04-02T03:40:17.525Z:
Sent simulated chunk: internet

2025-04-02T03:40:17.557Z:
Sent simulated chunk: access.

(This

2025-04-02T03:40:17.635Z:
Sent simulated chunk: response

2025-04-02T03:40:17.697Z:
Sent simulated chunk: was

2025-04-02T03:40:17.775Z:
Sent simulated chunk: generated

2025-04-02T03:40:17.837Z:
Sent simulated chunk: using

2025-04-02T03:40:17.930Z:
Sent simulated chunk: a

2025-04-02T03:40:18.007Z:
Sent simulated chunk: simulated

2025-04-02T03:40:18.099Z:
Sent simulated chunk: version

2025-04-02T03:40:18.161Z:
Sent simulated chunk: of

2025-04-02T03:40:18.254Z:
Sent simulated chunk: the

2025-04-02T03:40:18.332Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:40:18.426Z:
Sent simulated chunk: model)

2025-04-02T03:40:18.426Z:
Simulated response generation completed

2025-04-02T03:47:00.390Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:00.391Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:47:00.391Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:00.392Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:47:00.793Z:
Unloading model...

2025-04-02T03:47:00.793Z:
Model unloaded successfully

2025-04-02T03:47:00.794Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:00.794Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:47:00.794Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:00.794Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:47:10.621Z:
Generate response called with message: give me a detailed recipe for chocolate chip cookies...

2025-04-02T03:47:10.622Z:
Spawning process: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe -m C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf --prompt USER: give me a detailed recipe for chocolate chip cookies
ASSISTANT: --n-predict 1024 --temp 0.7 --repeat-penalty 1.1 -c 2048 --no-display-prompt

2025-04-02T03:47:10.758Z:
Process stderr: build: 5022 (f423981a) with MSVC 19.43.34808.0 for x64
main: llama backend init
main: load the model and apply lora adapter, if any


2025-04-02T03:47:10.786Z:
Process stderr: llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe


2025-04-02T03:47:10.805Z:
Process stderr: llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...


2025-04-02T03:47:10.813Z:
Process stderr: llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...


2025-04-02T03:47:10.853Z:
Process stderr: llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 


2025-04-02T03:47:10.937Z:
Process stderr: load: special tokens cache size = 256


2025-04-02T03:47:10.951Z:
Process stderr: load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512


2025-04-02T03:47:10.952Z:
Process stderr: print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)


2025-04-02T03:47:10.999Z:
Process stderr: load_tensors: offloading 0 repeating layers to GPU
load_tensors: offloaded 0/17 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB
...

2025-04-02T03:47:10.999Z:
Process stderr: ....................................................

2025-04-02T03:47:11.000Z:
Process stderr: .......


2025-04-02T03:47:11.001Z:
Process stderr: llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized


2025-04-02T03:47:11.001Z:
Process stderr: llama_context:        CPU  output buffer size =     0.49 MiB
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1


2025-04-02T03:47:11.012Z:
Process stderr: init:        CPU KV buffer size =    64.00 MiB
llama_context: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB


2025-04-02T03:47:11.017Z:
Process stderr: llama_context:        CPU compute buffer size =   254.50 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)


2025-04-02T03:47:11.167Z:
Process stderr: main: llama threadpool init, n_threads = 24
main: chat template is available, enabling conversation mode (disable it with -no-cnv)
*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?
main: chat template example:
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>

How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>





2025-04-02T03:47:11.167Z:
Process stderr: system_info: n_threads = 24 (n_threads_batch = 24) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 

main: interactive mode on.


2025-04-02T03:47:11.168Z:
Process stderr: sampler seed: 1805351502
sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
generate: n_ctx = 2048, n_batch = 2048, n_predict = 1024, n_keep = 1

== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to the AI.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.
 - Not using system message. To change it, set a different value via -sys PROMPT



2025-04-02T03:47:11.254Z:
Received chunk: Who...

2025-04-02T03:47:11.279Z:
Received chunk:  can...

2025-04-02T03:47:11.304Z:
Received chunk:  resist...

2025-04-02T03:47:11.330Z:
Received chunk:  the...

2025-04-02T03:47:11.354Z:
Received chunk:  allure...

2025-04-02T03:47:11.379Z:
Received chunk:  of...

2025-04-02T03:47:11.404Z:
Received chunk:  a...

2025-04-02T03:47:11.430Z:
Received chunk:  classic...

2025-04-02T03:47:11.455Z:
Received chunk:  chocolate...

2025-04-02T03:47:11.479Z:
Received chunk:  chip...

2025-04-02T03:47:11.504Z:
Received chunk:  cookie...

2025-04-02T03:47:11.529Z:
Received chunk: ?...

2025-04-02T03:47:11.554Z:
Received chunk:  Here...

2025-04-02T03:47:11.579Z:
Received chunk: 's...

2025-04-02T03:47:11.603Z:
Received chunk:  a...

2025-04-02T03:47:11.628Z:
Received chunk:  detailed...

2025-04-02T03:47:11.653Z:
Received chunk:  recipe...

2025-04-02T03:47:11.678Z:
Received chunk:  that...

2025-04-02T03:47:11.703Z:
Received chunk:  yields...

2025-04-02T03:47:11.728Z:
Received chunk:  soft...

2025-04-02T03:47:11.752Z:
Received chunk: ,...

2025-04-02T03:47:11.777Z:
Received chunk:  chew...

2025-04-02T03:47:11.803Z:
Received chunk: y...

2025-04-02T03:47:11.828Z:
Received chunk: ,...

2025-04-02T03:47:11.854Z:
Received chunk:  and...

2025-04-02T03:47:11.883Z:
Received chunk:  utterly...

2025-04-02T03:47:11.907Z:
Received chunk:  delicious...

2025-04-02T03:47:11.932Z:
Received chunk:  cookies...

2025-04-02T03:47:11.958Z:
Received chunk: :

...

2025-04-02T03:47:11.982Z:
Received chunk: **...

2025-04-02T03:47:12.007Z:
Received chunk: Chocolate...

2025-04-02T03:47:12.033Z:
Received chunk:  Chip...

2025-04-02T03:47:12.058Z:
Received chunk:  Cookies...

2025-04-02T03:47:12.082Z:
Received chunk:  Recipe...

2025-04-02T03:47:12.106Z:
Received chunk: **

...

2025-04-02T03:47:12.131Z:
Received chunk: **...

2025-04-02T03:47:12.155Z:
Received chunk: Ingredients...

2025-04-02T03:47:12.179Z:
Received chunk: :...

2025-04-02T03:47:12.204Z:
Received chunk: **

...

2025-04-02T03:47:12.229Z:
Received chunk: *...

2025-04-02T03:47:12.253Z:
Received chunk:  ...

2025-04-02T03:47:12.277Z:
Received chunk: 2...

2025-04-02T03:47:12.301Z:
Received chunk:  ...

2025-04-02T03:47:12.327Z:
Received chunk: 1...

2025-04-02T03:47:12.351Z:
Received chunk: /...

2025-04-02T03:47:12.376Z:
Received chunk: 4...

2025-04-02T03:47:12.400Z:
Received chunk:  cups...

2025-04-02T03:47:12.425Z:
Received chunk:  all...

2025-04-02T03:47:12.449Z:
Received chunk: -purpose...

2025-04-02T03:47:12.474Z:
Received chunk:  flour...

2025-04-02T03:47:12.499Z:
Received chunk:  (...

2025-04-02T03:47:12.524Z:
Received chunk: bread...

2025-04-02T03:47:12.549Z:
Received chunk:  flour...

2025-04-02T03:47:12.573Z:
Received chunk:  or...

2025-04-02T03:47:12.598Z:
Received chunk:  cake...

2025-04-02T03:47:12.622Z:
Received chunk:  flour...

2025-04-02T03:47:12.646Z:
Received chunk:  works...

2025-04-02T03:47:12.680Z:
Received chunk:  too...

2025-04-02T03:47:12.697Z:
Received chunk: )
...

2025-04-02T03:47:12.722Z:
Received chunk: *...

2025-04-02T03:47:12.747Z:
Received chunk:  ...

2025-04-02T03:47:12.772Z:
Received chunk: 1...

2025-04-02T03:47:12.796Z:
Received chunk:  tsp...

2025-04-02T03:47:12.824Z:
Received chunk:  baking...

2025-04-02T03:47:12.847Z:
Received chunk:  soda...

2025-04-02T03:47:12.872Z:
Received chunk: 
...

2025-04-02T03:47:12.897Z:
Received chunk: *...

2025-04-02T03:47:12.922Z:
Received chunk:  ...

2025-04-02T03:47:12.947Z:
Received chunk: 1...

2025-04-02T03:47:12.971Z:
Received chunk:  tsp...

2025-04-02T03:47:12.996Z:
Received chunk:  salt...

2025-04-02T03:47:13.021Z:
Received chunk: 
...

2025-04-02T03:47:13.046Z:
Received chunk: *...

2025-04-02T03:47:13.070Z:
Received chunk:  ...

2025-04-02T03:47:13.094Z:
Received chunk: 1...

2025-04-02T03:47:13.118Z:
Received chunk:  cup...

2025-04-02T03:47:13.142Z:
Received chunk:  (...

2025-04-02T03:47:13.167Z:
Received chunk: 2...

2025-04-02T03:47:13.191Z:
Received chunk:  sticks...

2025-04-02T03:47:13.216Z:
Received chunk: )...

2025-04-02T03:47:13.240Z:
Received chunk:  uns...

2025-04-02T03:47:13.264Z:
Received chunk: alted...

2025-04-02T03:47:13.288Z:
Received chunk:  butter...

2025-04-02T03:47:13.312Z:
Received chunk: ,...

2025-04-02T03:47:13.337Z:
Received chunk:  at...

2025-04-02T03:47:13.362Z:
Received chunk:  room...

2025-04-02T03:47:13.386Z:
Received chunk:  temperature...

2025-04-02T03:47:13.411Z:
Received chunk: 
...

2025-04-02T03:47:13.436Z:
Received chunk: *...

2025-04-02T03:47:13.460Z:
Received chunk:  ...

2025-04-02T03:47:13.485Z:
Received chunk: 3...

2025-04-02T03:47:13.510Z:
Received chunk: /...

2025-04-02T03:47:13.535Z:
Received chunk: 4...

2025-04-02T03:47:13.560Z:
Received chunk:  cup...

2025-04-02T03:47:13.584Z:
Received chunk:  white...

2025-04-02T03:47:13.609Z:
Received chunk:  gran...

2025-04-02T03:47:13.633Z:
Received chunk: ulated...

2025-04-02T03:47:13.657Z:
Received chunk:  sugar...

2025-04-02T03:47:13.684Z:
Received chunk: 
...

2025-04-02T03:47:13.708Z:
Received chunk: *...

2025-04-02T03:47:13.733Z:
Received chunk:  ...

2025-04-02T03:47:13.758Z:
Received chunk: 3...

2025-04-02T03:47:13.783Z:
Received chunk: /...

2025-04-02T03:47:13.807Z:
Received chunk: 4...

2025-04-02T03:47:13.832Z:
Received chunk:  cup...

2025-04-02T03:47:13.858Z:
Received chunk:  brown...

2025-04-02T03:47:13.884Z:
Received chunk:  sugar...

2025-04-02T03:47:13.909Z:
Received chunk: 
...

2025-04-02T03:47:13.934Z:
Received chunk: *...

2025-04-02T03:47:13.959Z:
Received chunk:  ...

2025-04-02T03:47:13.982Z:
Received chunk: 2...

2025-04-02T03:47:14.007Z:
Received chunk:  large...

2025-04-02T03:47:14.031Z:
Received chunk:  eggs...

2025-04-02T03:47:14.055Z:
Received chunk: 
...

2025-04-02T03:47:14.079Z:
Received chunk: *...

2025-04-02T03:47:14.103Z:
Received chunk:  ...

2025-04-02T03:47:14.126Z:
Received chunk: 2...

2025-04-02T03:47:14.151Z:
Received chunk:  cups...

2025-04-02T03:47:14.175Z:
Received chunk:  semi...

2025-04-02T03:47:14.199Z:
Received chunk: -s...

2025-04-02T03:47:14.223Z:
Received chunk: weet...

2025-04-02T03:47:14.247Z:
Received chunk:  chocolate...

2025-04-02T03:47:14.271Z:
Received chunk:  chips...

2025-04-02T03:47:14.296Z:
Received chunk: 

...

2025-04-02T03:47:14.321Z:
Received chunk: **...

2025-04-02T03:47:14.345Z:
Received chunk: Instructions...

2025-04-02T03:47:14.370Z:
Received chunk: :...

2025-04-02T03:47:14.394Z:
Received chunk: **

...

2025-04-02T03:47:14.419Z:
Received chunk: 1...

2025-04-02T03:47:14.443Z:
Received chunk: ....

2025-04-02T03:47:14.468Z:
Received chunk:  **...

2025-04-02T03:47:14.493Z:
Received chunk: Pre...

2025-04-02T03:47:14.517Z:
Received chunk: heat...

2025-04-02T03:47:14.542Z:
Received chunk:  your...

2025-04-02T03:47:14.567Z:
Received chunk:  oven...

2025-04-02T03:47:14.592Z:
Received chunk: :**...

2025-04-02T03:47:14.616Z:
Received chunk:  Set...

2025-04-02T03:47:14.641Z:
Received chunk:  your...

2025-04-02T03:47:14.666Z:
Received chunk:  oven...

2025-04-02T03:47:14.691Z:
Received chunk:  to...

2025-04-02T03:47:14.717Z:
Received chunk:  ...

2025-04-02T03:47:14.742Z:
Received chunk: 375...

2025-04-02T03:47:14.767Z:
Received chunk: °F...

2025-04-02T03:47:14.792Z:
Received chunk:  (...

2025-04-02T03:47:14.816Z:
Received chunk: 190...

2025-04-02T03:47:14.843Z:
Received chunk: °C...

2025-04-02T03:47:14.868Z:
Received chunk: )....

2025-04-02T03:47:14.893Z:
Received chunk:  Line...

2025-04-02T03:47:14.919Z:
Received chunk:  a...

2025-04-02T03:47:14.944Z:
Received chunk:  baking...

2025-04-02T03:47:14.969Z:
Received chunk:  sheet...

2025-04-02T03:47:14.993Z:
Received chunk:  with...

2025-04-02T03:47:15.018Z:
Received chunk:  parchment...

2025-04-02T03:47:15.042Z:
Received chunk:  paper...

2025-04-02T03:47:15.067Z:
Received chunk:  or...

2025-04-02T03:47:15.091Z:
Received chunk:  a...

2025-04-02T03:47:15.115Z:
Received chunk:  silicone...

2025-04-02T03:47:15.138Z:
Received chunk:  mat...

2025-04-02T03:47:15.162Z:
Received chunk: .
...

2025-04-02T03:47:15.185Z:
Received chunk: 2...

2025-04-02T03:47:15.209Z:
Received chunk: ....

2025-04-02T03:47:15.234Z:
Received chunk:  **...

2025-04-02T03:47:15.258Z:
Received chunk: Wh...

2025-04-02T03:47:15.282Z:
Received chunk: isk...

2025-04-02T03:47:15.305Z:
Received chunk:  dry...

2025-04-02T03:47:15.330Z:
Received chunk:  ingredients...

2025-04-02T03:47:15.354Z:
Received chunk: :**...

2025-04-02T03:47:15.379Z:
Received chunk:  In...

2025-04-02T03:47:15.404Z:
Received chunk:  a...

2025-04-02T03:47:15.429Z:
Received chunk:  medium...

2025-04-02T03:47:15.453Z:
Received chunk:  bowl...

2025-04-02T03:47:15.479Z:
Received chunk: ,...

2025-04-02T03:47:15.504Z:
Received chunk:  whisk...

2025-04-02T03:47:15.529Z:
Received chunk:  together...

2025-04-02T03:47:15.554Z:
Received chunk:  flour...

2025-04-02T03:47:15.578Z:
Received chunk: ,...

2025-04-02T03:47:15.603Z:
Received chunk:  baking...

2025-04-02T03:47:15.628Z:
Received chunk:  soda...

2025-04-02T03:47:15.652Z:
Received chunk: ,...

2025-04-02T03:47:15.677Z:
Received chunk:  and...

2025-04-02T03:47:15.703Z:
Received chunk:  salt...

2025-04-02T03:47:15.728Z:
Received chunk: ....

2025-04-02T03:47:15.753Z:
Received chunk:  Set...

2025-04-02T03:47:15.778Z:
Received chunk:  aside...

2025-04-02T03:47:15.803Z:
Received chunk: .
...

2025-04-02T03:47:15.828Z:
Received chunk: 3...

2025-04-02T03:47:15.853Z:
Received chunk: ....

2025-04-02T03:47:15.879Z:
Received chunk:  **...

2025-04-02T03:47:15.905Z:
Received chunk: Cream...

2025-04-02T03:47:15.931Z:
Received chunk:  butter...

2025-04-02T03:47:15.956Z:
Received chunk:  and...

2025-04-02T03:47:15.982Z:
Received chunk:  sugars...

2025-04-02T03:47:16.007Z:
Received chunk: :**...

2025-04-02T03:47:16.031Z:
Received chunk:  In...

2025-04-02T03:47:16.056Z:
Received chunk:  a...

2025-04-02T03:47:16.080Z:
Received chunk:  large...

2025-04-02T03:47:16.104Z:
Received chunk:  bowl...

2025-04-02T03:47:16.129Z:
Received chunk: ,...

2025-04-02T03:47:16.153Z:
Received chunk:  use...

2025-04-02T03:47:16.177Z:
Received chunk:  an...

2025-04-02T03:47:16.201Z:
Received chunk:  electric...

2025-04-02T03:47:16.225Z:
Received chunk:  mixer...

2025-04-02T03:47:16.249Z:
Received chunk:  to...

2025-04-02T03:47:16.273Z:
Received chunk:  cream...

2025-04-02T03:47:16.298Z:
Received chunk:  the...

2025-04-02T03:47:16.323Z:
Received chunk:  butter...

2025-04-02T03:47:16.347Z:
Received chunk:  and...

2025-04-02T03:47:16.372Z:
Received chunk:  sugars...

2025-04-02T03:47:16.397Z:
Received chunk:  until...

2025-04-02T03:47:16.422Z:
Received chunk:  light...

2025-04-02T03:47:16.448Z:
Received chunk:  and...

2025-04-02T03:47:16.474Z:
Received chunk:  fluffy...

2025-04-02T03:47:16.500Z:
Received chunk: ,...

2025-04-02T03:47:16.526Z:
Received chunk:  about...

2025-04-02T03:47:16.551Z:
Received chunk:  ...

2025-04-02T03:47:16.576Z:
Received chunk: 2...

2025-04-02T03:47:16.602Z:
Received chunk: -...

2025-04-02T03:47:16.628Z:
Received chunk: 3...

2025-04-02T03:47:16.654Z:
Received chunk:  minutes...

2025-04-02T03:47:16.679Z:
Received chunk: .
...

2025-04-02T03:47:16.704Z:
Received chunk: 4...

2025-04-02T03:47:16.731Z:
Received chunk: ....

2025-04-02T03:47:16.756Z:
Received chunk:  **...

2025-04-02T03:47:16.781Z:
Received chunk: Beat...

2025-04-02T03:47:16.807Z:
Received chunk:  in...

2025-04-02T03:47:16.842Z:
Received chunk:  eggs...

2025-04-02T03:47:16.858Z:
Received chunk: :**...

2025-04-02T03:47:16.884Z:
Received chunk:  Beat...

2025-04-02T03:47:16.910Z:
Received chunk:  in...

2025-04-02T03:47:16.935Z:
Received chunk:  the...

2025-04-02T03:47:16.960Z:
Received chunk:  eggs...

2025-04-02T03:47:16.987Z:
Received chunk:  one...

2025-04-02T03:47:17.012Z:
Received chunk:  at...

2025-04-02T03:47:17.036Z:
Received chunk:  a...

2025-04-02T03:47:17.062Z:
Received chunk:  time...

2025-04-02T03:47:17.087Z:
Received chunk: ,...

2025-04-02T03:47:17.111Z:
Received chunk:  making...

2025-04-02T03:47:17.135Z:
Received chunk:  sure...

2025-04-02T03:47:17.159Z:
Received chunk:  each...

2025-04-02T03:47:17.184Z:
Received chunk:  egg...

2025-04-02T03:47:17.208Z:
Received chunk:  is...

2025-04-02T03:47:17.233Z:
Received chunk:  fully...

2025-04-02T03:47:17.257Z:
Received chunk:  incorporated...

2025-04-02T03:47:17.282Z:
Received chunk:  before...

2025-04-02T03:47:17.307Z:
Received chunk:  adding...

2025-04-02T03:47:17.332Z:
Received chunk:  the...

2025-04-02T03:47:17.357Z:
Received chunk:  next...

2025-04-02T03:47:17.382Z:
Received chunk: .
...

2025-04-02T03:47:17.407Z:
Received chunk: 5...

2025-04-02T03:47:17.431Z:
Received chunk: ....

2025-04-02T03:47:17.457Z:
Received chunk:  **...

2025-04-02T03:47:17.483Z:
Received chunk: Mix...

2025-04-02T03:47:17.508Z:
Received chunk:  in...

2025-04-02T03:47:17.533Z:
Received chunk:  dry...

2025-04-02T03:47:17.558Z:
Received chunk:  ingredients...

2025-04-02T03:47:17.583Z:
Received chunk: :**...

2025-04-02T03:47:17.608Z:
Received chunk:  Grad...

2025-04-02T03:47:17.633Z:
Received chunk: ually...

2025-04-02T03:47:17.658Z:
Received chunk:  mix...

2025-04-02T03:47:17.683Z:
Received chunk:  in...

2025-04-02T03:47:17.708Z:
Received chunk:  the...

2025-04-02T03:47:17.733Z:
Received chunk:  flour...

2025-04-02T03:47:17.759Z:
Received chunk:  mixture...

2025-04-02T03:47:17.784Z:
Received chunk:  until...

2025-04-02T03:47:17.809Z:
Received chunk:  just...

2025-04-02T03:47:17.834Z:
Received chunk:  combined...

2025-04-02T03:47:17.860Z:
Received chunk: ,...

2025-04-02T03:47:17.886Z:
Received chunk:  being...

2025-04-02T03:47:17.912Z:
Received chunk:  careful...

2025-04-02T03:47:17.938Z:
Received chunk:  not...

2025-04-02T03:47:17.964Z:
Received chunk:  to...

2025-04-02T03:47:17.990Z:
Received chunk:  over...

2025-04-02T03:47:18.015Z:
Received chunk: mix...

2025-04-02T03:47:18.040Z:
Received chunk: .
...

2025-04-02T03:47:18.064Z:
Received chunk: 6...

2025-04-02T03:47:18.090Z:
Received chunk: ....

2025-04-02T03:47:18.115Z:
Received chunk:  **...

2025-04-02T03:47:18.139Z:
Received chunk: St...

2025-04-02T03:47:18.164Z:
Received chunk: ir...

2025-04-02T03:47:18.188Z:
Received chunk:  in...

2025-04-02T03:47:18.214Z:
Received chunk:  chocolate...

2025-04-02T03:47:18.238Z:
Received chunk:  chips...

2025-04-02T03:47:18.263Z:
Received chunk: :**...

2025-04-02T03:47:18.288Z:
Received chunk:  Stir...

2025-04-02T03:47:18.313Z:
Received chunk:  in...

2025-04-02T03:47:18.338Z:
Received chunk:  the...

2025-04-02T03:47:18.363Z:
Received chunk:  chocolate...

2025-04-02T03:47:18.388Z:
Received chunk:  chips...

2025-04-02T03:47:18.413Z:
Received chunk:  to...

2025-04-02T03:47:18.438Z:
Received chunk:  distribute...

2025-04-02T03:47:18.462Z:
Received chunk:  them...

2025-04-02T03:47:18.487Z:
Received chunk:  evenly...

2025-04-02T03:47:18.512Z:
Received chunk:  throughout...

2025-04-02T03:47:18.538Z:
Received chunk:  the...

2025-04-02T03:47:18.562Z:
Received chunk:  dough...

2025-04-02T03:47:18.587Z:
Received chunk: .
...

2025-04-02T03:47:18.612Z:
Received chunk: 7...

2025-04-02T03:47:18.637Z:
Received chunk: ....

2025-04-02T03:47:18.663Z:
Received chunk:  **...

2025-04-02T03:47:18.688Z:
Received chunk: Ch...

2025-04-02T03:47:18.719Z:
Received chunk: ill...

2025-04-02T03:47:18.739Z:
Received chunk:  the...

2025-04-02T03:47:18.765Z:
Received chunk:  dough...

2025-04-02T03:47:18.791Z:
Received chunk:  (...

2025-04-02T03:47:18.816Z:
Received chunk: optional...

2025-04-02T03:47:18.843Z:
Received chunk: ):...

2025-04-02T03:47:18.871Z:
Received chunk: **...

2025-04-02T03:47:18.900Z:
Received chunk:  If...

2025-04-02T03:47:18.926Z:
Received chunk:  you...

2025-04-02T03:47:18.954Z:
Received chunk:  want...

2025-04-02T03:47:18.981Z:
Received chunk:  a...

2025-04-02T03:47:19.006Z:
Received chunk:  cr...

2025-04-02T03:47:19.032Z:
Received chunk: isper...

2025-04-02T03:47:19.058Z:
Received chunk:  cookie...

2025-04-02T03:47:19.083Z:
Received chunk: ,...

2025-04-02T03:47:19.109Z:
Received chunk:  chill...

2025-04-02T03:47:19.134Z:
Received chunk:  the...

2025-04-02T03:47:19.159Z:
Received chunk:  dough...

2025-04-02T03:47:19.184Z:
Received chunk:  for...

2025-04-02T03:47:19.209Z:
Received chunk:  at...

2025-04-02T03:47:19.234Z:
Received chunk:  least...

2025-04-02T03:47:19.259Z:
Received chunk:  ...

2025-04-02T03:47:19.284Z:
Received chunk: 30...

2025-04-02T03:47:19.309Z:
Received chunk:  minutes...

2025-04-02T03:47:19.335Z:
Received chunk:  or...

2025-04-02T03:47:19.360Z:
Received chunk:  up...

2025-04-02T03:47:19.385Z:
Received chunk:  to...

2025-04-02T03:47:19.410Z:
Received chunk:  ...

2025-04-02T03:47:19.435Z:
Received chunk: 2...

2025-04-02T03:47:19.460Z:
Received chunk:  hours...

2025-04-02T03:47:19.485Z:
Received chunk: ....

2025-04-02T03:47:19.510Z:
Received chunk:  This...

2025-04-02T03:47:19.536Z:
Received chunk:  will...

2025-04-02T03:47:19.561Z:
Received chunk:  help...

2025-04-02T03:47:19.585Z:
Received chunk:  the...

2025-04-02T03:47:19.610Z:
Received chunk:  cookies...

2025-04-02T03:47:19.634Z:
Received chunk:  retain...

2025-04-02T03:47:19.660Z:
Received chunk:  their...

2025-04-02T03:47:19.685Z:
Received chunk:  shape...

2025-04-02T03:47:19.720Z:
Received chunk:  better...

2025-04-02T03:47:19.737Z:
Received chunk:  and...

2025-04-02T03:47:19.763Z:
Received chunk:  bake...

2025-04-02T03:47:19.788Z:
Received chunk:  up...

2025-04-02T03:47:19.814Z:
Received chunk:  with...

2025-04-02T03:47:19.839Z:
Received chunk:  a...

2025-04-02T03:47:19.867Z:
Received chunk:  nice...

2025-04-02T03:47:19.893Z:
Received chunk:  texture...

2025-04-02T03:47:19.919Z:
Received chunk: .

...

2025-04-02T03:47:19.946Z:
Received chunk: **...

2025-04-02T03:47:19.972Z:
Received chunk: B...

2025-04-02T03:47:19.998Z:
Received chunk: aking...

2025-04-02T03:47:20.024Z:
Received chunk: :...

2025-04-02T03:47:20.049Z:
Received chunk: **

...

2025-04-02T03:47:20.073Z:
Received chunk: 1...

2025-04-02T03:47:20.098Z:
Received chunk: ....

2025-04-02T03:47:20.123Z:
Received chunk:  **...

2025-04-02T03:47:20.148Z:
Received chunk: S...

2025-04-02T03:47:20.173Z:
Received chunk: coop...

2025-04-02T03:47:20.198Z:
Received chunk:  and...

2025-04-02T03:47:20.224Z:
Received chunk:  place...

2025-04-02T03:47:20.248Z:
Received chunk: :**...

2025-04-02T03:47:20.273Z:
Received chunk:  Use...

2025-04-02T03:47:20.298Z:
Received chunk:  a...

2025-04-02T03:47:20.323Z:
Received chunk:  tablespoon...

2025-04-02T03:47:20.348Z:
Received chunk:  or...

2025-04-02T03:47:20.373Z:
Received chunk:  ice...

2025-04-02T03:47:20.397Z:
Received chunk:  cream...

2025-04-02T03:47:20.422Z:
Received chunk:  scoop...

2025-04-02T03:47:20.448Z:
Received chunk:  to...

2025-04-02T03:47:20.474Z:
Received chunk:  portion...

2025-04-02T03:47:20.500Z:
Received chunk:  out...

2025-04-02T03:47:20.525Z:
Received chunk:  balls...

2025-04-02T03:47:20.550Z:
Received chunk:  of...

2025-04-02T03:47:20.575Z:
Received chunk:  dough...

2025-04-02T03:47:20.600Z:
Received chunk:  onto...

2025-04-02T03:47:20.625Z:
Received chunk:  the...

2025-04-02T03:47:20.650Z:
Received chunk:  prepared...

2025-04-02T03:47:20.676Z:
Received chunk:  baking...

2025-04-02T03:47:20.703Z:
Received chunk:  sheet...

2025-04-02T03:47:20.729Z:
Received chunk: ,...

2025-04-02T03:47:20.755Z:
Received chunk:  leaving...

2025-04-02T03:47:20.781Z:
Received chunk:  about...

2025-04-02T03:47:20.805Z:
Received chunk:  ...

2025-04-02T03:47:20.830Z:
Received chunk: 2...

2025-04-02T03:47:20.862Z:
Received chunk:  inches...

2025-04-02T03:47:20.880Z:
Received chunk:  of...

2025-04-02T03:47:20.906Z:
Received chunk:  space...

2025-04-02T03:47:20.932Z:
Received chunk:  between...

2025-04-02T03:47:20.958Z:
Received chunk:  each...

2025-04-02T03:47:20.984Z:
Received chunk:  cookie...

2025-04-02T03:47:21.009Z:
Received chunk: .
...

2025-04-02T03:47:21.034Z:
Received chunk: 2...

2025-04-02T03:47:21.060Z:
Received chunk: ....

2025-04-02T03:47:21.086Z:
Received chunk:  **...

2025-04-02T03:47:21.112Z:
Received chunk: Fl...

2025-04-02T03:47:21.137Z:
Received chunk: atten...

2025-04-02T03:47:21.163Z:
Received chunk:  gently...

2025-04-02T03:47:21.188Z:
Received chunk: :**...

2025-04-02T03:47:21.214Z:
Received chunk:  G...

2025-04-02T03:47:21.239Z:
Received chunk: ently...

2025-04-02T03:47:21.264Z:
Received chunk:  flatten...

2025-04-02T03:47:21.290Z:
Received chunk:  each...

2025-04-02T03:47:21.316Z:
Received chunk:  ball...

2025-04-02T03:47:21.344Z:
Received chunk:  of...

2025-04-02T03:47:21.369Z:
Received chunk:  dough...

2025-04-02T03:47:21.395Z:
Received chunk:  into...

2025-04-02T03:47:21.421Z:
Received chunk:  a...

2025-04-02T03:47:21.446Z:
Received chunk:  disk...

2025-04-02T03:47:21.472Z:
Received chunk:  shape...

2025-04-02T03:47:21.498Z:
Received chunk:  using...

2025-04-02T03:47:21.523Z:
Received chunk:  your...

2025-04-02T03:47:21.548Z:
Received chunk:  fingers...

2025-04-02T03:47:21.574Z:
Received chunk:  or...

2025-04-02T03:47:21.599Z:
Received chunk:  a...

2025-04-02T03:47:21.624Z:
Received chunk:  spat...

2025-04-02T03:47:21.649Z:
Received chunk: ula...

2025-04-02T03:47:21.675Z:
Received chunk: .
...

2025-04-02T03:47:21.700Z:
Received chunk: 3...

2025-04-02T03:47:21.728Z:
Received chunk: ....

2025-04-02T03:47:21.752Z:
Received chunk:  **...

2025-04-02T03:47:21.778Z:
Received chunk: B...

2025-04-02T03:47:21.802Z:
Received chunk: ake...

2025-04-02T03:47:21.828Z:
Received chunk: :**...

2025-04-02T03:47:21.853Z:
Received chunk:  Bake...

2025-04-02T03:47:21.880Z:
Received chunk:  for...

2025-04-02T03:47:21.907Z:
Received chunk:  ...

2025-04-02T03:47:21.934Z:
Received chunk: 10...

2025-04-02T03:47:21.960Z:
Received chunk: -...

2025-04-02T03:47:21.987Z:
Received chunk: 12...

2025-04-02T03:47:22.012Z:
Received chunk:  minutes...

2025-04-02T03:47:22.037Z:
Received chunk:  or...

2025-04-02T03:47:22.062Z:
Received chunk:  until...

2025-04-02T03:47:22.087Z:
Received chunk:  the...

2025-04-02T03:47:22.112Z:
Received chunk:  edges...

2025-04-02T03:47:22.137Z:
Received chunk:  are...

2025-04-02T03:47:22.161Z:
Received chunk:  lightly...

2025-04-02T03:47:22.187Z:
Received chunk:  golden...

2025-04-02T03:47:22.211Z:
Received chunk:  brown...

2025-04-02T03:47:22.236Z:
Received chunk:  and...

2025-04-02T03:47:22.261Z:
Received chunk:  the...

2025-04-02T03:47:22.285Z:
Received chunk:  centers...

2025-04-02T03:47:22.310Z:
Received chunk:  are...

2025-04-02T03:47:22.335Z:
Received chunk:  set...

2025-04-02T03:47:22.359Z:
Received chunk: .

...

2025-04-02T03:47:22.384Z:
Received chunk: **...

2025-04-02T03:47:22.409Z:
Received chunk: Tips...

2025-04-02T03:47:22.434Z:
Received chunk: :...

2025-04-02T03:47:22.459Z:
Received chunk: **

...

2025-04-02T03:47:22.484Z:
Received chunk: *...

2025-04-02T03:47:22.510Z:
Received chunk:  Use...

2025-04-02T03:47:22.535Z:
Received chunk:  high...

2025-04-02T03:47:22.560Z:
Received chunk: -quality...

2025-04-02T03:47:22.585Z:
Received chunk:  chocolate...

2025-04-02T03:47:22.610Z:
Received chunk:  chips...

2025-04-02T03:47:22.636Z:
Received chunk:  for...

2025-04-02T03:47:22.662Z:
Received chunk:  the...

2025-04-02T03:47:22.687Z:
Received chunk:  best...

2025-04-02T03:47:22.712Z:
Received chunk:  flavor...

2025-04-02T03:47:22.736Z:
Received chunk: .
...

2025-04-02T03:47:22.763Z:
Received chunk: *...

2025-04-02T03:47:22.788Z:
Received chunk:  If...

2025-04-02T03:47:22.813Z:
Received chunk:  you...

2025-04-02T03:47:22.838Z:
Received chunk:  want...

2025-04-02T03:47:22.863Z:
Received chunk:  a...

2025-04-02T03:47:22.889Z:
Received chunk:  chew...

2025-04-02T03:47:22.916Z:
Received chunk: ier...

2025-04-02T03:47:22.942Z:
Received chunk:  cookie...

2025-04-02T03:47:22.968Z:
Received chunk: ,...

2025-04-02T03:47:22.994Z:
Received chunk:  bake...

2025-04-02T03:47:23.019Z:
Received chunk:  for...

2025-04-02T03:47:23.044Z:
Received chunk:  ...

2025-04-02T03:47:23.068Z:
Received chunk: 8...

2025-04-02T03:47:23.093Z:
Received chunk: -...

2025-04-02T03:47:23.118Z:
Received chunk: 10...

2025-04-02T03:47:23.142Z:
Received chunk:  minutes...

2025-04-02T03:47:23.166Z:
Received chunk: ....

2025-04-02T03:47:23.191Z:
Received chunk:  For...

2025-04-02T03:47:23.216Z:
Received chunk:  a...

2025-04-02T03:47:23.240Z:
Received chunk:  cr...

2025-04-02T03:47:23.266Z:
Received chunk: isper...

2025-04-02T03:47:23.290Z:
Received chunk:  cookie...

2025-04-02T03:47:23.315Z:
Received chunk: ,...

2025-04-02T03:47:23.340Z:
Received chunk:  bake...

2025-04-02T03:47:23.364Z:
Received chunk:  for...

2025-04-02T03:47:23.389Z:
Received chunk:  ...

2025-04-02T03:47:23.414Z:
Received chunk: 12...

2025-04-02T03:47:23.439Z:
Received chunk: -...

2025-04-02T03:47:23.464Z:
Received chunk: 14...

2025-04-02T03:47:23.488Z:
Received chunk:  minutes...

2025-04-02T03:47:23.513Z:
Received chunk: .
...

2025-04-02T03:47:23.538Z:
Received chunk: *...

2025-04-02T03:47:23.563Z:
Received chunk:  Chill...

2025-04-02T03:47:23.588Z:
Received chunk:  the...

2025-04-02T03:47:23.612Z:
Received chunk:  dough...

2025-04-02T03:47:23.638Z:
Received chunk:  before...

2025-04-02T03:47:23.664Z:
Received chunk:  baking...

2025-04-02T03:47:23.691Z:
Received chunk:  to...

2025-04-02T03:47:23.718Z:
Received chunk:  help...

2025-04-02T03:47:23.745Z:
Received chunk:  the...

2025-04-02T03:47:23.773Z:
Received chunk:  cookies...

2025-04-02T03:47:23.800Z:
Received chunk:  retain...

2025-04-02T03:47:23.826Z:
Received chunk:  their...

2025-04-02T03:47:23.852Z:
Received chunk:  shape...

2025-04-02T03:47:23.880Z:
Received chunk:  and...

2025-04-02T03:47:23.908Z:
Received chunk:  bake...

2025-04-02T03:47:23.935Z:
Received chunk:  up...

2025-04-02T03:47:23.962Z:
Received chunk:  with...

2025-04-02T03:47:23.989Z:
Received chunk:  a...

2025-04-02T03:47:24.015Z:
Received chunk:  nice...

2025-04-02T03:47:24.040Z:
Received chunk:  texture...

2025-04-02T03:47:24.064Z:
Received chunk: .
...

2025-04-02T03:47:24.089Z:
Received chunk: *...

2025-04-02T03:47:24.114Z:
Received chunk:  Don...

2025-04-02T03:47:24.139Z:
Received chunk: 't...

2025-04-02T03:47:24.163Z:
Received chunk:  over...

2025-04-02T03:47:24.188Z:
Received chunk: b...

2025-04-02T03:47:24.212Z:
Received chunk: ake...

2025-04-02T03:47:24.237Z:
Received chunk: !...

2025-04-02T03:47:24.261Z:
Received chunk:  Chocolate...

2025-04-02T03:47:24.286Z:
Received chunk:  chip...

2025-04-02T03:47:24.310Z:
Received chunk:  cookies...

2025-04-02T03:47:24.335Z:
Received chunk:  are...

2025-04-02T03:47:24.360Z:
Received chunk:  best...

2025-04-02T03:47:24.384Z:
Received chunk:  when...

2025-04-02T03:47:24.409Z:
Received chunk:  they...

2025-04-02T03:47:24.434Z:
Received chunk: 're...

2025-04-02T03:47:24.459Z:
Received chunk:  lightly...

2025-04-02T03:47:24.483Z:
Received chunk:  golden...

2025-04-02T03:47:24.508Z:
Received chunk:  brown...

2025-04-02T03:47:24.533Z:
Received chunk:  and...

2025-04-02T03:47:24.558Z:
Received chunk:  still...

2025-04-02T03:47:24.583Z:
Received chunk:  slightly...

2025-04-02T03:47:24.608Z:
Received chunk:  soft...

2025-04-02T03:47:24.633Z:
Received chunk:  in...

2025-04-02T03:47:24.658Z:
Received chunk:  the...

2025-04-02T03:47:24.682Z:
Received chunk:  center...

2025-04-02T03:47:24.707Z:
Received chunk: .

...

2025-04-02T03:47:24.741Z:
Received chunk: **...

2025-04-02T03:47:24.758Z:
Received chunk: Trou...

2025-04-02T03:47:24.783Z:
Received chunk: b...

2025-04-02T03:47:24.809Z:
Received chunk: leshooting...

2025-04-02T03:47:24.834Z:
Received chunk: :...

2025-04-02T03:47:24.859Z:
Received chunk: **

...

2025-04-02T03:47:24.885Z:
Received chunk: *...

2025-04-02T03:47:24.912Z:
Received chunk:  **...

2025-04-02T03:47:24.938Z:
Received chunk: Cookies...

2025-04-02T03:47:24.964Z:
Received chunk:  spread...

2025-04-02T03:47:24.989Z:
Received chunk:  too...

2025-04-02T03:47:25.014Z:
Received chunk:  much...

2025-04-02T03:47:25.039Z:
Received chunk: :**...

2025-04-02T03:47:25.063Z:
Received chunk:  Try...

2025-04-02T03:47:25.088Z:
Received chunk:  chilling...

2025-04-02T03:47:25.113Z:
Received chunk:  the...

2025-04-02T03:47:25.137Z:
Received chunk:  dough...

2025-04-02T03:47:25.162Z:
Received chunk:  for...

2025-04-02T03:47:25.186Z:
Received chunk:  ...

2025-04-02T03:47:25.211Z:
Received chunk: 30...

2025-04-02T03:47:25.237Z:
Received chunk:  minutes...

2025-04-02T03:47:25.262Z:
Received chunk:  or...

2025-04-02T03:47:25.286Z:
Received chunk:  more...

2025-04-02T03:47:25.311Z:
Received chunk: ....

2025-04-02T03:47:25.336Z:
Received chunk:  This...

2025-04-02T03:47:25.361Z:
Received chunk:  will...

2025-04-02T03:47:25.387Z:
Received chunk:  help...

2025-04-02T03:47:25.411Z:
Received chunk:  the...

2025-04-02T03:47:25.437Z:
Received chunk:  cookies...

2025-04-02T03:47:25.462Z:
Received chunk:  retain...

2025-04-02T03:47:25.487Z:
Received chunk:  their...

2025-04-02T03:47:25.513Z:
Received chunk:  shape...

2025-04-02T03:47:25.538Z:
Received chunk:  better...

2025-04-02T03:47:25.562Z:
Received chunk: .
...

2025-04-02T03:47:25.587Z:
Received chunk: *...

2025-04-02T03:47:25.613Z:
Received chunk:  **...

2025-04-02T03:47:25.639Z:
Received chunk: Cookies...

2025-04-02T03:47:25.665Z:
Received chunk:  don...

2025-04-02T03:47:25.691Z:
Received chunk: 't...

2025-04-02T03:47:25.716Z:
Received chunk:  rise...

2025-04-02T03:47:25.747Z:
Received chunk: :**...

2025-04-02T03:47:25.767Z:
Received chunk:  Check...

2025-04-02T03:47:25.794Z:
Received chunk:  your...

2025-04-02T03:47:25.819Z:
Received chunk:  oven...

2025-04-02T03:47:25.844Z:
Received chunk:  temperature...

2025-04-02T03:47:25.870Z:
Received chunk: !...

2025-04-02T03:47:25.896Z:
Received chunk:  If...

2025-04-02T03:47:25.922Z:
Received chunk:  it...

2025-04-02T03:47:25.948Z:
Received chunk: 's...

2025-04-02T03:47:25.974Z:
Received chunk:  off...

2025-04-02T03:47:26.000Z:
Received chunk: ,...

2025-04-02T03:47:26.026Z:
Received chunk:  bake...

2025-04-02T03:47:26.050Z:
Received chunk:  for...

2025-04-02T03:47:26.075Z:
Received chunk:  a...

2025-04-02T03:47:26.099Z:
Received chunk:  few...

2025-04-02T03:47:26.125Z:
Received chunk:  minutes...

2025-04-02T03:47:26.149Z:
Received chunk:  longer...

2025-04-02T03:47:26.174Z:
Received chunk:  to...

2025-04-02T03:47:26.199Z:
Received chunk:  ensure...

2025-04-02T03:47:26.224Z:
Received chunk:  even...

2025-04-02T03:47:26.249Z:
Received chunk:  baking...

2025-04-02T03:47:26.275Z:
Received chunk: .
...

2025-04-02T03:47:26.300Z:
Received chunk: *...

2025-04-02T03:47:26.326Z:
Received chunk:  **...

2025-04-02T03:47:26.351Z:
Received chunk: Cookies...

2025-04-02T03:47:26.375Z:
Received chunk:  are...

2025-04-02T03:47:26.400Z:
Received chunk:  too...

2025-04-02T03:47:26.425Z:
Received chunk:  hard...

2025-04-02T03:47:26.450Z:
Received chunk: :**...

2025-04-02T03:47:26.475Z:
Received chunk:  Try...

2025-04-02T03:47:26.500Z:
Received chunk:  using...

2025-04-02T03:47:26.525Z:
Received chunk:  a...

2025-04-02T03:47:26.550Z:
Received chunk:  lower...

2025-04-02T03:47:26.575Z:
Received chunk:  oven...

2025-04-02T03:47:26.600Z:
Received chunk:  temperature...

2025-04-02T03:47:26.625Z:
Received chunk:  (...

2025-04-02T03:47:26.651Z:
Received chunk: 350...

2025-04-02T03:47:26.676Z:
Received chunk: °F...

2025-04-02T03:47:26.701Z:
Received chunk: /...

2025-04-02T03:47:26.727Z:
Received chunk: 180...

2025-04-02T03:47:26.753Z:
Received chunk: °C...

2025-04-02T03:47:26.779Z:
Received chunk: )...

2025-04-02T03:47:26.804Z:
Received chunk:  and...

2025-04-02T03:47:26.830Z:
Received chunk:  baking...

2025-04-02T03:47:26.855Z:
Received chunk:  for...

2025-04-02T03:47:26.882Z:
Received chunk:  ...

2025-04-02T03:47:26.908Z:
Received chunk: 12...

2025-04-02T03:47:26.935Z:
Received chunk: -...

2025-04-02T03:47:26.962Z:
Received chunk: 14...

2025-04-02T03:47:26.989Z:
Received chunk:  minutes...

2025-04-02T03:47:27.014Z:
Received chunk: .

...

2025-04-02T03:47:27.039Z:
Received chunk: Enjoy...

2025-04-02T03:47:27.063Z:
Received chunk:  your...

2025-04-02T03:47:27.089Z:
Received chunk:  delicious...

2025-04-02T03:47:27.114Z:
Received chunk:  homemade...

2025-04-02T03:47:27.139Z:
Received chunk:  chocolate...

2025-04-02T03:47:27.164Z:
Received chunk:  chip...

2025-04-02T03:47:27.189Z:
Received chunk:  cookies...

2025-04-02T03:47:27.214Z:
Received chunk: !...

2025-04-02T03:47:27.239Z:
Received chunk: 

> ...

2025-04-02T03:47:44.107Z:
Unloading model...

2025-04-02T03:47:44.107Z:
Terminating running process

2025-04-02T03:47:44.107Z:
Model unloaded successfully

2025-04-02T03:47:44.115Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:44.115Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T03:47:44.115Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T03:47:44.115Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T03:47:44.184Z:
Process exited with code null

2025-04-02T03:47:44.184Z:
Process failed, falling back to simulation

2025-04-02T03:47:44.185Z:
Generating simulated response

2025-04-02T03:47:44.256Z:
Sent simulated chunk: I'm

2025-04-02T03:47:44.364Z:
Sent simulated chunk: a

2025-04-02T03:47:44.395Z:
Sent simulated chunk: locally

2025-04-02T03:47:44.488Z:
Sent simulated chunk: hosted

2025-04-02T03:47:44.596Z:
Sent simulated chunk: AI

2025-04-02T03:47:44.690Z:
Sent simulated chunk: assistant

2025-04-02T03:47:44.781Z:
Sent simulated chunk: running

2025-04-02T03:47:44.860Z:
Sent simulated chunk: directly

2025-04-02T03:47:44.905Z:
Sent simulated chunk: on

2025-04-02T03:47:44.950Z:
Sent simulated chunk: your

2025-04-02T03:47:44.997Z:
Sent simulated chunk: device.

2025-04-02T03:47:45.090Z:
Sent simulated chunk: I

2025-04-02T03:47:45.185Z:
Sent simulated chunk: process

2025-04-02T03:47:45.233Z:
Sent simulated chunk: information

2025-04-02T03:47:45.280Z:
Sent simulated chunk: using

2025-04-02T03:47:45.342Z:
Sent simulated chunk: the

2025-04-02T03:47:45.421Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:47:45.514Z:
Sent simulated chunk: model

2025-04-02T03:47:45.608Z:
Sent simulated chunk: loaded

2025-04-02T03:47:45.686Z:
Sent simulated chunk: in

2025-04-02T03:47:45.779Z:
Sent simulated chunk: the

2025-04-02T03:47:45.857Z:
Sent simulated chunk: LM

2025-04-02T03:47:45.903Z:
Sent simulated chunk: Terminal

2025-04-02T03:47:45.950Z:
Sent simulated chunk: application.

2025-04-02T03:47:45.997Z:
Sent simulated chunk: I've

2025-04-02T03:47:46.074Z:
Sent simulated chunk: received

2025-04-02T03:47:46.167Z:
Sent simulated chunk: your

2025-04-02T03:47:46.214Z:
Sent simulated chunk: message:

2025-04-02T03:47:46.277Z:
Sent simulated chunk: "give

2025-04-02T03:47:46.371Z:
Sent simulated chunk: me

2025-04-02T03:47:46.481Z:
Sent simulated chunk: a

2025-04-02T03:47:46.558Z:
Sent simulated chunk: detailed

2025-04-02T03:47:46.605Z:
Sent simulated chunk: recipe

2025-04-02T03:47:46.652Z:
Sent simulated chunk: for

2025-04-02T03:47:46.730Z:
Sent simulated chunk: chocolate

2025-04-02T03:47:46.792Z:
Sent simulated chunk: chip

2025-04-02T03:47:46.870Z:
Sent simulated chunk: cookies"

2025-04-02T03:47:46.932Z:
Sent simulated chunk: and

2025-04-02T03:47:47.026Z:
Sent simulated chunk: am

2025-04-02T03:47:47.120Z:
Sent simulated chunk: responding

2025-04-02T03:47:47.167Z:
Sent simulated chunk: without

2025-04-02T03:47:47.214Z:
Sent simulated chunk: requiring

2025-04-02T03:47:47.292Z:
Sent simulated chunk: internet

2025-04-02T03:47:47.400Z:
Sent simulated chunk: access.

(This

2025-04-02T03:47:47.493Z:
Sent simulated chunk: response

2025-04-02T03:47:47.539Z:
Sent simulated chunk: was

2025-04-02T03:47:47.617Z:
Sent simulated chunk: generated

2025-04-02T03:47:47.710Z:
Sent simulated chunk: using

2025-04-02T03:47:47.756Z:
Sent simulated chunk: a

2025-04-02T03:47:47.850Z:
Sent simulated chunk: simulated

2025-04-02T03:47:47.912Z:
Sent simulated chunk: version

2025-04-02T03:47:47.959Z:
Sent simulated chunk: of

2025-04-02T03:47:48.052Z:
Sent simulated chunk: the

2025-04-02T03:47:48.150Z:
Sent simulated chunk: llama-3.2-1b-instruct-q8_0

2025-04-02T03:47:48.208Z:
Sent simulated chunk: model)

2025-04-02T03:47:48.209Z:
Simulated response generation completed

2025-04-02T09:09:47.497Z:
Unloading model...

2025-04-02T09:09:47.497Z:
Model unloaded successfully

2025-04-02T09:09:47.498Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T09:09:47.498Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T09:09:47.498Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T09:09:47.499Z:
Model llama-3.2-1b-instruct-q8_0 file verified

2025-04-02T09:09:47.944Z:
Unloading model...

2025-04-02T09:09:47.944Z:
Model unloaded successfully

2025-04-02T09:09:47.944Z:
LLMService constructor called with model path: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T09:09:47.944Z:
Using binary path: C:\Users\tyler\Coding2025\ArxAIv\lm-terminal\bin\llama-cli.exe

2025-04-02T09:09:47.945Z:
Starting to load model from: C:\Users\tyler\.lmstudio\models\lmstudio-community\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf

2025-04-02T09:09:47.945Z:
Model llama-3.2-1b-instruct-q8_0 file verified
